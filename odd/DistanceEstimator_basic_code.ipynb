{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xXp-L947DlL"
   },
   "source": [
    "#Distance Estimator\n",
    "To estimate the real distance(unit: meter) of the object\n",
    "\n",
    "__Input__: Bounding box coordinates(xmin, ymin, xmax, ymax)   \n",
    "__Output__: 3D location z of carmera coordinates(z_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LiXtU2475cb"
   },
   "source": [
    "## Load Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "J4GISwk4884Q"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from custom_datasets import CustomDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./weights', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJxQzId_79SS"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../datasets/kitti_train.csv')\n",
    "df_valid = pd.read_csv('../datasets/kitti_valid.csv')\n",
    "df_test = pd.read_csv('../datasets/kitti_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['person', 'car', 'truck', 'train', 'bicycle', 'Misc'], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encoding\n",
    "class_dummy = pd.get_dummies(df_train['class'])\n",
    "df_train = pd.concat([df_train, class_dummy], axis=1)\n",
    "\n",
    "class_dummy = pd.get_dummies(df_valid['class'])\n",
    "df_valid = pd.concat([df_valid, class_dummy], axis=1)\n",
    "\n",
    "class_dummy = pd.get_dummies(df_test['class'])\n",
    "df_test = pd.concat([df_test, class_dummy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22022 entries, 0 to 22021\n",
      "Data columns (total 21 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   filename    22022 non-null  object \n",
      " 1   class       22022 non-null  object \n",
      " 2   xmin        22022 non-null  float64\n",
      " 3   ymin        22022 non-null  float64\n",
      " 4   xmax        22022 non-null  float64\n",
      " 5   ymax        22022 non-null  float64\n",
      " 6   angle       22022 non-null  float64\n",
      " 7   zloc        22022 non-null  float64\n",
      " 8   weather     22022 non-null  object \n",
      " 9   depth_y     22022 non-null  int64  \n",
      " 10  depth_mean  22022 non-null  float64\n",
      " 11  depth_x     22022 non-null  int64  \n",
      " 12  depth_min   22022 non-null  float64\n",
      " 13  width       22022 non-null  float64\n",
      " 14  height      22022 non-null  float64\n",
      " 15  Misc        22022 non-null  uint8  \n",
      " 16  bicycle     22022 non-null  uint8  \n",
      " 17  car         22022 non-null  uint8  \n",
      " 18  person      22022 non-null  uint8  \n",
      " 19  train       22022 non-null  uint8  \n",
      " 20  truck       22022 non-null  uint8  \n",
      "dtypes: float64(10), int64(2), object(3), uint8(6)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = ['xmin','ymin','xmax','ymax','width', 'height', 'depth_mean', 'depth_min', 'Misc', 'bicycle', 'car', 'person', 'train', 'truck']\n",
    "batch_sz = 8\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train\n",
    "train_dataset = CustomDataset(df_train, variable,True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_sz, shuffle=True)\n",
    "\n",
    "# valid\n",
    "valid_dataset = CustomDataset(df_valid, variable,True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_sz, shuffle=True)\n",
    "\n",
    "# train\n",
    "test_dataset = CustomDataset(df_test, variable,True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_sz, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3866, 0.5618, 0.4316, 0.4002, 0.0850, 0.0960, 0.2926, 0.2762, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3332, 0.5571, 0.3995, 0.5051, 0.1218, 0.1704, 0.2015, 0.1809, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2676, 0.5544, 0.3203, 0.2898, 0.0983, 0.0292, 0.1989, 0.1422, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5898, 0.4736, 0.5894, 0.1900, 0.0065, 0.0364, 0.1514, 0.1207, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.3361, 0.4934, 0.3399, 0.1985, 0.0140, 0.0240, 0.6967, 0.2162, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.3213, 0.5690, 0.3999, 0.4967, 0.1429, 0.1539, 0.1888, 0.1876, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7583, 0.4549, 0.7963, 0.1902, 0.0726, 0.0537, 0.4242, 0.4163, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2919, 0.5787, 0.3789, 0.5848, 0.1575, 0.2038, 0.1515, 0.1514, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000]])\n",
      "torch.Size([8, 14])\n",
      "tensor([[22.9500],\n",
      "        [17.6500],\n",
      "        [20.0300],\n",
      "        [28.7000],\n",
      "        [17.0200],\n",
      "        [17.6300],\n",
      "        [33.0900],\n",
      "        [14.5100]])\n"
     ]
    }
   ],
   "source": [
    "# look dataset\n",
    "for idx, batch in enumerate(train_dataloader):\n",
    "    if idx == 1:\n",
    "        break\n",
    "    print(batch[0])\n",
    "    print(batch[0].shape)\n",
    "    print(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "HxfoPh3l9n7f"
   },
   "outputs": [],
   "source": [
    "# standardized data\n",
    "#scalar = StandardScaler()\n",
    "#X_train = scalar.fit_transform(X_train)\n",
    "#y_train = scalar.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_18WIN49vj6"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "6SqWrYRLCdaO"
   },
   "outputs": [],
   "source": [
    "class DistanceEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistanceEstimator, self).__init__()\n",
    "        \n",
    "        #Layer\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(14,32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32,64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64,128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128,256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256,512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512,1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(16,1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make  variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistanceEstimator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=14, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (21): ReLU()\n",
       "    (22): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistanceEstimator()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       patience = 8,\n",
    "                                                       mode='min', # 우리는 낮아지는 값을 기대\n",
    "                                                       verbose=True)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400737"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train parameters\n",
    "def count_parameter(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameter(model) # 87585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7pqhZ4a9y99"
   },
   "source": [
    "## Make Train, Valid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, idx_interval):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_rmse = 0\n",
    "    \n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        prediction = model(batch[0].to(device))\n",
    "        loss = loss_fn(prediction, batch[1].to(device)).cpu()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_loss += loss.item()\n",
    "        train_rmse += np.sqrt(loss.item())\n",
    "        \n",
    "        if idx % idx_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{}] \\t Train Loss(MSE): {:.4f} \\t Train RMSE: {:.4f}\".format(epoch, batch_sz*(idx+1), \\\n",
    "                                                                            len(train_dataloader)*batch_sz, \\\n",
    "                                                                            loss.item(), np.sqrt(loss.item())))\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_rmse /= len(train_dataloader)\n",
    "        \n",
    "    return train_loss, train_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_rmse = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(valid_dataloader):\n",
    "            predictions = model(batch[0].to(device))\n",
    "            loss = loss_fn(predictions, batch[1].to(device)).cpu()\n",
    "            valid_loss += loss.item()\n",
    "            valid_rmse += np.sqrt(loss.item())\n",
    "            \n",
    "    valid_loss /= len(valid_dataloader)\n",
    "    valid_rmse /= len(valid_dataloader)\n",
    "    \n",
    "    return valid_loss,valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "IIL0JPDXAvST"
   },
   "outputs": [],
   "source": [
    "# Function to save the model \n",
    "def saveModel(model): \n",
    "    path = \"./weights/ODD_basic.pth\" \n",
    "    torch.save(model.state_dict(), path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [8/22024] \t Train Loss(MSE): 1924.6122 \t Train RMSE: 43.8704\n",
      "Train Epoch: 1 [2008/22024] \t Train Loss(MSE): 5.4375 \t Train RMSE: 2.3318\n",
      "Train Epoch: 1 [4008/22024] \t Train Loss(MSE): 14.0375 \t Train RMSE: 3.7467\n",
      "Train Epoch: 1 [6008/22024] \t Train Loss(MSE): 21.5756 \t Train RMSE: 4.6450\n",
      "Train Epoch: 1 [8008/22024] \t Train Loss(MSE): 34.9350 \t Train RMSE: 5.9106\n",
      "Train Epoch: 1 [10008/22024] \t Train Loss(MSE): 34.3282 \t Train RMSE: 5.8590\n",
      "Train Epoch: 1 [12008/22024] \t Train Loss(MSE): 20.8787 \t Train RMSE: 4.5693\n",
      "Train Epoch: 1 [14008/22024] \t Train Loss(MSE): 40.9569 \t Train RMSE: 6.3998\n",
      "Train Epoch: 1 [16008/22024] \t Train Loss(MSE): 21.6679 \t Train RMSE: 4.6549\n",
      "Train Epoch: 1 [18008/22024] \t Train Loss(MSE): 20.2897 \t Train RMSE: 4.5044\n",
      "Train Epoch: 1 [20008/22024] \t Train Loss(MSE): 10.8037 \t Train RMSE: 3.2869\n",
      "Train Epoch: 1 [22008/22024] \t Train Loss(MSE): 1.9784 \t Train RMSE: 1.4065\n",
      "[Epoch: 1 \t Valid MSE: 29.8974 \t Valid RMSE: 4.9122]\n",
      "Train Epoch: 2 [8/22024] \t Train Loss(MSE): 20.2320 \t Train RMSE: 4.4980\n",
      "Train Epoch: 2 [2008/22024] \t Train Loss(MSE): 11.1658 \t Train RMSE: 3.3415\n",
      "Train Epoch: 2 [4008/22024] \t Train Loss(MSE): 2.6517 \t Train RMSE: 1.6284\n",
      "Train Epoch: 2 [6008/22024] \t Train Loss(MSE): 38.7704 \t Train RMSE: 6.2266\n",
      "Train Epoch: 2 [8008/22024] \t Train Loss(MSE): 26.6215 \t Train RMSE: 5.1596\n",
      "Train Epoch: 2 [10008/22024] \t Train Loss(MSE): 14.7814 \t Train RMSE: 3.8447\n",
      "Train Epoch: 2 [12008/22024] \t Train Loss(MSE): 6.6405 \t Train RMSE: 2.5769\n",
      "Train Epoch: 2 [14008/22024] \t Train Loss(MSE): 3.2758 \t Train RMSE: 1.8099\n",
      "Train Epoch: 2 [16008/22024] \t Train Loss(MSE): 35.4332 \t Train RMSE: 5.9526\n",
      "Train Epoch: 2 [18008/22024] \t Train Loss(MSE): 5.4888 \t Train RMSE: 2.3428\n",
      "Train Epoch: 2 [20008/22024] \t Train Loss(MSE): 10.2523 \t Train RMSE: 3.2019\n",
      "Train Epoch: 2 [22008/22024] \t Train Loss(MSE): 96.0283 \t Train RMSE: 9.7994\n",
      "[Epoch: 2 \t Valid MSE: 26.9357 \t Valid RMSE: 4.6127]\n",
      "Train Epoch: 3 [8/22024] \t Train Loss(MSE): 5.9738 \t Train RMSE: 2.4441\n",
      "Train Epoch: 3 [2008/22024] \t Train Loss(MSE): 9.9496 \t Train RMSE: 3.1543\n",
      "Train Epoch: 3 [4008/22024] \t Train Loss(MSE): 28.2178 \t Train RMSE: 5.3120\n",
      "Train Epoch: 3 [6008/22024] \t Train Loss(MSE): 21.1010 \t Train RMSE: 4.5936\n",
      "Train Epoch: 3 [8008/22024] \t Train Loss(MSE): 4.2237 \t Train RMSE: 2.0552\n",
      "Train Epoch: 3 [10008/22024] \t Train Loss(MSE): 11.7340 \t Train RMSE: 3.4255\n",
      "Train Epoch: 3 [12008/22024] \t Train Loss(MSE): 151.1940 \t Train RMSE: 12.2961\n",
      "Train Epoch: 3 [14008/22024] \t Train Loss(MSE): 12.0114 \t Train RMSE: 3.4657\n",
      "Train Epoch: 3 [16008/22024] \t Train Loss(MSE): 6.4563 \t Train RMSE: 2.5409\n",
      "Train Epoch: 3 [18008/22024] \t Train Loss(MSE): 23.2714 \t Train RMSE: 4.8240\n",
      "Train Epoch: 3 [20008/22024] \t Train Loss(MSE): 10.8143 \t Train RMSE: 3.2885\n",
      "Train Epoch: 3 [22008/22024] \t Train Loss(MSE): 8.1282 \t Train RMSE: 2.8510\n",
      "[Epoch: 3 \t Valid MSE: 46.7913 \t Valid RMSE: 6.3949]\n",
      "Train Epoch: 4 [8/22024] \t Train Loss(MSE): 24.4283 \t Train RMSE: 4.9425\n",
      "Train Epoch: 4 [2008/22024] \t Train Loss(MSE): 42.6650 \t Train RMSE: 6.5318\n",
      "Train Epoch: 4 [4008/22024] \t Train Loss(MSE): 4.7407 \t Train RMSE: 2.1773\n",
      "Train Epoch: 4 [6008/22024] \t Train Loss(MSE): 9.3050 \t Train RMSE: 3.0504\n",
      "Train Epoch: 4 [8008/22024] \t Train Loss(MSE): 4.3704 \t Train RMSE: 2.0905\n",
      "Train Epoch: 4 [10008/22024] \t Train Loss(MSE): 10.3680 \t Train RMSE: 3.2199\n",
      "Train Epoch: 4 [12008/22024] \t Train Loss(MSE): 24.9830 \t Train RMSE: 4.9983\n",
      "Train Epoch: 4 [14008/22024] \t Train Loss(MSE): 9.1322 \t Train RMSE: 3.0220\n",
      "Train Epoch: 4 [16008/22024] \t Train Loss(MSE): 3.1912 \t Train RMSE: 1.7864\n",
      "Train Epoch: 4 [18008/22024] \t Train Loss(MSE): 9.4357 \t Train RMSE: 3.0718\n",
      "Train Epoch: 4 [20008/22024] \t Train Loss(MSE): 42.9550 \t Train RMSE: 6.5540\n",
      "Train Epoch: 4 [22008/22024] \t Train Loss(MSE): 25.3405 \t Train RMSE: 5.0339\n",
      "[Epoch: 4 \t Valid MSE: 22.6834 \t Valid RMSE: 4.1964]\n",
      "Train Epoch: 5 [8/22024] \t Train Loss(MSE): 5.9626 \t Train RMSE: 2.4418\n",
      "Train Epoch: 5 [2008/22024] \t Train Loss(MSE): 2.1162 \t Train RMSE: 1.4547\n",
      "Train Epoch: 5 [4008/22024] \t Train Loss(MSE): 8.5342 \t Train RMSE: 2.9213\n",
      "Train Epoch: 5 [6008/22024] \t Train Loss(MSE): 4.8985 \t Train RMSE: 2.2133\n",
      "Train Epoch: 5 [8008/22024] \t Train Loss(MSE): 9.6629 \t Train RMSE: 3.1085\n",
      "Train Epoch: 5 [10008/22024] \t Train Loss(MSE): 5.6520 \t Train RMSE: 2.3774\n",
      "Train Epoch: 5 [12008/22024] \t Train Loss(MSE): 11.6009 \t Train RMSE: 3.4060\n",
      "Train Epoch: 5 [14008/22024] \t Train Loss(MSE): 64.3878 \t Train RMSE: 8.0242\n",
      "Train Epoch: 5 [16008/22024] \t Train Loss(MSE): 16.9423 \t Train RMSE: 4.1161\n",
      "Train Epoch: 5 [18008/22024] \t Train Loss(MSE): 38.1204 \t Train RMSE: 6.1742\n",
      "Train Epoch: 5 [20008/22024] \t Train Loss(MSE): 7.6192 \t Train RMSE: 2.7603\n",
      "Train Epoch: 5 [22008/22024] \t Train Loss(MSE): 10.9715 \t Train RMSE: 3.3123\n",
      "[Epoch: 5 \t Valid MSE: 20.9431 \t Valid RMSE: 4.0167]\n",
      "Train Epoch: 6 [8/22024] \t Train Loss(MSE): 1.4922 \t Train RMSE: 1.2216\n",
      "Train Epoch: 6 [2008/22024] \t Train Loss(MSE): 31.8192 \t Train RMSE: 5.6408\n",
      "Train Epoch: 6 [4008/22024] \t Train Loss(MSE): 5.8326 \t Train RMSE: 2.4151\n",
      "Train Epoch: 6 [6008/22024] \t Train Loss(MSE): 22.7951 \t Train RMSE: 4.7744\n",
      "Train Epoch: 6 [8008/22024] \t Train Loss(MSE): 11.6085 \t Train RMSE: 3.4071\n",
      "Train Epoch: 6 [10008/22024] \t Train Loss(MSE): 42.5569 \t Train RMSE: 6.5236\n",
      "Train Epoch: 6 [12008/22024] \t Train Loss(MSE): 17.4089 \t Train RMSE: 4.1724\n",
      "Train Epoch: 6 [14008/22024] \t Train Loss(MSE): 1.1691 \t Train RMSE: 1.0812\n",
      "Train Epoch: 6 [16008/22024] \t Train Loss(MSE): 18.2371 \t Train RMSE: 4.2705\n",
      "Train Epoch: 6 [18008/22024] \t Train Loss(MSE): 12.7073 \t Train RMSE: 3.5647\n",
      "Train Epoch: 6 [20008/22024] \t Train Loss(MSE): 3.5654 \t Train RMSE: 1.8882\n",
      "Train Epoch: 6 [22008/22024] \t Train Loss(MSE): 2.5413 \t Train RMSE: 1.5941\n",
      "[Epoch: 6 \t Valid MSE: 18.8983 \t Valid RMSE: 3.7052]\n",
      "Train Epoch: 7 [8/22024] \t Train Loss(MSE): 4.9214 \t Train RMSE: 2.2184\n",
      "Train Epoch: 7 [2008/22024] \t Train Loss(MSE): 5.8505 \t Train RMSE: 2.4188\n",
      "Train Epoch: 7 [4008/22024] \t Train Loss(MSE): 15.9286 \t Train RMSE: 3.9911\n",
      "Train Epoch: 7 [6008/22024] \t Train Loss(MSE): 1.5142 \t Train RMSE: 1.2305\n",
      "Train Epoch: 7 [8008/22024] \t Train Loss(MSE): 5.1258 \t Train RMSE: 2.2640\n",
      "Train Epoch: 7 [10008/22024] \t Train Loss(MSE): 8.5046 \t Train RMSE: 2.9163\n",
      "Train Epoch: 7 [12008/22024] \t Train Loss(MSE): 11.7573 \t Train RMSE: 3.4289\n",
      "Train Epoch: 7 [14008/22024] \t Train Loss(MSE): 5.4715 \t Train RMSE: 2.3391\n",
      "Train Epoch: 7 [16008/22024] \t Train Loss(MSE): 19.2151 \t Train RMSE: 4.3835\n",
      "Train Epoch: 7 [18008/22024] \t Train Loss(MSE): 15.3990 \t Train RMSE: 3.9242\n",
      "Train Epoch: 7 [20008/22024] \t Train Loss(MSE): 36.5552 \t Train RMSE: 6.0461\n",
      "Train Epoch: 7 [22008/22024] \t Train Loss(MSE): 28.3587 \t Train RMSE: 5.3253\n",
      "[Epoch: 7 \t Valid MSE: 25.2531 \t Valid RMSE: 4.4990]\n",
      "Train Epoch: 8 [8/22024] \t Train Loss(MSE): 13.9115 \t Train RMSE: 3.7298\n",
      "Train Epoch: 8 [2008/22024] \t Train Loss(MSE): 1.7427 \t Train RMSE: 1.3201\n",
      "Train Epoch: 8 [4008/22024] \t Train Loss(MSE): 2.3180 \t Train RMSE: 1.5225\n",
      "Train Epoch: 8 [6008/22024] \t Train Loss(MSE): 5.3418 \t Train RMSE: 2.3112\n",
      "Train Epoch: 8 [8008/22024] \t Train Loss(MSE): 17.6536 \t Train RMSE: 4.2016\n",
      "Train Epoch: 8 [10008/22024] \t Train Loss(MSE): 4.4271 \t Train RMSE: 2.1041\n",
      "Train Epoch: 8 [12008/22024] \t Train Loss(MSE): 15.7127 \t Train RMSE: 3.9639\n",
      "Train Epoch: 8 [14008/22024] \t Train Loss(MSE): 9.6869 \t Train RMSE: 3.1124\n",
      "Train Epoch: 8 [16008/22024] \t Train Loss(MSE): 13.7794 \t Train RMSE: 3.7121\n",
      "Train Epoch: 8 [18008/22024] \t Train Loss(MSE): 6.0437 \t Train RMSE: 2.4584\n",
      "Train Epoch: 8 [20008/22024] \t Train Loss(MSE): 3.1868 \t Train RMSE: 1.7852\n",
      "Train Epoch: 8 [22008/22024] \t Train Loss(MSE): 18.8395 \t Train RMSE: 4.3404\n",
      "[Epoch: 8 \t Valid MSE: 16.9486 \t Valid RMSE: 3.4129]\n",
      "Train Epoch: 9 [8/22024] \t Train Loss(MSE): 15.1065 \t Train RMSE: 3.8867\n",
      "Train Epoch: 9 [2008/22024] \t Train Loss(MSE): 6.6826 \t Train RMSE: 2.5851\n",
      "Train Epoch: 9 [4008/22024] \t Train Loss(MSE): 9.4865 \t Train RMSE: 3.0800\n",
      "Train Epoch: 9 [6008/22024] \t Train Loss(MSE): 21.7947 \t Train RMSE: 4.6685\n",
      "Train Epoch: 9 [8008/22024] \t Train Loss(MSE): 2.4511 \t Train RMSE: 1.5656\n",
      "Train Epoch: 9 [10008/22024] \t Train Loss(MSE): 3.0290 \t Train RMSE: 1.7404\n",
      "Train Epoch: 9 [12008/22024] \t Train Loss(MSE): 4.9583 \t Train RMSE: 2.2267\n",
      "Train Epoch: 9 [14008/22024] \t Train Loss(MSE): 2.3745 \t Train RMSE: 1.5409\n",
      "Train Epoch: 9 [16008/22024] \t Train Loss(MSE): 63.1257 \t Train RMSE: 7.9452\n",
      "Train Epoch: 9 [18008/22024] \t Train Loss(MSE): 5.4098 \t Train RMSE: 2.3259\n",
      "Train Epoch: 9 [20008/22024] \t Train Loss(MSE): 22.0461 \t Train RMSE: 4.6953\n",
      "Train Epoch: 9 [22008/22024] \t Train Loss(MSE): 6.0014 \t Train RMSE: 2.4498\n",
      "[Epoch: 9 \t Valid MSE: 17.4907 \t Valid RMSE: 3.5087]\n",
      "Train Epoch: 10 [8/22024] \t Train Loss(MSE): 43.9588 \t Train RMSE: 6.6301\n",
      "Train Epoch: 10 [2008/22024] \t Train Loss(MSE): 6.6755 \t Train RMSE: 2.5837\n",
      "Train Epoch: 10 [4008/22024] \t Train Loss(MSE): 3.9307 \t Train RMSE: 1.9826\n",
      "Train Epoch: 10 [6008/22024] \t Train Loss(MSE): 7.7011 \t Train RMSE: 2.7751\n",
      "Train Epoch: 10 [8008/22024] \t Train Loss(MSE): 8.9516 \t Train RMSE: 2.9919\n",
      "Train Epoch: 10 [10008/22024] \t Train Loss(MSE): 10.2638 \t Train RMSE: 3.2037\n",
      "Train Epoch: 10 [12008/22024] \t Train Loss(MSE): 8.5761 \t Train RMSE: 2.9285\n",
      "Train Epoch: 10 [14008/22024] \t Train Loss(MSE): 2.3072 \t Train RMSE: 1.5189\n",
      "Train Epoch: 10 [16008/22024] \t Train Loss(MSE): 2.8371 \t Train RMSE: 1.6844\n",
      "Train Epoch: 10 [18008/22024] \t Train Loss(MSE): 41.2902 \t Train RMSE: 6.4257\n",
      "Train Epoch: 10 [20008/22024] \t Train Loss(MSE): 39.7812 \t Train RMSE: 6.3072\n",
      "Train Epoch: 10 [22008/22024] \t Train Loss(MSE): 15.7042 \t Train RMSE: 3.9628\n",
      "[Epoch: 10 \t Valid MSE: 18.2169 \t Valid RMSE: 3.6608]\n",
      "Train Epoch: 11 [8/22024] \t Train Loss(MSE): 17.1978 \t Train RMSE: 4.1470\n",
      "Train Epoch: 11 [2008/22024] \t Train Loss(MSE): 1.3359 \t Train RMSE: 1.1558\n",
      "Train Epoch: 11 [4008/22024] \t Train Loss(MSE): 8.8884 \t Train RMSE: 2.9813\n",
      "Train Epoch: 11 [6008/22024] \t Train Loss(MSE): 7.5136 \t Train RMSE: 2.7411\n",
      "Train Epoch: 11 [8008/22024] \t Train Loss(MSE): 53.2951 \t Train RMSE: 7.3003\n",
      "Train Epoch: 11 [10008/22024] \t Train Loss(MSE): 9.5362 \t Train RMSE: 3.0881\n",
      "Train Epoch: 11 [12008/22024] \t Train Loss(MSE): 3.7033 \t Train RMSE: 1.9244\n",
      "Train Epoch: 11 [14008/22024] \t Train Loss(MSE): 4.4285 \t Train RMSE: 2.1044\n",
      "Train Epoch: 11 [16008/22024] \t Train Loss(MSE): 19.1707 \t Train RMSE: 4.3784\n",
      "Train Epoch: 11 [18008/22024] \t Train Loss(MSE): 24.1123 \t Train RMSE: 4.9104\n",
      "Train Epoch: 11 [20008/22024] \t Train Loss(MSE): 3.0404 \t Train RMSE: 1.7437\n",
      "Train Epoch: 11 [22008/22024] \t Train Loss(MSE): 14.9649 \t Train RMSE: 3.8685\n",
      "[Epoch: 11 \t Valid MSE: 20.0651 \t Valid RMSE: 3.8575]\n",
      "Train Epoch: 12 [8/22024] \t Train Loss(MSE): 6.6899 \t Train RMSE: 2.5865\n",
      "Train Epoch: 12 [2008/22024] \t Train Loss(MSE): 5.2268 \t Train RMSE: 2.2862\n",
      "Train Epoch: 12 [4008/22024] \t Train Loss(MSE): 40.4191 \t Train RMSE: 6.3576\n",
      "Train Epoch: 12 [6008/22024] \t Train Loss(MSE): 5.5722 \t Train RMSE: 2.3605\n",
      "Train Epoch: 12 [8008/22024] \t Train Loss(MSE): 12.2742 \t Train RMSE: 3.5035\n",
      "Train Epoch: 12 [10008/22024] \t Train Loss(MSE): 7.0924 \t Train RMSE: 2.6632\n",
      "Train Epoch: 12 [12008/22024] \t Train Loss(MSE): 1.5937 \t Train RMSE: 1.2624\n",
      "Train Epoch: 12 [14008/22024] \t Train Loss(MSE): 2.7907 \t Train RMSE: 1.6705\n",
      "Train Epoch: 12 [16008/22024] \t Train Loss(MSE): 22.3663 \t Train RMSE: 4.7293\n",
      "Train Epoch: 12 [18008/22024] \t Train Loss(MSE): 8.5913 \t Train RMSE: 2.9311\n",
      "Train Epoch: 12 [20008/22024] \t Train Loss(MSE): 12.3845 \t Train RMSE: 3.5192\n",
      "Train Epoch: 12 [22008/22024] \t Train Loss(MSE): 2.0344 \t Train RMSE: 1.4263\n",
      "[Epoch: 12 \t Valid MSE: 19.2251 \t Valid RMSE: 3.8120]\n",
      "Train Epoch: 13 [8/22024] \t Train Loss(MSE): 7.4344 \t Train RMSE: 2.7266\n",
      "Train Epoch: 13 [2008/22024] \t Train Loss(MSE): 7.0635 \t Train RMSE: 2.6577\n",
      "Train Epoch: 13 [4008/22024] \t Train Loss(MSE): 10.7224 \t Train RMSE: 3.2745\n",
      "Train Epoch: 13 [6008/22024] \t Train Loss(MSE): 18.9377 \t Train RMSE: 4.3517\n",
      "Train Epoch: 13 [8008/22024] \t Train Loss(MSE): 6.2413 \t Train RMSE: 2.4983\n",
      "Train Epoch: 13 [10008/22024] \t Train Loss(MSE): 7.1353 \t Train RMSE: 2.6712\n",
      "Train Epoch: 13 [12008/22024] \t Train Loss(MSE): 7.2241 \t Train RMSE: 2.6878\n",
      "Train Epoch: 13 [14008/22024] \t Train Loss(MSE): 7.9611 \t Train RMSE: 2.8215\n",
      "Train Epoch: 13 [16008/22024] \t Train Loss(MSE): 22.3529 \t Train RMSE: 4.7279\n",
      "Train Epoch: 13 [18008/22024] \t Train Loss(MSE): 5.8618 \t Train RMSE: 2.4211\n",
      "Train Epoch: 13 [20008/22024] \t Train Loss(MSE): 7.7629 \t Train RMSE: 2.7862\n",
      "Train Epoch: 13 [22008/22024] \t Train Loss(MSE): 10.8620 \t Train RMSE: 3.2958\n",
      "[Epoch: 13 \t Valid MSE: 17.3263 \t Valid RMSE: 3.5318]\n",
      "Train Epoch: 14 [8/22024] \t Train Loss(MSE): 25.0793 \t Train RMSE: 5.0079\n",
      "Train Epoch: 14 [2008/22024] \t Train Loss(MSE): 8.0096 \t Train RMSE: 2.8301\n",
      "Train Epoch: 14 [4008/22024] \t Train Loss(MSE): 46.0694 \t Train RMSE: 6.7874\n",
      "Train Epoch: 14 [6008/22024] \t Train Loss(MSE): 16.0529 \t Train RMSE: 4.0066\n",
      "Train Epoch: 14 [8008/22024] \t Train Loss(MSE): 3.9433 \t Train RMSE: 1.9858\n",
      "Train Epoch: 14 [10008/22024] \t Train Loss(MSE): 9.1922 \t Train RMSE: 3.0319\n",
      "Train Epoch: 14 [12008/22024] \t Train Loss(MSE): 2.2190 \t Train RMSE: 1.4896\n",
      "Train Epoch: 14 [14008/22024] \t Train Loss(MSE): 9.9468 \t Train RMSE: 3.1538\n",
      "Train Epoch: 14 [16008/22024] \t Train Loss(MSE): 2.8667 \t Train RMSE: 1.6931\n",
      "Train Epoch: 14 [18008/22024] \t Train Loss(MSE): 29.4617 \t Train RMSE: 5.4279\n",
      "Train Epoch: 14 [20008/22024] \t Train Loss(MSE): 12.8319 \t Train RMSE: 3.5822\n",
      "Train Epoch: 14 [22008/22024] \t Train Loss(MSE): 2.9625 \t Train RMSE: 1.7212\n",
      "[Epoch: 14 \t Valid MSE: 22.8030 \t Valid RMSE: 4.2023]\n",
      "Train Epoch: 15 [8/22024] \t Train Loss(MSE): 2.3191 \t Train RMSE: 1.5228\n",
      "Train Epoch: 15 [2008/22024] \t Train Loss(MSE): 56.4684 \t Train RMSE: 7.5145\n",
      "Train Epoch: 15 [4008/22024] \t Train Loss(MSE): 10.9409 \t Train RMSE: 3.3077\n",
      "Train Epoch: 15 [6008/22024] \t Train Loss(MSE): 54.0940 \t Train RMSE: 7.3549\n",
      "Train Epoch: 15 [8008/22024] \t Train Loss(MSE): 4.1204 \t Train RMSE: 2.0299\n",
      "Train Epoch: 15 [10008/22024] \t Train Loss(MSE): 18.3268 \t Train RMSE: 4.2810\n",
      "Train Epoch: 15 [12008/22024] \t Train Loss(MSE): 3.4063 \t Train RMSE: 1.8456\n",
      "Train Epoch: 15 [14008/22024] \t Train Loss(MSE): 3.7982 \t Train RMSE: 1.9489\n",
      "Train Epoch: 15 [16008/22024] \t Train Loss(MSE): 22.8495 \t Train RMSE: 4.7801\n",
      "Train Epoch: 15 [18008/22024] \t Train Loss(MSE): 19.4326 \t Train RMSE: 4.4082\n",
      "Train Epoch: 15 [20008/22024] \t Train Loss(MSE): 4.6230 \t Train RMSE: 2.1501\n",
      "Train Epoch: 15 [22008/22024] \t Train Loss(MSE): 5.6316 \t Train RMSE: 2.3731\n",
      "[Epoch: 15 \t Valid MSE: 17.5686 \t Valid RMSE: 3.5559]\n",
      "Train Epoch: 16 [8/22024] \t Train Loss(MSE): 19.6950 \t Train RMSE: 4.4379\n",
      "Train Epoch: 16 [2008/22024] \t Train Loss(MSE): 6.5023 \t Train RMSE: 2.5500\n",
      "Train Epoch: 16 [4008/22024] \t Train Loss(MSE): 9.3569 \t Train RMSE: 3.0589\n",
      "Train Epoch: 16 [6008/22024] \t Train Loss(MSE): 6.1354 \t Train RMSE: 2.4770\n",
      "Train Epoch: 16 [8008/22024] \t Train Loss(MSE): 33.8855 \t Train RMSE: 5.8211\n",
      "Train Epoch: 16 [10008/22024] \t Train Loss(MSE): 52.2739 \t Train RMSE: 7.2301\n",
      "Train Epoch: 16 [12008/22024] \t Train Loss(MSE): 9.0450 \t Train RMSE: 3.0075\n",
      "Train Epoch: 16 [14008/22024] \t Train Loss(MSE): 26.7033 \t Train RMSE: 5.1675\n",
      "Train Epoch: 16 [16008/22024] \t Train Loss(MSE): 6.4680 \t Train RMSE: 2.5432\n",
      "Train Epoch: 16 [18008/22024] \t Train Loss(MSE): 14.8195 \t Train RMSE: 3.8496\n",
      "Train Epoch: 16 [20008/22024] \t Train Loss(MSE): 7.3935 \t Train RMSE: 2.7191\n",
      "Train Epoch: 16 [22008/22024] \t Train Loss(MSE): 3.6013 \t Train RMSE: 1.8977\n",
      "[Epoch: 16 \t Valid MSE: 18.3194 \t Valid RMSE: 3.6136]\n",
      "Train Epoch: 17 [8/22024] \t Train Loss(MSE): 10.8740 \t Train RMSE: 3.2976\n",
      "Train Epoch: 17 [2008/22024] \t Train Loss(MSE): 1.0845 \t Train RMSE: 1.0414\n",
      "Train Epoch: 17 [4008/22024] \t Train Loss(MSE): 16.3437 \t Train RMSE: 4.0427\n",
      "Train Epoch: 17 [6008/22024] \t Train Loss(MSE): 2.7996 \t Train RMSE: 1.6732\n",
      "Train Epoch: 17 [8008/22024] \t Train Loss(MSE): 4.8337 \t Train RMSE: 2.1986\n",
      "Train Epoch: 17 [10008/22024] \t Train Loss(MSE): 25.4887 \t Train RMSE: 5.0486\n",
      "Train Epoch: 17 [12008/22024] \t Train Loss(MSE): 2.6235 \t Train RMSE: 1.6197\n",
      "Train Epoch: 17 [14008/22024] \t Train Loss(MSE): 50.5009 \t Train RMSE: 7.1064\n",
      "Train Epoch: 17 [16008/22024] \t Train Loss(MSE): 6.1961 \t Train RMSE: 2.4892\n",
      "Train Epoch: 17 [18008/22024] \t Train Loss(MSE): 34.7412 \t Train RMSE: 5.8942\n",
      "Train Epoch: 17 [20008/22024] \t Train Loss(MSE): 28.2673 \t Train RMSE: 5.3167\n",
      "Train Epoch: 17 [22008/22024] \t Train Loss(MSE): 4.8489 \t Train RMSE: 2.2020\n",
      "[Epoch: 17 \t Valid MSE: 21.2038 \t Valid RMSE: 3.9883]\n",
      "Train Epoch: 18 [8/22024] \t Train Loss(MSE): 5.5249 \t Train RMSE: 2.3505\n",
      "Train Epoch: 18 [2008/22024] \t Train Loss(MSE): 3.4309 \t Train RMSE: 1.8523\n",
      "Train Epoch: 18 [4008/22024] \t Train Loss(MSE): 29.5638 \t Train RMSE: 5.4373\n",
      "Train Epoch: 18 [6008/22024] \t Train Loss(MSE): 8.2919 \t Train RMSE: 2.8796\n",
      "Train Epoch: 18 [8008/22024] \t Train Loss(MSE): 10.2477 \t Train RMSE: 3.2012\n",
      "Train Epoch: 18 [10008/22024] \t Train Loss(MSE): 3.6073 \t Train RMSE: 1.8993\n",
      "Train Epoch: 18 [12008/22024] \t Train Loss(MSE): 1.9018 \t Train RMSE: 1.3791\n",
      "Train Epoch: 18 [14008/22024] \t Train Loss(MSE): 17.9316 \t Train RMSE: 4.2346\n",
      "Train Epoch: 18 [16008/22024] \t Train Loss(MSE): 18.9912 \t Train RMSE: 4.3579\n",
      "Train Epoch: 18 [18008/22024] \t Train Loss(MSE): 9.0242 \t Train RMSE: 3.0040\n",
      "Train Epoch: 18 [20008/22024] \t Train Loss(MSE): 27.8254 \t Train RMSE: 5.2750\n",
      "Train Epoch: 18 [22008/22024] \t Train Loss(MSE): 12.2258 \t Train RMSE: 3.4965\n",
      "[Epoch: 18 \t Valid MSE: 28.3090 \t Valid RMSE: 4.8572]\n",
      "Train Epoch: 19 [8/22024] \t Train Loss(MSE): 9.6203 \t Train RMSE: 3.1017\n",
      "Train Epoch: 19 [2008/22024] \t Train Loss(MSE): 34.1278 \t Train RMSE: 5.8419\n",
      "Train Epoch: 19 [4008/22024] \t Train Loss(MSE): 12.0838 \t Train RMSE: 3.4762\n",
      "Train Epoch: 19 [6008/22024] \t Train Loss(MSE): 11.8126 \t Train RMSE: 3.4369\n",
      "Train Epoch: 19 [8008/22024] \t Train Loss(MSE): 20.0162 \t Train RMSE: 4.4739\n",
      "Train Epoch: 19 [10008/22024] \t Train Loss(MSE): 17.1588 \t Train RMSE: 4.1423\n",
      "Train Epoch: 19 [12008/22024] \t Train Loss(MSE): 24.7444 \t Train RMSE: 4.9744\n",
      "Train Epoch: 19 [14008/22024] \t Train Loss(MSE): 14.8424 \t Train RMSE: 3.8526\n",
      "Train Epoch: 19 [16008/22024] \t Train Loss(MSE): 2.9505 \t Train RMSE: 1.7177\n",
      "Train Epoch: 19 [18008/22024] \t Train Loss(MSE): 6.2205 \t Train RMSE: 2.4941\n",
      "Train Epoch: 19 [20008/22024] \t Train Loss(MSE): 82.8852 \t Train RMSE: 9.1041\n",
      "Train Epoch: 19 [22008/22024] \t Train Loss(MSE): 11.2877 \t Train RMSE: 3.3597\n",
      "[Epoch: 19 \t Valid MSE: 17.4769 \t Valid RMSE: 3.4761]\n",
      "Train Epoch: 20 [8/22024] \t Train Loss(MSE): 7.3962 \t Train RMSE: 2.7196\n",
      "Train Epoch: 20 [2008/22024] \t Train Loss(MSE): 3.4855 \t Train RMSE: 1.8670\n",
      "Train Epoch: 20 [4008/22024] \t Train Loss(MSE): 3.6295 \t Train RMSE: 1.9051\n",
      "Train Epoch: 20 [6008/22024] \t Train Loss(MSE): 10.3489 \t Train RMSE: 3.2170\n",
      "Train Epoch: 20 [8008/22024] \t Train Loss(MSE): 14.5091 \t Train RMSE: 3.8091\n",
      "Train Epoch: 20 [10008/22024] \t Train Loss(MSE): 8.6561 \t Train RMSE: 2.9421\n",
      "Train Epoch: 20 [12008/22024] \t Train Loss(MSE): 12.6737 \t Train RMSE: 3.5600\n",
      "Train Epoch: 20 [14008/22024] \t Train Loss(MSE): 2.6000 \t Train RMSE: 1.6125\n",
      "Train Epoch: 20 [16008/22024] \t Train Loss(MSE): 16.7878 \t Train RMSE: 4.0973\n",
      "Train Epoch: 20 [18008/22024] \t Train Loss(MSE): 62.9456 \t Train RMSE: 7.9338\n",
      "Train Epoch: 20 [20008/22024] \t Train Loss(MSE): 11.3642 \t Train RMSE: 3.3711\n",
      "Train Epoch: 20 [22008/22024] \t Train Loss(MSE): 5.7154 \t Train RMSE: 2.3907\n",
      "[Epoch: 20 \t Valid MSE: 20.4913 \t Valid RMSE: 3.9622]\n",
      "Train Epoch: 21 [8/22024] \t Train Loss(MSE): 10.1096 \t Train RMSE: 3.1796\n",
      "Train Epoch: 21 [2008/22024] \t Train Loss(MSE): 8.9908 \t Train RMSE: 2.9985\n",
      "Train Epoch: 21 [4008/22024] \t Train Loss(MSE): 3.1362 \t Train RMSE: 1.7709\n",
      "Train Epoch: 21 [6008/22024] \t Train Loss(MSE): 41.3054 \t Train RMSE: 6.4269\n",
      "Train Epoch: 21 [8008/22024] \t Train Loss(MSE): 8.2278 \t Train RMSE: 2.8684\n",
      "Train Epoch: 21 [10008/22024] \t Train Loss(MSE): 23.0171 \t Train RMSE: 4.7976\n",
      "Train Epoch: 21 [12008/22024] \t Train Loss(MSE): 3.2327 \t Train RMSE: 1.7980\n",
      "Train Epoch: 21 [14008/22024] \t Train Loss(MSE): 4.5471 \t Train RMSE: 2.1324\n",
      "Train Epoch: 21 [16008/22024] \t Train Loss(MSE): 19.9722 \t Train RMSE: 4.4690\n",
      "Train Epoch: 21 [18008/22024] \t Train Loss(MSE): 5.2671 \t Train RMSE: 2.2950\n",
      "Train Epoch: 21 [20008/22024] \t Train Loss(MSE): 56.4193 \t Train RMSE: 7.5113\n",
      "Train Epoch: 21 [22008/22024] \t Train Loss(MSE): 13.8300 \t Train RMSE: 3.7189\n",
      "[Epoch: 21 \t Valid MSE: 23.6931 \t Valid RMSE: 4.3352]\n",
      "Train Epoch: 22 [8/22024] \t Train Loss(MSE): 7.9608 \t Train RMSE: 2.8215\n",
      "Train Epoch: 22 [2008/22024] \t Train Loss(MSE): 10.3021 \t Train RMSE: 3.2097\n",
      "Train Epoch: 22 [4008/22024] \t Train Loss(MSE): 1.5656 \t Train RMSE: 1.2512\n",
      "Train Epoch: 22 [6008/22024] \t Train Loss(MSE): 9.9181 \t Train RMSE: 3.1493\n",
      "Train Epoch: 22 [8008/22024] \t Train Loss(MSE): 3.7113 \t Train RMSE: 1.9265\n",
      "Train Epoch: 22 [10008/22024] \t Train Loss(MSE): 0.8888 \t Train RMSE: 0.9428\n",
      "Train Epoch: 22 [12008/22024] \t Train Loss(MSE): 5.1928 \t Train RMSE: 2.2788\n",
      "Train Epoch: 22 [14008/22024] \t Train Loss(MSE): 9.9828 \t Train RMSE: 3.1596\n",
      "Train Epoch: 22 [16008/22024] \t Train Loss(MSE): 3.0661 \t Train RMSE: 1.7510\n",
      "Train Epoch: 22 [18008/22024] \t Train Loss(MSE): 2.1806 \t Train RMSE: 1.4767\n",
      "Train Epoch: 22 [20008/22024] \t Train Loss(MSE): 4.8053 \t Train RMSE: 2.1921\n",
      "Train Epoch: 22 [22008/22024] \t Train Loss(MSE): 24.2733 \t Train RMSE: 4.9268\n",
      "[Epoch: 22 \t Valid MSE: 23.0642 \t Valid RMSE: 4.2427]\n",
      "Train Epoch: 23 [8/22024] \t Train Loss(MSE): 8.0122 \t Train RMSE: 2.8306\n",
      "Train Epoch: 23 [2008/22024] \t Train Loss(MSE): 17.5001 \t Train RMSE: 4.1833\n",
      "Train Epoch: 23 [4008/22024] \t Train Loss(MSE): 3.7022 \t Train RMSE: 1.9241\n",
      "Train Epoch: 23 [6008/22024] \t Train Loss(MSE): 6.3146 \t Train RMSE: 2.5129\n",
      "Train Epoch: 23 [8008/22024] \t Train Loss(MSE): 6.7890 \t Train RMSE: 2.6056\n",
      "Train Epoch: 23 [10008/22024] \t Train Loss(MSE): 6.5207 \t Train RMSE: 2.5536\n",
      "Train Epoch: 23 [12008/22024] \t Train Loss(MSE): 22.9559 \t Train RMSE: 4.7912\n",
      "Train Epoch: 23 [14008/22024] \t Train Loss(MSE): 7.3087 \t Train RMSE: 2.7035\n",
      "Train Epoch: 23 [16008/22024] \t Train Loss(MSE): 3.4505 \t Train RMSE: 1.8575\n",
      "Train Epoch: 23 [18008/22024] \t Train Loss(MSE): 9.6414 \t Train RMSE: 3.1051\n",
      "Train Epoch: 23 [20008/22024] \t Train Loss(MSE): 5.0037 \t Train RMSE: 2.2369\n",
      "Train Epoch: 23 [22008/22024] \t Train Loss(MSE): 21.1535 \t Train RMSE: 4.5993\n",
      "[Epoch: 23 \t Valid MSE: 20.1829 \t Valid RMSE: 3.8760]\n",
      "Train Epoch: 24 [8/22024] \t Train Loss(MSE): 8.7332 \t Train RMSE: 2.9552\n",
      "Train Epoch: 24 [2008/22024] \t Train Loss(MSE): 3.6820 \t Train RMSE: 1.9189\n",
      "Train Epoch: 24 [4008/22024] \t Train Loss(MSE): 8.6719 \t Train RMSE: 2.9448\n",
      "Train Epoch: 24 [6008/22024] \t Train Loss(MSE): 7.6515 \t Train RMSE: 2.7661\n",
      "Train Epoch: 24 [8008/22024] \t Train Loss(MSE): 2.8833 \t Train RMSE: 1.6980\n",
      "Train Epoch: 24 [10008/22024] \t Train Loss(MSE): 16.8805 \t Train RMSE: 4.1086\n",
      "Train Epoch: 24 [12008/22024] \t Train Loss(MSE): 4.2025 \t Train RMSE: 2.0500\n",
      "Train Epoch: 24 [14008/22024] \t Train Loss(MSE): 6.4896 \t Train RMSE: 2.5475\n",
      "Train Epoch: 24 [16008/22024] \t Train Loss(MSE): 5.6960 \t Train RMSE: 2.3866\n",
      "Train Epoch: 24 [18008/22024] \t Train Loss(MSE): 4.3734 \t Train RMSE: 2.0913\n",
      "Train Epoch: 24 [20008/22024] \t Train Loss(MSE): 16.5829 \t Train RMSE: 4.0722\n",
      "Train Epoch: 24 [22008/22024] \t Train Loss(MSE): 5.0366 \t Train RMSE: 2.2442\n",
      "[Epoch: 24 \t Valid MSE: 17.2827 \t Valid RMSE: 3.5182]\n",
      "Train Epoch: 25 [8/22024] \t Train Loss(MSE): 1.9293 \t Train RMSE: 1.3890\n",
      "Train Epoch: 25 [2008/22024] \t Train Loss(MSE): 4.7114 \t Train RMSE: 2.1706\n",
      "Train Epoch: 25 [4008/22024] \t Train Loss(MSE): 6.8689 \t Train RMSE: 2.6209\n",
      "Train Epoch: 25 [6008/22024] \t Train Loss(MSE): 10.9865 \t Train RMSE: 3.3146\n",
      "Train Epoch: 25 [8008/22024] \t Train Loss(MSE): 14.6567 \t Train RMSE: 3.8284\n",
      "Train Epoch: 25 [10008/22024] \t Train Loss(MSE): 0.9021 \t Train RMSE: 0.9498\n",
      "Train Epoch: 25 [12008/22024] \t Train Loss(MSE): 51.9107 \t Train RMSE: 7.2049\n",
      "Train Epoch: 25 [14008/22024] \t Train Loss(MSE): 20.6489 \t Train RMSE: 4.5441\n",
      "Train Epoch: 25 [16008/22024] \t Train Loss(MSE): 32.8788 \t Train RMSE: 5.7340\n",
      "Train Epoch: 25 [18008/22024] \t Train Loss(MSE): 1.4572 \t Train RMSE: 1.2071\n",
      "Train Epoch: 25 [20008/22024] \t Train Loss(MSE): 5.1950 \t Train RMSE: 2.2792\n",
      "Train Epoch: 25 [22008/22024] \t Train Loss(MSE): 47.8196 \t Train RMSE: 6.9152\n",
      "[Epoch: 25 \t Valid MSE: 18.6131 \t Valid RMSE: 3.7461]\n",
      "Train Epoch: 26 [8/22024] \t Train Loss(MSE): 3.1882 \t Train RMSE: 1.7856\n",
      "Train Epoch: 26 [2008/22024] \t Train Loss(MSE): 5.0704 \t Train RMSE: 2.2517\n",
      "Train Epoch: 26 [4008/22024] \t Train Loss(MSE): 10.2015 \t Train RMSE: 3.1940\n",
      "Train Epoch: 26 [6008/22024] \t Train Loss(MSE): 21.1315 \t Train RMSE: 4.5969\n",
      "Train Epoch: 26 [8008/22024] \t Train Loss(MSE): 8.7720 \t Train RMSE: 2.9618\n",
      "Train Epoch: 26 [10008/22024] \t Train Loss(MSE): 15.4698 \t Train RMSE: 3.9332\n",
      "Train Epoch: 26 [12008/22024] \t Train Loss(MSE): 1.7624 \t Train RMSE: 1.3276\n",
      "Train Epoch: 26 [14008/22024] \t Train Loss(MSE): 40.3914 \t Train RMSE: 6.3554\n",
      "Train Epoch: 26 [16008/22024] \t Train Loss(MSE): 14.2515 \t Train RMSE: 3.7751\n",
      "Train Epoch: 26 [18008/22024] \t Train Loss(MSE): 3.8573 \t Train RMSE: 1.9640\n",
      "Train Epoch: 26 [20008/22024] \t Train Loss(MSE): 4.1775 \t Train RMSE: 2.0439\n",
      "Train Epoch: 26 [22008/22024] \t Train Loss(MSE): 8.7142 \t Train RMSE: 2.9520\n",
      "[Epoch: 26 \t Valid MSE: 31.8726 \t Valid RMSE: 5.1751]\n",
      "Train Epoch: 27 [8/22024] \t Train Loss(MSE): 15.7647 \t Train RMSE: 3.9705\n",
      "Train Epoch: 27 [2008/22024] \t Train Loss(MSE): 6.8373 \t Train RMSE: 2.6148\n",
      "Train Epoch: 27 [4008/22024] \t Train Loss(MSE): 1.7606 \t Train RMSE: 1.3269\n",
      "Train Epoch: 27 [6008/22024] \t Train Loss(MSE): 6.0637 \t Train RMSE: 2.4625\n",
      "Train Epoch: 27 [8008/22024] \t Train Loss(MSE): 24.2255 \t Train RMSE: 4.9219\n",
      "Train Epoch: 27 [10008/22024] \t Train Loss(MSE): 2.2301 \t Train RMSE: 1.4934\n",
      "Train Epoch: 27 [12008/22024] \t Train Loss(MSE): 1.5868 \t Train RMSE: 1.2597\n",
      "Train Epoch: 27 [14008/22024] \t Train Loss(MSE): 42.3253 \t Train RMSE: 6.5058\n",
      "Train Epoch: 27 [16008/22024] \t Train Loss(MSE): 16.6484 \t Train RMSE: 4.0802\n",
      "Train Epoch: 27 [18008/22024] \t Train Loss(MSE): 9.3081 \t Train RMSE: 3.0509\n",
      "Train Epoch: 27 [20008/22024] \t Train Loss(MSE): 3.2995 \t Train RMSE: 1.8165\n",
      "Train Epoch: 27 [22008/22024] \t Train Loss(MSE): 2.0197 \t Train RMSE: 1.4212\n",
      "[Epoch: 27 \t Valid MSE: 17.5214 \t Valid RMSE: 3.5621]\n",
      "Train Epoch: 28 [8/22024] \t Train Loss(MSE): 5.2660 \t Train RMSE: 2.2948\n",
      "Train Epoch: 28 [2008/22024] \t Train Loss(MSE): 1.5273 \t Train RMSE: 1.2359\n",
      "Train Epoch: 28 [4008/22024] \t Train Loss(MSE): 5.3646 \t Train RMSE: 2.3162\n",
      "Train Epoch: 28 [6008/22024] \t Train Loss(MSE): 4.3365 \t Train RMSE: 2.0824\n",
      "Train Epoch: 28 [8008/22024] \t Train Loss(MSE): 4.6501 \t Train RMSE: 2.1564\n",
      "Train Epoch: 28 [10008/22024] \t Train Loss(MSE): 2.9062 \t Train RMSE: 1.7048\n",
      "Train Epoch: 28 [12008/22024] \t Train Loss(MSE): 4.8822 \t Train RMSE: 2.2096\n",
      "Train Epoch: 28 [14008/22024] \t Train Loss(MSE): 2.4973 \t Train RMSE: 1.5803\n",
      "Train Epoch: 28 [16008/22024] \t Train Loss(MSE): 16.5918 \t Train RMSE: 4.0733\n",
      "Train Epoch: 28 [18008/22024] \t Train Loss(MSE): 4.1319 \t Train RMSE: 2.0327\n",
      "Train Epoch: 28 [20008/22024] \t Train Loss(MSE): 133.5969 \t Train RMSE: 11.5584\n",
      "Train Epoch: 28 [22008/22024] \t Train Loss(MSE): 1.4146 \t Train RMSE: 1.1894\n",
      "[Epoch: 28 \t Valid MSE: 25.4883 \t Valid RMSE: 4.5797]\n",
      "Train Epoch: 29 [8/22024] \t Train Loss(MSE): 11.8088 \t Train RMSE: 3.4364\n",
      "Train Epoch: 29 [2008/22024] \t Train Loss(MSE): 4.8248 \t Train RMSE: 2.1965\n",
      "Train Epoch: 29 [4008/22024] \t Train Loss(MSE): 63.8288 \t Train RMSE: 7.9893\n",
      "Train Epoch: 29 [6008/22024] \t Train Loss(MSE): 134.2880 \t Train RMSE: 11.5883\n",
      "Train Epoch: 29 [8008/22024] \t Train Loss(MSE): 17.6559 \t Train RMSE: 4.2019\n",
      "Train Epoch: 29 [10008/22024] \t Train Loss(MSE): 9.4536 \t Train RMSE: 3.0747\n",
      "Train Epoch: 29 [12008/22024] \t Train Loss(MSE): 10.3291 \t Train RMSE: 3.2139\n",
      "Train Epoch: 29 [14008/22024] \t Train Loss(MSE): 26.0411 \t Train RMSE: 5.1030\n",
      "Train Epoch: 29 [16008/22024] \t Train Loss(MSE): 76.3018 \t Train RMSE: 8.7351\n",
      "Train Epoch: 29 [18008/22024] \t Train Loss(MSE): 28.8025 \t Train RMSE: 5.3668\n",
      "Train Epoch: 29 [20008/22024] \t Train Loss(MSE): 4.7865 \t Train RMSE: 2.1878\n",
      "Train Epoch: 29 [22008/22024] \t Train Loss(MSE): 27.7567 \t Train RMSE: 5.2685\n",
      "[Epoch: 29 \t Valid MSE: 27.2476 \t Valid RMSE: 4.6765]\n",
      "Train Epoch: 30 [8/22024] \t Train Loss(MSE): 7.2506 \t Train RMSE: 2.6927\n",
      "Train Epoch: 30 [2008/22024] \t Train Loss(MSE): 15.1680 \t Train RMSE: 3.8946\n",
      "Train Epoch: 30 [4008/22024] \t Train Loss(MSE): 3.3691 \t Train RMSE: 1.8355\n",
      "Train Epoch: 30 [6008/22024] \t Train Loss(MSE): 17.2440 \t Train RMSE: 4.1526\n",
      "Train Epoch: 30 [8008/22024] \t Train Loss(MSE): 11.0614 \t Train RMSE: 3.3259\n",
      "Train Epoch: 30 [10008/22024] \t Train Loss(MSE): 3.2812 \t Train RMSE: 1.8114\n",
      "Train Epoch: 30 [12008/22024] \t Train Loss(MSE): 7.8040 \t Train RMSE: 2.7936\n",
      "Train Epoch: 30 [14008/22024] \t Train Loss(MSE): 40.2302 \t Train RMSE: 6.3427\n",
      "Train Epoch: 30 [16008/22024] \t Train Loss(MSE): 76.6872 \t Train RMSE: 8.7571\n",
      "Train Epoch: 30 [18008/22024] \t Train Loss(MSE): 12.3254 \t Train RMSE: 3.5108\n",
      "Train Epoch: 30 [20008/22024] \t Train Loss(MSE): 2.2337 \t Train RMSE: 1.4945\n",
      "Train Epoch: 30 [22008/22024] \t Train Loss(MSE): 22.1018 \t Train RMSE: 4.7013\n",
      "[Epoch: 30 \t Valid MSE: 21.0670 \t Valid RMSE: 4.0316]\n",
      "Train Epoch: 31 [8/22024] \t Train Loss(MSE): 1.8700 \t Train RMSE: 1.3675\n",
      "Train Epoch: 31 [2008/22024] \t Train Loss(MSE): 9.8759 \t Train RMSE: 3.1426\n",
      "Train Epoch: 31 [4008/22024] \t Train Loss(MSE): 35.6286 \t Train RMSE: 5.9690\n",
      "Train Epoch: 31 [6008/22024] \t Train Loss(MSE): 12.3569 \t Train RMSE: 3.5152\n",
      "Train Epoch: 31 [8008/22024] \t Train Loss(MSE): 2.4406 \t Train RMSE: 1.5622\n",
      "Train Epoch: 31 [10008/22024] \t Train Loss(MSE): 15.4036 \t Train RMSE: 3.9247\n",
      "Train Epoch: 31 [12008/22024] \t Train Loss(MSE): 2.1812 \t Train RMSE: 1.4769\n",
      "Train Epoch: 31 [14008/22024] \t Train Loss(MSE): 19.3910 \t Train RMSE: 4.4035\n",
      "Train Epoch: 31 [16008/22024] \t Train Loss(MSE): 24.5246 \t Train RMSE: 4.9522\n",
      "Train Epoch: 31 [18008/22024] \t Train Loss(MSE): 1.6574 \t Train RMSE: 1.2874\n",
      "Train Epoch: 31 [20008/22024] \t Train Loss(MSE): 2.7371 \t Train RMSE: 1.6544\n",
      "Train Epoch: 31 [22008/22024] \t Train Loss(MSE): 1.6402 \t Train RMSE: 1.2807\n",
      "[Epoch: 31 \t Valid MSE: 20.2240 \t Valid RMSE: 3.8657]\n",
      "Train Epoch: 32 [8/22024] \t Train Loss(MSE): 12.0001 \t Train RMSE: 3.4641\n",
      "Train Epoch: 32 [2008/22024] \t Train Loss(MSE): 8.9599 \t Train RMSE: 2.9933\n",
      "Train Epoch: 32 [4008/22024] \t Train Loss(MSE): 21.0900 \t Train RMSE: 4.5924\n",
      "Train Epoch: 32 [6008/22024] \t Train Loss(MSE): 2.7710 \t Train RMSE: 1.6646\n",
      "Train Epoch: 32 [8008/22024] \t Train Loss(MSE): 5.7238 \t Train RMSE: 2.3924\n",
      "Train Epoch: 32 [10008/22024] \t Train Loss(MSE): 6.8398 \t Train RMSE: 2.6153\n",
      "Train Epoch: 32 [12008/22024] \t Train Loss(MSE): 21.8451 \t Train RMSE: 4.6739\n",
      "Train Epoch: 32 [14008/22024] \t Train Loss(MSE): 5.4118 \t Train RMSE: 2.3263\n",
      "Train Epoch: 32 [16008/22024] \t Train Loss(MSE): 19.3944 \t Train RMSE: 4.4039\n",
      "Train Epoch: 32 [18008/22024] \t Train Loss(MSE): 10.0773 \t Train RMSE: 3.1745\n",
      "Train Epoch: 32 [20008/22024] \t Train Loss(MSE): 25.7983 \t Train RMSE: 5.0792\n",
      "Train Epoch: 32 [22008/22024] \t Train Loss(MSE): 9.0492 \t Train RMSE: 3.0082\n",
      "[Epoch: 32 \t Valid MSE: 18.0486 \t Valid RMSE: 3.5943]\n",
      "Train Epoch: 33 [8/22024] \t Train Loss(MSE): 12.1577 \t Train RMSE: 3.4868\n",
      "Train Epoch: 33 [2008/22024] \t Train Loss(MSE): 4.6835 \t Train RMSE: 2.1641\n",
      "Train Epoch: 33 [4008/22024] \t Train Loss(MSE): 8.7800 \t Train RMSE: 2.9631\n",
      "Train Epoch: 33 [6008/22024] \t Train Loss(MSE): 7.5333 \t Train RMSE: 2.7447\n",
      "Train Epoch: 33 [8008/22024] \t Train Loss(MSE): 2.6323 \t Train RMSE: 1.6224\n",
      "Train Epoch: 33 [10008/22024] \t Train Loss(MSE): 12.3690 \t Train RMSE: 3.5170\n",
      "Train Epoch: 33 [12008/22024] \t Train Loss(MSE): 5.4086 \t Train RMSE: 2.3256\n",
      "Train Epoch: 33 [14008/22024] \t Train Loss(MSE): 1.8383 \t Train RMSE: 1.3559\n",
      "Train Epoch: 33 [16008/22024] \t Train Loss(MSE): 2.4101 \t Train RMSE: 1.5525\n",
      "Train Epoch: 33 [18008/22024] \t Train Loss(MSE): 16.0460 \t Train RMSE: 4.0057\n",
      "Train Epoch: 33 [20008/22024] \t Train Loss(MSE): 0.7821 \t Train RMSE: 0.8844\n",
      "Train Epoch: 33 [22008/22024] \t Train Loss(MSE): 11.9214 \t Train RMSE: 3.4527\n",
      "[Epoch: 33 \t Valid MSE: 19.7705 \t Valid RMSE: 3.8743]\n",
      "Train Epoch: 34 [8/22024] \t Train Loss(MSE): 2.0437 \t Train RMSE: 1.4296\n",
      "Train Epoch: 34 [2008/22024] \t Train Loss(MSE): 8.3948 \t Train RMSE: 2.8974\n",
      "Train Epoch: 34 [4008/22024] \t Train Loss(MSE): 2.8772 \t Train RMSE: 1.6962\n",
      "Train Epoch: 34 [6008/22024] \t Train Loss(MSE): 17.2829 \t Train RMSE: 4.1573\n",
      "Train Epoch: 34 [8008/22024] \t Train Loss(MSE): 8.4976 \t Train RMSE: 2.9151\n",
      "Train Epoch: 34 [10008/22024] \t Train Loss(MSE): 11.2502 \t Train RMSE: 3.3541\n",
      "Train Epoch: 34 [12008/22024] \t Train Loss(MSE): 1.8274 \t Train RMSE: 1.3518\n",
      "Train Epoch: 34 [14008/22024] \t Train Loss(MSE): 5.6751 \t Train RMSE: 2.3822\n",
      "Train Epoch: 34 [16008/22024] \t Train Loss(MSE): 11.1419 \t Train RMSE: 3.3379\n",
      "Train Epoch: 34 [18008/22024] \t Train Loss(MSE): 0.2701 \t Train RMSE: 0.5197\n",
      "Train Epoch: 34 [20008/22024] \t Train Loss(MSE): 6.0019 \t Train RMSE: 2.4499\n",
      "Train Epoch: 34 [22008/22024] \t Train Loss(MSE): 6.8618 \t Train RMSE: 2.6195\n",
      "[Epoch: 34 \t Valid MSE: 24.1708 \t Valid RMSE: 4.3870]\n",
      "Train Epoch: 35 [8/22024] \t Train Loss(MSE): 10.9782 \t Train RMSE: 3.3133\n",
      "Train Epoch: 35 [2008/22024] \t Train Loss(MSE): 4.9462 \t Train RMSE: 2.2240\n",
      "Train Epoch: 35 [4008/22024] \t Train Loss(MSE): 12.6850 \t Train RMSE: 3.5616\n",
      "Train Epoch: 35 [6008/22024] \t Train Loss(MSE): 8.0480 \t Train RMSE: 2.8369\n",
      "Train Epoch: 35 [8008/22024] \t Train Loss(MSE): 7.9398 \t Train RMSE: 2.8178\n",
      "Train Epoch: 35 [10008/22024] \t Train Loss(MSE): 56.4011 \t Train RMSE: 7.5101\n",
      "Train Epoch: 35 [12008/22024] \t Train Loss(MSE): 13.3664 \t Train RMSE: 3.6560\n",
      "Train Epoch: 35 [14008/22024] \t Train Loss(MSE): 11.8681 \t Train RMSE: 3.4450\n",
      "Train Epoch: 35 [16008/22024] \t Train Loss(MSE): 31.7493 \t Train RMSE: 5.6346\n",
      "Train Epoch: 35 [18008/22024] \t Train Loss(MSE): 5.8545 \t Train RMSE: 2.4196\n",
      "Train Epoch: 35 [20008/22024] \t Train Loss(MSE): 1.7288 \t Train RMSE: 1.3149\n",
      "Train Epoch: 35 [22008/22024] \t Train Loss(MSE): 7.4265 \t Train RMSE: 2.7252\n",
      "[Epoch: 35 \t Valid MSE: 23.0189 \t Valid RMSE: 4.2386]\n",
      "Train Epoch: 36 [8/22024] \t Train Loss(MSE): 2.0165 \t Train RMSE: 1.4200\n",
      "Train Epoch: 36 [2008/22024] \t Train Loss(MSE): 6.9043 \t Train RMSE: 2.6276\n",
      "Train Epoch: 36 [4008/22024] \t Train Loss(MSE): 8.3978 \t Train RMSE: 2.8979\n",
      "Train Epoch: 36 [6008/22024] \t Train Loss(MSE): 89.4497 \t Train RMSE: 9.4578\n",
      "Train Epoch: 36 [8008/22024] \t Train Loss(MSE): 11.5094 \t Train RMSE: 3.3925\n",
      "Train Epoch: 36 [10008/22024] \t Train Loss(MSE): 2.0903 \t Train RMSE: 1.4458\n",
      "Train Epoch: 36 [12008/22024] \t Train Loss(MSE): 3.1974 \t Train RMSE: 1.7881\n",
      "Train Epoch: 36 [14008/22024] \t Train Loss(MSE): 1.3189 \t Train RMSE: 1.1484\n",
      "Train Epoch: 36 [16008/22024] \t Train Loss(MSE): 31.9511 \t Train RMSE: 5.6525\n",
      "Train Epoch: 36 [18008/22024] \t Train Loss(MSE): 6.7071 \t Train RMSE: 2.5898\n",
      "Train Epoch: 36 [20008/22024] \t Train Loss(MSE): 2.1126 \t Train RMSE: 1.4535\n",
      "Train Epoch: 36 [22008/22024] \t Train Loss(MSE): 13.4653 \t Train RMSE: 3.6695\n",
      "[Epoch: 36 \t Valid MSE: 17.4110 \t Valid RMSE: 3.5243]\n",
      "Train Epoch: 37 [8/22024] \t Train Loss(MSE): 8.7108 \t Train RMSE: 2.9514\n",
      "Train Epoch: 37 [2008/22024] \t Train Loss(MSE): 8.0964 \t Train RMSE: 2.8454\n",
      "Train Epoch: 37 [4008/22024] \t Train Loss(MSE): 2.7177 \t Train RMSE: 1.6486\n",
      "Train Epoch: 37 [6008/22024] \t Train Loss(MSE): 7.4478 \t Train RMSE: 2.7291\n",
      "Train Epoch: 37 [8008/22024] \t Train Loss(MSE): 2.0568 \t Train RMSE: 1.4341\n",
      "Train Epoch: 37 [10008/22024] \t Train Loss(MSE): 6.0818 \t Train RMSE: 2.4661\n",
      "Train Epoch: 37 [12008/22024] \t Train Loss(MSE): 0.7246 \t Train RMSE: 0.8513\n",
      "Train Epoch: 37 [14008/22024] \t Train Loss(MSE): 1.7357 \t Train RMSE: 1.3175\n",
      "Train Epoch: 37 [16008/22024] \t Train Loss(MSE): 1.2453 \t Train RMSE: 1.1160\n",
      "Train Epoch: 37 [18008/22024] \t Train Loss(MSE): 15.2055 \t Train RMSE: 3.8994\n",
      "Train Epoch: 37 [20008/22024] \t Train Loss(MSE): 7.1973 \t Train RMSE: 2.6828\n",
      "Train Epoch: 37 [22008/22024] \t Train Loss(MSE): 6.8779 \t Train RMSE: 2.6226\n",
      "[Epoch: 37 \t Valid MSE: 17.8853 \t Valid RMSE: 3.6054]\n",
      "Train Epoch: 38 [8/22024] \t Train Loss(MSE): 0.8061 \t Train RMSE: 0.8979\n",
      "Train Epoch: 38 [2008/22024] \t Train Loss(MSE): 6.2307 \t Train RMSE: 2.4961\n",
      "Train Epoch: 38 [4008/22024] \t Train Loss(MSE): 1.5033 \t Train RMSE: 1.2261\n",
      "Train Epoch: 38 [6008/22024] \t Train Loss(MSE): 2.8697 \t Train RMSE: 1.6940\n",
      "Train Epoch: 38 [8008/22024] \t Train Loss(MSE): 3.2497 \t Train RMSE: 1.8027\n",
      "Train Epoch: 38 [10008/22024] \t Train Loss(MSE): 20.1532 \t Train RMSE: 4.4892\n",
      "Train Epoch: 38 [12008/22024] \t Train Loss(MSE): 6.7599 \t Train RMSE: 2.6000\n",
      "Train Epoch: 38 [14008/22024] \t Train Loss(MSE): 9.3059 \t Train RMSE: 3.0506\n",
      "Train Epoch: 38 [16008/22024] \t Train Loss(MSE): 3.7178 \t Train RMSE: 1.9282\n",
      "Train Epoch: 38 [18008/22024] \t Train Loss(MSE): 5.0087 \t Train RMSE: 2.2380\n",
      "Train Epoch: 38 [20008/22024] \t Train Loss(MSE): 10.0688 \t Train RMSE: 3.1731\n",
      "Train Epoch: 38 [22008/22024] \t Train Loss(MSE): 1.1838 \t Train RMSE: 1.0880\n",
      "[Epoch: 38 \t Valid MSE: 20.6431 \t Valid RMSE: 3.9244]\n",
      "Train Epoch: 39 [8/22024] \t Train Loss(MSE): 7.0774 \t Train RMSE: 2.6603\n",
      "Train Epoch: 39 [2008/22024] \t Train Loss(MSE): 3.2847 \t Train RMSE: 1.8124\n",
      "Train Epoch: 39 [4008/22024] \t Train Loss(MSE): 9.1000 \t Train RMSE: 3.0166\n",
      "Train Epoch: 39 [6008/22024] \t Train Loss(MSE): 2.5643 \t Train RMSE: 1.6014\n",
      "Train Epoch: 39 [8008/22024] \t Train Loss(MSE): 15.0929 \t Train RMSE: 3.8850\n",
      "Train Epoch: 39 [10008/22024] \t Train Loss(MSE): 15.5237 \t Train RMSE: 3.9400\n",
      "Train Epoch: 39 [12008/22024] \t Train Loss(MSE): 9.5458 \t Train RMSE: 3.0896\n",
      "Train Epoch: 39 [14008/22024] \t Train Loss(MSE): 2.0874 \t Train RMSE: 1.4448\n",
      "Train Epoch: 39 [16008/22024] \t Train Loss(MSE): 8.6128 \t Train RMSE: 2.9348\n",
      "Train Epoch: 39 [18008/22024] \t Train Loss(MSE): 7.0275 \t Train RMSE: 2.6509\n",
      "Train Epoch: 39 [20008/22024] \t Train Loss(MSE): 1.7467 \t Train RMSE: 1.3216\n",
      "Train Epoch: 39 [22008/22024] \t Train Loss(MSE): 9.4882 \t Train RMSE: 3.0803\n",
      "[Epoch: 39 \t Valid MSE: 20.9874 \t Valid RMSE: 4.0415]\n",
      "Train Epoch: 40 [8/22024] \t Train Loss(MSE): 9.2006 \t Train RMSE: 3.0332\n",
      "Train Epoch: 40 [2008/22024] \t Train Loss(MSE): 25.4221 \t Train RMSE: 5.0420\n",
      "Train Epoch: 40 [4008/22024] \t Train Loss(MSE): 6.2678 \t Train RMSE: 2.5036\n",
      "Train Epoch: 40 [6008/22024] \t Train Loss(MSE): 4.0546 \t Train RMSE: 2.0136\n",
      "Train Epoch: 40 [8008/22024] \t Train Loss(MSE): 2.0926 \t Train RMSE: 1.4466\n",
      "Train Epoch: 40 [10008/22024] \t Train Loss(MSE): 5.9366 \t Train RMSE: 2.4365\n",
      "Train Epoch: 40 [12008/22024] \t Train Loss(MSE): 9.2653 \t Train RMSE: 3.0439\n",
      "Train Epoch: 40 [14008/22024] \t Train Loss(MSE): 10.9293 \t Train RMSE: 3.3060\n",
      "Train Epoch: 40 [16008/22024] \t Train Loss(MSE): 2.9628 \t Train RMSE: 1.7213\n",
      "Train Epoch: 40 [18008/22024] \t Train Loss(MSE): 9.9013 \t Train RMSE: 3.1466\n",
      "Train Epoch: 40 [20008/22024] \t Train Loss(MSE): 6.3447 \t Train RMSE: 2.5189\n",
      "Train Epoch: 40 [22008/22024] \t Train Loss(MSE): 28.6945 \t Train RMSE: 5.3567\n",
      "[Epoch: 40 \t Valid MSE: 21.5967 \t Valid RMSE: 4.0730]\n",
      "Train Epoch: 41 [8/22024] \t Train Loss(MSE): 19.7781 \t Train RMSE: 4.4473\n",
      "Train Epoch: 41 [2008/22024] \t Train Loss(MSE): 6.1948 \t Train RMSE: 2.4889\n",
      "Train Epoch: 41 [4008/22024] \t Train Loss(MSE): 4.3611 \t Train RMSE: 2.0883\n",
      "Train Epoch: 41 [6008/22024] \t Train Loss(MSE): 21.0598 \t Train RMSE: 4.5891\n",
      "Train Epoch: 41 [8008/22024] \t Train Loss(MSE): 10.9986 \t Train RMSE: 3.3164\n",
      "Train Epoch: 41 [10008/22024] \t Train Loss(MSE): 7.3671 \t Train RMSE: 2.7142\n",
      "Train Epoch: 41 [12008/22024] \t Train Loss(MSE): 75.7079 \t Train RMSE: 8.7010\n",
      "Train Epoch: 41 [14008/22024] \t Train Loss(MSE): 3.0392 \t Train RMSE: 1.7433\n",
      "Train Epoch: 41 [16008/22024] \t Train Loss(MSE): 8.6732 \t Train RMSE: 2.9450\n",
      "Train Epoch: 41 [18008/22024] \t Train Loss(MSE): 8.0870 \t Train RMSE: 2.8438\n",
      "Train Epoch: 41 [20008/22024] \t Train Loss(MSE): 9.9002 \t Train RMSE: 3.1465\n",
      "Train Epoch: 41 [22008/22024] \t Train Loss(MSE): 8.3773 \t Train RMSE: 2.8944\n",
      "[Epoch: 41 \t Valid MSE: 17.1384 \t Valid RMSE: 3.4553]\n",
      "Train Epoch: 42 [8/22024] \t Train Loss(MSE): 2.4400 \t Train RMSE: 1.5621\n",
      "Train Epoch: 42 [2008/22024] \t Train Loss(MSE): 83.6861 \t Train RMSE: 9.1480\n",
      "Train Epoch: 42 [4008/22024] \t Train Loss(MSE): 2.1270 \t Train RMSE: 1.4584\n",
      "Train Epoch: 42 [6008/22024] \t Train Loss(MSE): 1.0578 \t Train RMSE: 1.0285\n",
      "Train Epoch: 42 [8008/22024] \t Train Loss(MSE): 14.8798 \t Train RMSE: 3.8574\n",
      "Train Epoch: 42 [10008/22024] \t Train Loss(MSE): 2.4493 \t Train RMSE: 1.5650\n",
      "Train Epoch: 42 [12008/22024] \t Train Loss(MSE): 15.8065 \t Train RMSE: 3.9757\n",
      "Train Epoch: 42 [14008/22024] \t Train Loss(MSE): 7.9995 \t Train RMSE: 2.8283\n",
      "Train Epoch: 42 [16008/22024] \t Train Loss(MSE): 15.8204 \t Train RMSE: 3.9775\n",
      "Train Epoch: 42 [18008/22024] \t Train Loss(MSE): 126.3801 \t Train RMSE: 11.2419\n",
      "Train Epoch: 42 [20008/22024] \t Train Loss(MSE): 8.9500 \t Train RMSE: 2.9917\n",
      "Train Epoch: 42 [22008/22024] \t Train Loss(MSE): 39.9873 \t Train RMSE: 6.3236\n",
      "[Epoch: 42 \t Valid MSE: 17.5093 \t Valid RMSE: 3.4375]\n",
      "Train Epoch: 43 [8/22024] \t Train Loss(MSE): 52.2928 \t Train RMSE: 7.2314\n",
      "Train Epoch: 43 [2008/22024] \t Train Loss(MSE): 3.2134 \t Train RMSE: 1.7926\n",
      "Train Epoch: 43 [4008/22024] \t Train Loss(MSE): 73.5466 \t Train RMSE: 8.5759\n",
      "Train Epoch: 43 [6008/22024] \t Train Loss(MSE): 1.1979 \t Train RMSE: 1.0945\n",
      "Train Epoch: 43 [8008/22024] \t Train Loss(MSE): 22.2048 \t Train RMSE: 4.7122\n",
      "Train Epoch: 43 [10008/22024] \t Train Loss(MSE): 0.6989 \t Train RMSE: 0.8360\n",
      "Train Epoch: 43 [12008/22024] \t Train Loss(MSE): 13.0125 \t Train RMSE: 3.6073\n",
      "Train Epoch: 43 [14008/22024] \t Train Loss(MSE): 10.3339 \t Train RMSE: 3.2146\n",
      "Train Epoch: 43 [16008/22024] \t Train Loss(MSE): 2.9057 \t Train RMSE: 1.7046\n",
      "Train Epoch: 43 [18008/22024] \t Train Loss(MSE): 7.2069 \t Train RMSE: 2.6846\n",
      "Train Epoch: 43 [20008/22024] \t Train Loss(MSE): 20.3995 \t Train RMSE: 4.5166\n",
      "Train Epoch: 43 [22008/22024] \t Train Loss(MSE): 10.0671 \t Train RMSE: 3.1729\n",
      "[Epoch: 43 \t Valid MSE: 20.0719 \t Valid RMSE: 3.7884]\n",
      "Train Epoch: 44 [8/22024] \t Train Loss(MSE): 1.1779 \t Train RMSE: 1.0853\n",
      "Train Epoch: 44 [2008/22024] \t Train Loss(MSE): 27.6102 \t Train RMSE: 5.2545\n",
      "Train Epoch: 44 [4008/22024] \t Train Loss(MSE): 10.1765 \t Train RMSE: 3.1901\n",
      "Train Epoch: 44 [6008/22024] \t Train Loss(MSE): 3.0990 \t Train RMSE: 1.7604\n",
      "Train Epoch: 44 [8008/22024] \t Train Loss(MSE): 24.2127 \t Train RMSE: 4.9206\n",
      "Train Epoch: 44 [10008/22024] \t Train Loss(MSE): 2.3436 \t Train RMSE: 1.5309\n",
      "Train Epoch: 44 [12008/22024] \t Train Loss(MSE): 10.9027 \t Train RMSE: 3.3019\n",
      "Train Epoch: 44 [14008/22024] \t Train Loss(MSE): 7.7557 \t Train RMSE: 2.7849\n",
      "Train Epoch: 44 [16008/22024] \t Train Loss(MSE): 11.1010 \t Train RMSE: 3.3318\n",
      "Train Epoch: 44 [18008/22024] \t Train Loss(MSE): 0.9771 \t Train RMSE: 0.9885\n",
      "Train Epoch: 44 [20008/22024] \t Train Loss(MSE): 18.6760 \t Train RMSE: 4.3216\n",
      "Train Epoch: 44 [22008/22024] \t Train Loss(MSE): 33.4527 \t Train RMSE: 5.7838\n",
      "[Epoch: 44 \t Valid MSE: 23.7132 \t Valid RMSE: 4.3042]\n",
      "Train Epoch: 45 [8/22024] \t Train Loss(MSE): 3.0903 \t Train RMSE: 1.7579\n",
      "Train Epoch: 45 [2008/22024] \t Train Loss(MSE): 25.7657 \t Train RMSE: 5.0760\n",
      "Train Epoch: 45 [4008/22024] \t Train Loss(MSE): 23.3264 \t Train RMSE: 4.8297\n",
      "Train Epoch: 45 [6008/22024] \t Train Loss(MSE): 12.7177 \t Train RMSE: 3.5662\n",
      "Train Epoch: 45 [8008/22024] \t Train Loss(MSE): 6.6227 \t Train RMSE: 2.5735\n",
      "Train Epoch: 45 [10008/22024] \t Train Loss(MSE): 2.7065 \t Train RMSE: 1.6451\n",
      "Train Epoch: 45 [12008/22024] \t Train Loss(MSE): 19.3681 \t Train RMSE: 4.4009\n",
      "Train Epoch: 45 [14008/22024] \t Train Loss(MSE): 3.2905 \t Train RMSE: 1.8140\n",
      "Train Epoch: 45 [16008/22024] \t Train Loss(MSE): 4.1015 \t Train RMSE: 2.0252\n",
      "Train Epoch: 45 [18008/22024] \t Train Loss(MSE): 4.0315 \t Train RMSE: 2.0079\n",
      "Train Epoch: 45 [20008/22024] \t Train Loss(MSE): 6.1184 \t Train RMSE: 2.4735\n",
      "Train Epoch: 45 [22008/22024] \t Train Loss(MSE): 10.3073 \t Train RMSE: 3.2105\n",
      "[Epoch: 45 \t Valid MSE: 16.2011 \t Valid RMSE: 3.2942]\n",
      "Train Epoch: 46 [8/22024] \t Train Loss(MSE): 1.3453 \t Train RMSE: 1.1599\n",
      "Train Epoch: 46 [2008/22024] \t Train Loss(MSE): 2.1796 \t Train RMSE: 1.4763\n",
      "Train Epoch: 46 [4008/22024] \t Train Loss(MSE): 7.2787 \t Train RMSE: 2.6979\n",
      "Train Epoch: 46 [6008/22024] \t Train Loss(MSE): 1.6196 \t Train RMSE: 1.2726\n",
      "Train Epoch: 46 [8008/22024] \t Train Loss(MSE): 6.1758 \t Train RMSE: 2.4851\n",
      "Train Epoch: 46 [10008/22024] \t Train Loss(MSE): 8.0093 \t Train RMSE: 2.8301\n",
      "Train Epoch: 46 [12008/22024] \t Train Loss(MSE): 5.2156 \t Train RMSE: 2.2838\n",
      "Train Epoch: 46 [14008/22024] \t Train Loss(MSE): 8.9201 \t Train RMSE: 2.9866\n",
      "Train Epoch: 46 [16008/22024] \t Train Loss(MSE): 0.8255 \t Train RMSE: 0.9086\n",
      "Train Epoch: 46 [18008/22024] \t Train Loss(MSE): 7.4842 \t Train RMSE: 2.7357\n",
      "Train Epoch: 46 [20008/22024] \t Train Loss(MSE): 4.2085 \t Train RMSE: 2.0515\n",
      "Train Epoch: 46 [22008/22024] \t Train Loss(MSE): 7.1201 \t Train RMSE: 2.6683\n",
      "[Epoch: 46 \t Valid MSE: 21.9717 \t Valid RMSE: 4.1195]\n",
      "Train Epoch: 47 [8/22024] \t Train Loss(MSE): 67.7399 \t Train RMSE: 8.2304\n",
      "Train Epoch: 47 [2008/22024] \t Train Loss(MSE): 7.5282 \t Train RMSE: 2.7438\n",
      "Train Epoch: 47 [4008/22024] \t Train Loss(MSE): 13.9315 \t Train RMSE: 3.7325\n",
      "Train Epoch: 47 [6008/22024] \t Train Loss(MSE): 43.8873 \t Train RMSE: 6.6247\n",
      "Train Epoch: 47 [8008/22024] \t Train Loss(MSE): 4.2039 \t Train RMSE: 2.0503\n",
      "Train Epoch: 47 [10008/22024] \t Train Loss(MSE): 12.6163 \t Train RMSE: 3.5519\n",
      "Train Epoch: 47 [12008/22024] \t Train Loss(MSE): 2.5135 \t Train RMSE: 1.5854\n",
      "Train Epoch: 47 [14008/22024] \t Train Loss(MSE): 22.4126 \t Train RMSE: 4.7342\n",
      "Train Epoch: 47 [16008/22024] \t Train Loss(MSE): 41.3787 \t Train RMSE: 6.4326\n",
      "Train Epoch: 47 [18008/22024] \t Train Loss(MSE): 28.8860 \t Train RMSE: 5.3746\n",
      "Train Epoch: 47 [20008/22024] \t Train Loss(MSE): 8.4663 \t Train RMSE: 2.9097\n",
      "Train Epoch: 47 [22008/22024] \t Train Loss(MSE): 0.5743 \t Train RMSE: 0.7578\n",
      "[Epoch: 47 \t Valid MSE: 19.5319 \t Valid RMSE: 3.7810]\n",
      "Train Epoch: 48 [8/22024] \t Train Loss(MSE): 7.5026 \t Train RMSE: 2.7391\n",
      "Train Epoch: 48 [2008/22024] \t Train Loss(MSE): 9.0944 \t Train RMSE: 3.0157\n",
      "Train Epoch: 48 [4008/22024] \t Train Loss(MSE): 7.7305 \t Train RMSE: 2.7804\n",
      "Train Epoch: 48 [6008/22024] \t Train Loss(MSE): 2.4509 \t Train RMSE: 1.5655\n",
      "Train Epoch: 48 [8008/22024] \t Train Loss(MSE): 9.3259 \t Train RMSE: 3.0538\n",
      "Train Epoch: 48 [10008/22024] \t Train Loss(MSE): 1.5560 \t Train RMSE: 1.2474\n",
      "Train Epoch: 48 [12008/22024] \t Train Loss(MSE): 30.2577 \t Train RMSE: 5.5007\n",
      "Train Epoch: 48 [14008/22024] \t Train Loss(MSE): 1.3499 \t Train RMSE: 1.1619\n",
      "Train Epoch: 48 [16008/22024] \t Train Loss(MSE): 15.7371 \t Train RMSE: 3.9670\n",
      "Train Epoch: 48 [18008/22024] \t Train Loss(MSE): 5.6285 \t Train RMSE: 2.3724\n",
      "Train Epoch: 48 [20008/22024] \t Train Loss(MSE): 7.8192 \t Train RMSE: 2.7963\n",
      "Train Epoch: 48 [22008/22024] \t Train Loss(MSE): 6.3609 \t Train RMSE: 2.5221\n",
      "[Epoch: 48 \t Valid MSE: 16.6729 \t Valid RMSE: 3.3925]\n",
      "Train Epoch: 49 [8/22024] \t Train Loss(MSE): 15.5110 \t Train RMSE: 3.9384\n",
      "Train Epoch: 49 [2008/22024] \t Train Loss(MSE): 1.7030 \t Train RMSE: 1.3050\n",
      "Train Epoch: 49 [4008/22024] \t Train Loss(MSE): 21.1456 \t Train RMSE: 4.5984\n",
      "Train Epoch: 49 [6008/22024] \t Train Loss(MSE): 14.9715 \t Train RMSE: 3.8693\n",
      "Train Epoch: 49 [8008/22024] \t Train Loss(MSE): 3.5421 \t Train RMSE: 1.8820\n",
      "Train Epoch: 49 [10008/22024] \t Train Loss(MSE): 39.3688 \t Train RMSE: 6.2745\n",
      "Train Epoch: 49 [12008/22024] \t Train Loss(MSE): 7.0710 \t Train RMSE: 2.6591\n",
      "Train Epoch: 49 [14008/22024] \t Train Loss(MSE): 3.0045 \t Train RMSE: 1.7333\n",
      "Train Epoch: 49 [16008/22024] \t Train Loss(MSE): 3.6206 \t Train RMSE: 1.9028\n",
      "Train Epoch: 49 [18008/22024] \t Train Loss(MSE): 27.4931 \t Train RMSE: 5.2434\n",
      "Train Epoch: 49 [20008/22024] \t Train Loss(MSE): 7.8052 \t Train RMSE: 2.7938\n",
      "Train Epoch: 49 [22008/22024] \t Train Loss(MSE): 51.6757 \t Train RMSE: 7.1886\n",
      "[Epoch: 49 \t Valid MSE: 17.7045 \t Valid RMSE: 3.5590]\n",
      "Train Epoch: 50 [8/22024] \t Train Loss(MSE): 1.1335 \t Train RMSE: 1.0647\n",
      "Train Epoch: 50 [2008/22024] \t Train Loss(MSE): 11.7630 \t Train RMSE: 3.4297\n",
      "Train Epoch: 50 [4008/22024] \t Train Loss(MSE): 2.0805 \t Train RMSE: 1.4424\n",
      "Train Epoch: 50 [6008/22024] \t Train Loss(MSE): 31.1669 \t Train RMSE: 5.5827\n",
      "Train Epoch: 50 [8008/22024] \t Train Loss(MSE): 2.8153 \t Train RMSE: 1.6779\n",
      "Train Epoch: 50 [10008/22024] \t Train Loss(MSE): 6.9765 \t Train RMSE: 2.6413\n",
      "Train Epoch: 50 [12008/22024] \t Train Loss(MSE): 5.4924 \t Train RMSE: 2.3436\n",
      "Train Epoch: 50 [14008/22024] \t Train Loss(MSE): 3.7950 \t Train RMSE: 1.9481\n",
      "Train Epoch: 50 [16008/22024] \t Train Loss(MSE): 2.0908 \t Train RMSE: 1.4460\n",
      "Train Epoch: 50 [18008/22024] \t Train Loss(MSE): 4.1548 \t Train RMSE: 2.0383\n",
      "Train Epoch: 50 [20008/22024] \t Train Loss(MSE): 5.4817 \t Train RMSE: 2.3413\n",
      "Train Epoch: 50 [22008/22024] \t Train Loss(MSE): 13.8765 \t Train RMSE: 3.7251\n",
      "[Epoch: 50 \t Valid MSE: 19.2205 \t Valid RMSE: 3.6587]\n",
      "Train Epoch: 51 [8/22024] \t Train Loss(MSE): 4.3052 \t Train RMSE: 2.0749\n",
      "Train Epoch: 51 [2008/22024] \t Train Loss(MSE): 48.9865 \t Train RMSE: 6.9990\n",
      "Train Epoch: 51 [4008/22024] \t Train Loss(MSE): 18.8782 \t Train RMSE: 4.3449\n",
      "Train Epoch: 51 [6008/22024] \t Train Loss(MSE): 0.8973 \t Train RMSE: 0.9473\n",
      "Train Epoch: 51 [8008/22024] \t Train Loss(MSE): 2.7788 \t Train RMSE: 1.6670\n",
      "Train Epoch: 51 [10008/22024] \t Train Loss(MSE): 1.2696 \t Train RMSE: 1.1268\n",
      "Train Epoch: 51 [12008/22024] \t Train Loss(MSE): 2.0296 \t Train RMSE: 1.4247\n",
      "Train Epoch: 51 [14008/22024] \t Train Loss(MSE): 1.1408 \t Train RMSE: 1.0681\n",
      "Train Epoch: 51 [16008/22024] \t Train Loss(MSE): 51.5053 \t Train RMSE: 7.1767\n",
      "Train Epoch: 51 [18008/22024] \t Train Loss(MSE): 16.6035 \t Train RMSE: 4.0747\n",
      "Train Epoch: 51 [20008/22024] \t Train Loss(MSE): 8.8238 \t Train RMSE: 2.9705\n",
      "Train Epoch: 51 [22008/22024] \t Train Loss(MSE): 17.8583 \t Train RMSE: 4.2259\n",
      "[Epoch: 51 \t Valid MSE: 22.7472 \t Valid RMSE: 4.2439]\n",
      "Train Epoch: 52 [8/22024] \t Train Loss(MSE): 16.5973 \t Train RMSE: 4.0740\n",
      "Train Epoch: 52 [2008/22024] \t Train Loss(MSE): 7.1315 \t Train RMSE: 2.6705\n",
      "Train Epoch: 52 [4008/22024] \t Train Loss(MSE): 19.2002 \t Train RMSE: 4.3818\n",
      "Train Epoch: 52 [6008/22024] \t Train Loss(MSE): 26.5735 \t Train RMSE: 5.1549\n",
      "Train Epoch: 52 [8008/22024] \t Train Loss(MSE): 8.1808 \t Train RMSE: 2.8602\n",
      "Train Epoch: 52 [10008/22024] \t Train Loss(MSE): 25.7286 \t Train RMSE: 5.0723\n",
      "Train Epoch: 52 [12008/22024] \t Train Loss(MSE): 70.6735 \t Train RMSE: 8.4068\n",
      "Train Epoch: 52 [14008/22024] \t Train Loss(MSE): 1.2313 \t Train RMSE: 1.1096\n",
      "Train Epoch: 52 [16008/22024] \t Train Loss(MSE): 21.5685 \t Train RMSE: 4.6442\n",
      "Train Epoch: 52 [18008/22024] \t Train Loss(MSE): 1.3348 \t Train RMSE: 1.1553\n",
      "Train Epoch: 52 [20008/22024] \t Train Loss(MSE): 3.6227 \t Train RMSE: 1.9034\n",
      "Train Epoch: 52 [22008/22024] \t Train Loss(MSE): 5.1030 \t Train RMSE: 2.2590\n",
      "[Epoch: 52 \t Valid MSE: 19.4372 \t Valid RMSE: 3.6983]\n",
      "Train Epoch: 53 [8/22024] \t Train Loss(MSE): 31.8993 \t Train RMSE: 5.6479\n",
      "Train Epoch: 53 [2008/22024] \t Train Loss(MSE): 6.1764 \t Train RMSE: 2.4852\n",
      "Train Epoch: 53 [4008/22024] \t Train Loss(MSE): 10.7136 \t Train RMSE: 3.2732\n",
      "Train Epoch: 53 [6008/22024] \t Train Loss(MSE): 3.3970 \t Train RMSE: 1.8431\n",
      "Train Epoch: 53 [8008/22024] \t Train Loss(MSE): 17.4713 \t Train RMSE: 4.1799\n",
      "Train Epoch: 53 [10008/22024] \t Train Loss(MSE): 0.7229 \t Train RMSE: 0.8503\n",
      "Train Epoch: 53 [12008/22024] \t Train Loss(MSE): 3.0353 \t Train RMSE: 1.7422\n",
      "Train Epoch: 53 [14008/22024] \t Train Loss(MSE): 6.8014 \t Train RMSE: 2.6079\n",
      "Train Epoch: 53 [16008/22024] \t Train Loss(MSE): 6.7241 \t Train RMSE: 2.5931\n",
      "Train Epoch: 53 [18008/22024] \t Train Loss(MSE): 2.0720 \t Train RMSE: 1.4394\n",
      "Train Epoch: 53 [20008/22024] \t Train Loss(MSE): 28.9509 \t Train RMSE: 5.3806\n",
      "Train Epoch: 53 [22008/22024] \t Train Loss(MSE): 2.4010 \t Train RMSE: 1.5495\n",
      "[Epoch: 53 \t Valid MSE: 17.9684 \t Valid RMSE: 3.5624]\n",
      "Train Epoch: 54 [8/22024] \t Train Loss(MSE): 11.3264 \t Train RMSE: 3.3655\n",
      "Train Epoch: 54 [2008/22024] \t Train Loss(MSE): 3.6117 \t Train RMSE: 1.9004\n",
      "Train Epoch: 54 [4008/22024] \t Train Loss(MSE): 11.9431 \t Train RMSE: 3.4559\n",
      "Train Epoch: 54 [6008/22024] \t Train Loss(MSE): 1.2921 \t Train RMSE: 1.1367\n",
      "Train Epoch: 54 [8008/22024] \t Train Loss(MSE): 6.4950 \t Train RMSE: 2.5485\n",
      "Train Epoch: 54 [10008/22024] \t Train Loss(MSE): 3.1204 \t Train RMSE: 1.7665\n",
      "Train Epoch: 54 [12008/22024] \t Train Loss(MSE): 6.5099 \t Train RMSE: 2.5515\n",
      "Train Epoch: 54 [14008/22024] \t Train Loss(MSE): 5.9886 \t Train RMSE: 2.4472\n",
      "Train Epoch: 54 [16008/22024] \t Train Loss(MSE): 41.6737 \t Train RMSE: 6.4555\n",
      "Train Epoch: 54 [18008/22024] \t Train Loss(MSE): 48.1232 \t Train RMSE: 6.9371\n",
      "Train Epoch: 54 [20008/22024] \t Train Loss(MSE): 0.9986 \t Train RMSE: 0.9993\n",
      "Train Epoch: 54 [22008/22024] \t Train Loss(MSE): 3.4135 \t Train RMSE: 1.8476\n",
      "[Epoch: 54 \t Valid MSE: 17.6991 \t Valid RMSE: 3.5980]\n",
      "Train Epoch: 55 [8/22024] \t Train Loss(MSE): 12.9893 \t Train RMSE: 3.6041\n",
      "Train Epoch: 55 [2008/22024] \t Train Loss(MSE): 7.8552 \t Train RMSE: 2.8027\n",
      "Train Epoch: 55 [4008/22024] \t Train Loss(MSE): 22.2338 \t Train RMSE: 4.7153\n",
      "Train Epoch: 55 [6008/22024] \t Train Loss(MSE): 14.5815 \t Train RMSE: 3.8186\n",
      "Train Epoch: 55 [8008/22024] \t Train Loss(MSE): 8.2728 \t Train RMSE: 2.8762\n",
      "Train Epoch: 55 [10008/22024] \t Train Loss(MSE): 47.4979 \t Train RMSE: 6.8919\n",
      "Train Epoch: 55 [12008/22024] \t Train Loss(MSE): 4.9179 \t Train RMSE: 2.2176\n",
      "Train Epoch: 55 [14008/22024] \t Train Loss(MSE): 2.2094 \t Train RMSE: 1.4864\n",
      "Train Epoch: 55 [16008/22024] \t Train Loss(MSE): 1.9405 \t Train RMSE: 1.3930\n",
      "Train Epoch: 55 [18008/22024] \t Train Loss(MSE): 15.8679 \t Train RMSE: 3.9835\n",
      "Train Epoch: 55 [20008/22024] \t Train Loss(MSE): 5.3162 \t Train RMSE: 2.3057\n",
      "Train Epoch: 55 [22008/22024] \t Train Loss(MSE): 14.4260 \t Train RMSE: 3.7982\n",
      "[Epoch: 55 \t Valid MSE: 22.2581 \t Valid RMSE: 4.0764]\n",
      "Train Epoch: 56 [8/22024] \t Train Loss(MSE): 21.0821 \t Train RMSE: 4.5915\n",
      "Train Epoch: 56 [2008/22024] \t Train Loss(MSE): 4.8816 \t Train RMSE: 2.2094\n",
      "Train Epoch: 56 [4008/22024] \t Train Loss(MSE): 3.0813 \t Train RMSE: 1.7554\n",
      "Train Epoch: 56 [6008/22024] \t Train Loss(MSE): 45.3385 \t Train RMSE: 6.7334\n",
      "Train Epoch: 56 [8008/22024] \t Train Loss(MSE): 16.4877 \t Train RMSE: 4.0605\n",
      "Train Epoch: 56 [10008/22024] \t Train Loss(MSE): 17.9215 \t Train RMSE: 4.2334\n",
      "Train Epoch: 56 [12008/22024] \t Train Loss(MSE): 16.4573 \t Train RMSE: 4.0568\n",
      "Train Epoch: 56 [14008/22024] \t Train Loss(MSE): 20.9836 \t Train RMSE: 4.5808\n",
      "Train Epoch: 56 [16008/22024] \t Train Loss(MSE): 2.5874 \t Train RMSE: 1.6085\n",
      "Train Epoch: 56 [18008/22024] \t Train Loss(MSE): 7.8297 \t Train RMSE: 2.7982\n",
      "Train Epoch: 56 [20008/22024] \t Train Loss(MSE): 18.9645 \t Train RMSE: 4.3548\n",
      "Train Epoch: 56 [22008/22024] \t Train Loss(MSE): 1.0252 \t Train RMSE: 1.0125\n",
      "[Epoch: 56 \t Valid MSE: 20.1408 \t Valid RMSE: 3.8867]\n",
      "Train Epoch: 57 [8/22024] \t Train Loss(MSE): 6.8342 \t Train RMSE: 2.6142\n",
      "Train Epoch: 57 [2008/22024] \t Train Loss(MSE): 200.0499 \t Train RMSE: 14.1439\n",
      "Train Epoch: 57 [4008/22024] \t Train Loss(MSE): 7.7304 \t Train RMSE: 2.7804\n",
      "Train Epoch: 57 [6008/22024] \t Train Loss(MSE): 16.2055 \t Train RMSE: 4.0256\n",
      "Train Epoch: 57 [8008/22024] \t Train Loss(MSE): 6.2159 \t Train RMSE: 2.4932\n",
      "Train Epoch: 57 [10008/22024] \t Train Loss(MSE): 24.7753 \t Train RMSE: 4.9775\n",
      "Train Epoch: 57 [12008/22024] \t Train Loss(MSE): 5.9254 \t Train RMSE: 2.4342\n",
      "Train Epoch: 57 [14008/22024] \t Train Loss(MSE): 6.7069 \t Train RMSE: 2.5898\n",
      "Train Epoch: 57 [16008/22024] \t Train Loss(MSE): 8.4587 \t Train RMSE: 2.9084\n",
      "Train Epoch: 57 [18008/22024] \t Train Loss(MSE): 7.8136 \t Train RMSE: 2.7953\n",
      "Train Epoch: 57 [20008/22024] \t Train Loss(MSE): 1.9899 \t Train RMSE: 1.4106\n",
      "Train Epoch: 57 [22008/22024] \t Train Loss(MSE): 1.2555 \t Train RMSE: 1.1205\n",
      "[Epoch: 57 \t Valid MSE: 19.9586 \t Valid RMSE: 3.9291]\n",
      "Train Epoch: 58 [8/22024] \t Train Loss(MSE): 6.1573 \t Train RMSE: 2.4814\n",
      "Train Epoch: 58 [2008/22024] \t Train Loss(MSE): 1.0861 \t Train RMSE: 1.0422\n",
      "Train Epoch: 58 [4008/22024] \t Train Loss(MSE): 24.6459 \t Train RMSE: 4.9645\n",
      "Train Epoch: 58 [6008/22024] \t Train Loss(MSE): 17.8761 \t Train RMSE: 4.2280\n",
      "Train Epoch: 58 [8008/22024] \t Train Loss(MSE): 1.7720 \t Train RMSE: 1.3312\n",
      "Train Epoch: 58 [10008/22024] \t Train Loss(MSE): 3.1620 \t Train RMSE: 1.7782\n",
      "Train Epoch: 58 [12008/22024] \t Train Loss(MSE): 11.8526 \t Train RMSE: 3.4428\n",
      "Train Epoch: 58 [14008/22024] \t Train Loss(MSE): 4.0507 \t Train RMSE: 2.0126\n",
      "Train Epoch: 58 [16008/22024] \t Train Loss(MSE): 3.3881 \t Train RMSE: 1.8407\n",
      "Train Epoch: 58 [18008/22024] \t Train Loss(MSE): 3.2098 \t Train RMSE: 1.7916\n",
      "Train Epoch: 58 [20008/22024] \t Train Loss(MSE): 24.0215 \t Train RMSE: 4.9012\n",
      "Train Epoch: 58 [22008/22024] \t Train Loss(MSE): 1.5329 \t Train RMSE: 1.2381\n",
      "[Epoch: 58 \t Valid MSE: 18.8477 \t Valid RMSE: 3.7105]\n",
      "Train Epoch: 59 [8/22024] \t Train Loss(MSE): 30.0380 \t Train RMSE: 5.4807\n",
      "Train Epoch: 59 [2008/22024] \t Train Loss(MSE): 2.9285 \t Train RMSE: 1.7113\n",
      "Train Epoch: 59 [4008/22024] \t Train Loss(MSE): 2.8426 \t Train RMSE: 1.6860\n",
      "Train Epoch: 59 [6008/22024] \t Train Loss(MSE): 7.8993 \t Train RMSE: 2.8106\n",
      "Train Epoch: 59 [8008/22024] \t Train Loss(MSE): 1.0564 \t Train RMSE: 1.0278\n",
      "Train Epoch: 59 [10008/22024] \t Train Loss(MSE): 11.9885 \t Train RMSE: 3.4624\n",
      "Train Epoch: 59 [12008/22024] \t Train Loss(MSE): 260.6064 \t Train RMSE: 16.1433\n",
      "Train Epoch: 59 [14008/22024] \t Train Loss(MSE): 184.2614 \t Train RMSE: 13.5743\n",
      "Train Epoch: 59 [16008/22024] \t Train Loss(MSE): 2.7356 \t Train RMSE: 1.6540\n",
      "Train Epoch: 59 [18008/22024] \t Train Loss(MSE): 3.8586 \t Train RMSE: 1.9643\n",
      "Train Epoch: 59 [20008/22024] \t Train Loss(MSE): 6.9286 \t Train RMSE: 2.6322\n",
      "Train Epoch: 59 [22008/22024] \t Train Loss(MSE): 3.6949 \t Train RMSE: 1.9222\n",
      "[Epoch: 59 \t Valid MSE: 17.6308 \t Valid RMSE: 3.5480]\n",
      "Train Epoch: 60 [8/22024] \t Train Loss(MSE): 1.2022 \t Train RMSE: 1.0964\n",
      "Train Epoch: 60 [2008/22024] \t Train Loss(MSE): 17.6707 \t Train RMSE: 4.2037\n",
      "Train Epoch: 60 [4008/22024] \t Train Loss(MSE): 5.8695 \t Train RMSE: 2.4227\n",
      "Train Epoch: 60 [6008/22024] \t Train Loss(MSE): 1.7628 \t Train RMSE: 1.3277\n",
      "Train Epoch: 60 [8008/22024] \t Train Loss(MSE): 28.2399 \t Train RMSE: 5.3141\n",
      "Train Epoch: 60 [10008/22024] \t Train Loss(MSE): 16.4808 \t Train RMSE: 4.0597\n",
      "Train Epoch: 60 [12008/22024] \t Train Loss(MSE): 2.2995 \t Train RMSE: 1.5164\n",
      "Train Epoch: 60 [14008/22024] \t Train Loss(MSE): 16.6011 \t Train RMSE: 4.0744\n",
      "Train Epoch: 60 [16008/22024] \t Train Loss(MSE): 2.9810 \t Train RMSE: 1.7266\n",
      "Train Epoch: 60 [18008/22024] \t Train Loss(MSE): 7.3003 \t Train RMSE: 2.7019\n",
      "Train Epoch: 60 [20008/22024] \t Train Loss(MSE): 69.5963 \t Train RMSE: 8.3424\n",
      "Train Epoch: 60 [22008/22024] \t Train Loss(MSE): 5.8211 \t Train RMSE: 2.4127\n",
      "[Epoch: 60 \t Valid MSE: 26.0926 \t Valid RMSE: 4.5743]\n",
      "Train Epoch: 61 [8/22024] \t Train Loss(MSE): 8.3391 \t Train RMSE: 2.8878\n",
      "Train Epoch: 61 [2008/22024] \t Train Loss(MSE): 3.3584 \t Train RMSE: 1.8326\n",
      "Train Epoch: 61 [4008/22024] \t Train Loss(MSE): 1.1460 \t Train RMSE: 1.0705\n",
      "Train Epoch: 61 [6008/22024] \t Train Loss(MSE): 59.4933 \t Train RMSE: 7.7132\n",
      "Train Epoch: 61 [8008/22024] \t Train Loss(MSE): 25.2100 \t Train RMSE: 5.0210\n",
      "Train Epoch: 61 [10008/22024] \t Train Loss(MSE): 4.3166 \t Train RMSE: 2.0777\n",
      "Train Epoch: 61 [12008/22024] \t Train Loss(MSE): 0.6504 \t Train RMSE: 0.8065\n",
      "Train Epoch: 61 [14008/22024] \t Train Loss(MSE): 23.6235 \t Train RMSE: 4.8604\n",
      "Train Epoch: 61 [16008/22024] \t Train Loss(MSE): 3.4916 \t Train RMSE: 1.8686\n",
      "Train Epoch: 61 [18008/22024] \t Train Loss(MSE): 12.7434 \t Train RMSE: 3.5698\n",
      "Train Epoch: 61 [20008/22024] \t Train Loss(MSE): 4.6218 \t Train RMSE: 2.1498\n",
      "Train Epoch: 61 [22008/22024] \t Train Loss(MSE): 28.2552 \t Train RMSE: 5.3156\n",
      "[Epoch: 61 \t Valid MSE: 18.3350 \t Valid RMSE: 3.5313]\n",
      "Train Epoch: 62 [8/22024] \t Train Loss(MSE): 12.0568 \t Train RMSE: 3.4723\n",
      "Train Epoch: 62 [2008/22024] \t Train Loss(MSE): 9.1757 \t Train RMSE: 3.0291\n",
      "Train Epoch: 62 [4008/22024] \t Train Loss(MSE): 4.7006 \t Train RMSE: 2.1681\n",
      "Train Epoch: 62 [6008/22024] \t Train Loss(MSE): 9.6129 \t Train RMSE: 3.1005\n",
      "Train Epoch: 62 [8008/22024] \t Train Loss(MSE): 25.7788 \t Train RMSE: 5.0773\n",
      "Train Epoch: 62 [10008/22024] \t Train Loss(MSE): 1.6391 \t Train RMSE: 1.2803\n",
      "Train Epoch: 62 [12008/22024] \t Train Loss(MSE): 33.1174 \t Train RMSE: 5.7548\n",
      "Train Epoch: 62 [14008/22024] \t Train Loss(MSE): 1.5019 \t Train RMSE: 1.2255\n",
      "Train Epoch: 62 [16008/22024] \t Train Loss(MSE): 23.0036 \t Train RMSE: 4.7962\n",
      "Train Epoch: 62 [18008/22024] \t Train Loss(MSE): 2.8149 \t Train RMSE: 1.6778\n",
      "Train Epoch: 62 [20008/22024] \t Train Loss(MSE): 20.2011 \t Train RMSE: 4.4946\n",
      "Train Epoch: 62 [22008/22024] \t Train Loss(MSE): 15.1006 \t Train RMSE: 3.8859\n",
      "[Epoch: 62 \t Valid MSE: 19.1276 \t Valid RMSE: 3.7331]\n",
      "Train Epoch: 63 [8/22024] \t Train Loss(MSE): 2.6539 \t Train RMSE: 1.6291\n",
      "Train Epoch: 63 [2008/22024] \t Train Loss(MSE): 19.4597 \t Train RMSE: 4.4113\n",
      "Train Epoch: 63 [4008/22024] \t Train Loss(MSE): 30.0852 \t Train RMSE: 5.4850\n",
      "Train Epoch: 63 [6008/22024] \t Train Loss(MSE): 1.8558 \t Train RMSE: 1.3623\n",
      "Train Epoch: 63 [8008/22024] \t Train Loss(MSE): 5.2891 \t Train RMSE: 2.2998\n",
      "Train Epoch: 63 [10008/22024] \t Train Loss(MSE): 3.5980 \t Train RMSE: 1.8968\n",
      "Train Epoch: 63 [12008/22024] \t Train Loss(MSE): 9.4796 \t Train RMSE: 3.0789\n",
      "Train Epoch: 63 [14008/22024] \t Train Loss(MSE): 6.5689 \t Train RMSE: 2.5630\n",
      "Train Epoch: 63 [16008/22024] \t Train Loss(MSE): 10.4811 \t Train RMSE: 3.2374\n",
      "Train Epoch: 63 [18008/22024] \t Train Loss(MSE): 1.2187 \t Train RMSE: 1.1039\n",
      "Train Epoch: 63 [20008/22024] \t Train Loss(MSE): 8.6965 \t Train RMSE: 2.9490\n",
      "Train Epoch: 63 [22008/22024] \t Train Loss(MSE): 2.7662 \t Train RMSE: 1.6632\n",
      "[Epoch: 63 \t Valid MSE: 22.4244 \t Valid RMSE: 4.0599]\n",
      "Train Epoch: 64 [8/22024] \t Train Loss(MSE): 7.6169 \t Train RMSE: 2.7599\n",
      "Train Epoch: 64 [2008/22024] \t Train Loss(MSE): 3.7975 \t Train RMSE: 1.9487\n",
      "Train Epoch: 64 [4008/22024] \t Train Loss(MSE): 75.0426 \t Train RMSE: 8.6627\n",
      "Train Epoch: 64 [6008/22024] \t Train Loss(MSE): 15.3103 \t Train RMSE: 3.9128\n",
      "Train Epoch: 64 [8008/22024] \t Train Loss(MSE): 22.4154 \t Train RMSE: 4.7345\n",
      "Train Epoch: 64 [10008/22024] \t Train Loss(MSE): 7.4475 \t Train RMSE: 2.7290\n",
      "Train Epoch: 64 [12008/22024] \t Train Loss(MSE): 5.3225 \t Train RMSE: 2.3071\n",
      "Train Epoch: 64 [14008/22024] \t Train Loss(MSE): 1.4485 \t Train RMSE: 1.2035\n",
      "Train Epoch: 64 [16008/22024] \t Train Loss(MSE): 13.7664 \t Train RMSE: 3.7103\n",
      "Train Epoch: 64 [18008/22024] \t Train Loss(MSE): 19.2695 \t Train RMSE: 4.3897\n",
      "Train Epoch: 64 [20008/22024] \t Train Loss(MSE): 10.4400 \t Train RMSE: 3.2311\n",
      "Train Epoch: 64 [22008/22024] \t Train Loss(MSE): 8.8613 \t Train RMSE: 2.9768\n",
      "[Epoch: 64 \t Valid MSE: 26.0857 \t Valid RMSE: 4.5861]\n",
      "Train Epoch: 65 [8/22024] \t Train Loss(MSE): 7.1583 \t Train RMSE: 2.6755\n",
      "Train Epoch: 65 [2008/22024] \t Train Loss(MSE): 6.3275 \t Train RMSE: 2.5155\n",
      "Train Epoch: 65 [4008/22024] \t Train Loss(MSE): 15.9598 \t Train RMSE: 3.9950\n",
      "Train Epoch: 65 [6008/22024] \t Train Loss(MSE): 5.6696 \t Train RMSE: 2.3811\n",
      "Train Epoch: 65 [8008/22024] \t Train Loss(MSE): 12.5619 \t Train RMSE: 3.5443\n",
      "Train Epoch: 65 [10008/22024] \t Train Loss(MSE): 1.5425 \t Train RMSE: 1.2420\n",
      "Train Epoch: 65 [12008/22024] \t Train Loss(MSE): 1.7049 \t Train RMSE: 1.3057\n",
      "Train Epoch: 65 [14008/22024] \t Train Loss(MSE): 4.9547 \t Train RMSE: 2.2259\n",
      "Train Epoch: 65 [16008/22024] \t Train Loss(MSE): 4.6466 \t Train RMSE: 2.1556\n",
      "Train Epoch: 65 [18008/22024] \t Train Loss(MSE): 21.1002 \t Train RMSE: 4.5935\n",
      "Train Epoch: 65 [20008/22024] \t Train Loss(MSE): 4.4359 \t Train RMSE: 2.1062\n",
      "Train Epoch: 65 [22008/22024] \t Train Loss(MSE): 2.0682 \t Train RMSE: 1.4381\n",
      "[Epoch: 65 \t Valid MSE: 19.2112 \t Valid RMSE: 3.7366]\n",
      "Train Epoch: 66 [8/22024] \t Train Loss(MSE): 4.3960 \t Train RMSE: 2.0967\n",
      "Train Epoch: 66 [2008/22024] \t Train Loss(MSE): 1.9296 \t Train RMSE: 1.3891\n",
      "Train Epoch: 66 [4008/22024] \t Train Loss(MSE): 1.8586 \t Train RMSE: 1.3633\n",
      "Train Epoch: 66 [6008/22024] \t Train Loss(MSE): 7.3549 \t Train RMSE: 2.7120\n",
      "Train Epoch: 66 [8008/22024] \t Train Loss(MSE): 3.9943 \t Train RMSE: 1.9986\n",
      "Train Epoch: 66 [10008/22024] \t Train Loss(MSE): 5.4785 \t Train RMSE: 2.3406\n",
      "Train Epoch: 66 [12008/22024] \t Train Loss(MSE): 5.8383 \t Train RMSE: 2.4162\n",
      "Train Epoch: 66 [14008/22024] \t Train Loss(MSE): 7.1152 \t Train RMSE: 2.6674\n",
      "Train Epoch: 66 [16008/22024] \t Train Loss(MSE): 24.5282 \t Train RMSE: 4.9526\n",
      "Train Epoch: 66 [18008/22024] \t Train Loss(MSE): 6.0368 \t Train RMSE: 2.4570\n",
      "Train Epoch: 66 [20008/22024] \t Train Loss(MSE): 76.4920 \t Train RMSE: 8.7460\n",
      "Train Epoch: 66 [22008/22024] \t Train Loss(MSE): 4.4172 \t Train RMSE: 2.1017\n",
      "[Epoch: 66 \t Valid MSE: 17.0744 \t Valid RMSE: 3.4748]\n",
      "Train Epoch: 67 [8/22024] \t Train Loss(MSE): 5.9166 \t Train RMSE: 2.4324\n",
      "Train Epoch: 67 [2008/22024] \t Train Loss(MSE): 41.2598 \t Train RMSE: 6.4234\n",
      "Train Epoch: 67 [4008/22024] \t Train Loss(MSE): 1.3680 \t Train RMSE: 1.1696\n",
      "Train Epoch: 67 [6008/22024] \t Train Loss(MSE): 37.5830 \t Train RMSE: 6.1305\n",
      "Train Epoch: 67 [8008/22024] \t Train Loss(MSE): 2.1074 \t Train RMSE: 1.4517\n",
      "Train Epoch: 67 [10008/22024] \t Train Loss(MSE): 12.2978 \t Train RMSE: 3.5068\n",
      "Train Epoch: 67 [12008/22024] \t Train Loss(MSE): 3.1242 \t Train RMSE: 1.7675\n",
      "Train Epoch: 67 [14008/22024] \t Train Loss(MSE): 1.0174 \t Train RMSE: 1.0087\n",
      "Train Epoch: 67 [16008/22024] \t Train Loss(MSE): 18.1357 \t Train RMSE: 4.2586\n",
      "Train Epoch: 67 [18008/22024] \t Train Loss(MSE): 3.4647 \t Train RMSE: 1.8614\n",
      "Train Epoch: 67 [20008/22024] \t Train Loss(MSE): 41.2125 \t Train RMSE: 6.4197\n",
      "Train Epoch: 67 [22008/22024] \t Train Loss(MSE): 14.4006 \t Train RMSE: 3.7948\n",
      "[Epoch: 67 \t Valid MSE: 18.1382 \t Valid RMSE: 3.6044]\n",
      "Train Epoch: 68 [8/22024] \t Train Loss(MSE): 19.1439 \t Train RMSE: 4.3754\n",
      "Train Epoch: 68 [2008/22024] \t Train Loss(MSE): 2.7926 \t Train RMSE: 1.6711\n",
      "Train Epoch: 68 [4008/22024] \t Train Loss(MSE): 9.5241 \t Train RMSE: 3.0861\n",
      "Train Epoch: 68 [6008/22024] \t Train Loss(MSE): 18.2623 \t Train RMSE: 4.2734\n",
      "Train Epoch: 68 [8008/22024] \t Train Loss(MSE): 1.8032 \t Train RMSE: 1.3428\n",
      "Train Epoch: 68 [10008/22024] \t Train Loss(MSE): 5.4709 \t Train RMSE: 2.3390\n",
      "Train Epoch: 68 [12008/22024] \t Train Loss(MSE): 22.7838 \t Train RMSE: 4.7732\n",
      "Train Epoch: 68 [14008/22024] \t Train Loss(MSE): 5.4206 \t Train RMSE: 2.3282\n",
      "Train Epoch: 68 [16008/22024] \t Train Loss(MSE): 82.8563 \t Train RMSE: 9.1025\n",
      "Train Epoch: 68 [18008/22024] \t Train Loss(MSE): 8.5731 \t Train RMSE: 2.9280\n",
      "Train Epoch: 68 [20008/22024] \t Train Loss(MSE): 1.5177 \t Train RMSE: 1.2320\n",
      "Train Epoch: 68 [22008/22024] \t Train Loss(MSE): 8.6581 \t Train RMSE: 2.9425\n",
      "[Epoch: 68 \t Valid MSE: 18.9483 \t Valid RMSE: 3.7481]\n",
      "Train Epoch: 69 [8/22024] \t Train Loss(MSE): 18.3911 \t Train RMSE: 4.2885\n",
      "Train Epoch: 69 [2008/22024] \t Train Loss(MSE): 4.0904 \t Train RMSE: 2.0225\n",
      "Train Epoch: 69 [4008/22024] \t Train Loss(MSE): 5.3475 \t Train RMSE: 2.3125\n",
      "Train Epoch: 69 [6008/22024] \t Train Loss(MSE): 7.6054 \t Train RMSE: 2.7578\n",
      "Train Epoch: 69 [8008/22024] \t Train Loss(MSE): 1.0655 \t Train RMSE: 1.0322\n",
      "Train Epoch: 69 [10008/22024] \t Train Loss(MSE): 14.3553 \t Train RMSE: 3.7888\n",
      "Train Epoch: 69 [12008/22024] \t Train Loss(MSE): 3.1221 \t Train RMSE: 1.7669\n",
      "Train Epoch: 69 [14008/22024] \t Train Loss(MSE): 11.5361 \t Train RMSE: 3.3965\n",
      "Train Epoch: 69 [16008/22024] \t Train Loss(MSE): 14.6196 \t Train RMSE: 3.8236\n",
      "Train Epoch: 69 [18008/22024] \t Train Loss(MSE): 11.8804 \t Train RMSE: 3.4468\n",
      "Train Epoch: 69 [20008/22024] \t Train Loss(MSE): 15.6534 \t Train RMSE: 3.9564\n",
      "Train Epoch: 69 [22008/22024] \t Train Loss(MSE): 6.8835 \t Train RMSE: 2.6236\n",
      "[Epoch: 69 \t Valid MSE: 17.8215 \t Valid RMSE: 3.5244]\n",
      "Train Epoch: 70 [8/22024] \t Train Loss(MSE): 2.7247 \t Train RMSE: 1.6507\n",
      "Train Epoch: 70 [2008/22024] \t Train Loss(MSE): 22.4429 \t Train RMSE: 4.7374\n",
      "Train Epoch: 70 [4008/22024] \t Train Loss(MSE): 13.1254 \t Train RMSE: 3.6229\n",
      "Train Epoch: 70 [6008/22024] \t Train Loss(MSE): 31.1438 \t Train RMSE: 5.5807\n",
      "Train Epoch: 70 [8008/22024] \t Train Loss(MSE): 15.6342 \t Train RMSE: 3.9540\n",
      "Train Epoch: 70 [10008/22024] \t Train Loss(MSE): 3.6524 \t Train RMSE: 1.9111\n",
      "Train Epoch: 70 [12008/22024] \t Train Loss(MSE): 7.4228 \t Train RMSE: 2.7245\n",
      "Train Epoch: 70 [14008/22024] \t Train Loss(MSE): 2.9237 \t Train RMSE: 1.7099\n",
      "Train Epoch: 70 [16008/22024] \t Train Loss(MSE): 4.0324 \t Train RMSE: 2.0081\n",
      "Train Epoch: 70 [18008/22024] \t Train Loss(MSE): 3.1205 \t Train RMSE: 1.7665\n",
      "Train Epoch: 70 [20008/22024] \t Train Loss(MSE): 5.5611 \t Train RMSE: 2.3582\n",
      "Train Epoch: 70 [22008/22024] \t Train Loss(MSE): 1.6072 \t Train RMSE: 1.2677\n",
      "[Epoch: 70 \t Valid MSE: 20.6229 \t Valid RMSE: 3.9794]\n",
      "Train Epoch: 71 [8/22024] \t Train Loss(MSE): 3.7531 \t Train RMSE: 1.9373\n",
      "Train Epoch: 71 [2008/22024] \t Train Loss(MSE): 7.7041 \t Train RMSE: 2.7756\n",
      "Train Epoch: 71 [4008/22024] \t Train Loss(MSE): 20.0872 \t Train RMSE: 4.4819\n",
      "Train Epoch: 71 [6008/22024] \t Train Loss(MSE): 11.8877 \t Train RMSE: 3.4479\n",
      "Train Epoch: 71 [8008/22024] \t Train Loss(MSE): 1.2112 \t Train RMSE: 1.1006\n",
      "Train Epoch: 71 [10008/22024] \t Train Loss(MSE): 8.0467 \t Train RMSE: 2.8367\n",
      "Train Epoch: 71 [12008/22024] \t Train Loss(MSE): 4.4969 \t Train RMSE: 2.1206\n",
      "Train Epoch: 71 [14008/22024] \t Train Loss(MSE): 35.1269 \t Train RMSE: 5.9268\n",
      "Train Epoch: 71 [16008/22024] \t Train Loss(MSE): 6.0132 \t Train RMSE: 2.4522\n",
      "Train Epoch: 71 [18008/22024] \t Train Loss(MSE): 7.5135 \t Train RMSE: 2.7411\n",
      "Train Epoch: 71 [20008/22024] \t Train Loss(MSE): 5.5179 \t Train RMSE: 2.3490\n",
      "Train Epoch: 71 [22008/22024] \t Train Loss(MSE): 1.1818 \t Train RMSE: 1.0871\n",
      "[Epoch: 71 \t Valid MSE: 21.4571 \t Valid RMSE: 4.1271]\n",
      "Train Epoch: 72 [8/22024] \t Train Loss(MSE): 4.1989 \t Train RMSE: 2.0491\n",
      "Train Epoch: 72 [2008/22024] \t Train Loss(MSE): 7.2071 \t Train RMSE: 2.6846\n",
      "Train Epoch: 72 [4008/22024] \t Train Loss(MSE): 86.2612 \t Train RMSE: 9.2877\n",
      "Train Epoch: 72 [6008/22024] \t Train Loss(MSE): 5.5600 \t Train RMSE: 2.3580\n",
      "Train Epoch: 72 [8008/22024] \t Train Loss(MSE): 2.2887 \t Train RMSE: 1.5128\n",
      "Train Epoch: 72 [10008/22024] \t Train Loss(MSE): 1.9218 \t Train RMSE: 1.3863\n",
      "Train Epoch: 72 [12008/22024] \t Train Loss(MSE): 15.9332 \t Train RMSE: 3.9916\n",
      "Train Epoch: 72 [14008/22024] \t Train Loss(MSE): 8.8500 \t Train RMSE: 2.9749\n",
      "Train Epoch: 72 [16008/22024] \t Train Loss(MSE): 4.9550 \t Train RMSE: 2.2260\n",
      "Train Epoch: 72 [18008/22024] \t Train Loss(MSE): 2.7509 \t Train RMSE: 1.6586\n",
      "Train Epoch: 72 [20008/22024] \t Train Loss(MSE): 3.2967 \t Train RMSE: 1.8157\n",
      "Train Epoch: 72 [22008/22024] \t Train Loss(MSE): 17.1143 \t Train RMSE: 4.1369\n",
      "[Epoch: 72 \t Valid MSE: 20.9202 \t Valid RMSE: 3.9957]\n",
      "Train Epoch: 73 [8/22024] \t Train Loss(MSE): 2.0010 \t Train RMSE: 1.4146\n",
      "Train Epoch: 73 [2008/22024] \t Train Loss(MSE): 13.6859 \t Train RMSE: 3.6994\n",
      "Train Epoch: 73 [4008/22024] \t Train Loss(MSE): 4.6094 \t Train RMSE: 2.1470\n",
      "Train Epoch: 73 [6008/22024] \t Train Loss(MSE): 5.5941 \t Train RMSE: 2.3652\n",
      "Train Epoch: 73 [8008/22024] \t Train Loss(MSE): 13.0751 \t Train RMSE: 3.6160\n",
      "Train Epoch: 73 [10008/22024] \t Train Loss(MSE): 4.5402 \t Train RMSE: 2.1308\n",
      "Train Epoch: 73 [12008/22024] \t Train Loss(MSE): 151.9771 \t Train RMSE: 12.3279\n",
      "Train Epoch: 73 [14008/22024] \t Train Loss(MSE): 6.0903 \t Train RMSE: 2.4679\n",
      "Train Epoch: 73 [16008/22024] \t Train Loss(MSE): 3.2034 \t Train RMSE: 1.7898\n",
      "Train Epoch: 73 [18008/22024] \t Train Loss(MSE): 1.5390 \t Train RMSE: 1.2406\n",
      "Train Epoch: 73 [20008/22024] \t Train Loss(MSE): 16.7618 \t Train RMSE: 4.0941\n",
      "Train Epoch: 73 [22008/22024] \t Train Loss(MSE): 52.4798 \t Train RMSE: 7.2443\n",
      "[Epoch: 73 \t Valid MSE: 22.9825 \t Valid RMSE: 4.2728]\n",
      "Train Epoch: 74 [8/22024] \t Train Loss(MSE): 13.9242 \t Train RMSE: 3.7315\n",
      "Train Epoch: 74 [2008/22024] \t Train Loss(MSE): 26.3524 \t Train RMSE: 5.1335\n",
      "Train Epoch: 74 [4008/22024] \t Train Loss(MSE): 0.5177 \t Train RMSE: 0.7195\n",
      "Train Epoch: 74 [6008/22024] \t Train Loss(MSE): 1.1632 \t Train RMSE: 1.0785\n",
      "Train Epoch: 74 [8008/22024] \t Train Loss(MSE): 1.6298 \t Train RMSE: 1.2766\n",
      "Train Epoch: 74 [10008/22024] \t Train Loss(MSE): 6.0130 \t Train RMSE: 2.4521\n",
      "Train Epoch: 74 [12008/22024] \t Train Loss(MSE): 2.3496 \t Train RMSE: 1.5328\n",
      "Train Epoch: 74 [14008/22024] \t Train Loss(MSE): 8.0201 \t Train RMSE: 2.8320\n",
      "Train Epoch: 74 [16008/22024] \t Train Loss(MSE): 49.3952 \t Train RMSE: 7.0282\n",
      "Train Epoch: 74 [18008/22024] \t Train Loss(MSE): 14.5791 \t Train RMSE: 3.8183\n",
      "Train Epoch: 74 [20008/22024] \t Train Loss(MSE): 3.1188 \t Train RMSE: 1.7660\n",
      "Train Epoch: 74 [22008/22024] \t Train Loss(MSE): 7.7945 \t Train RMSE: 2.7919\n",
      "[Epoch: 74 \t Valid MSE: 20.3835 \t Valid RMSE: 3.9357]\n",
      "Train Epoch: 75 [8/22024] \t Train Loss(MSE): 20.1893 \t Train RMSE: 4.4932\n",
      "Train Epoch: 75 [2008/22024] \t Train Loss(MSE): 3.0378 \t Train RMSE: 1.7429\n",
      "Train Epoch: 75 [4008/22024] \t Train Loss(MSE): 12.0374 \t Train RMSE: 3.4695\n",
      "Train Epoch: 75 [6008/22024] \t Train Loss(MSE): 7.2599 \t Train RMSE: 2.6944\n",
      "Train Epoch: 75 [8008/22024] \t Train Loss(MSE): 2.2559 \t Train RMSE: 1.5020\n",
      "Train Epoch: 75 [10008/22024] \t Train Loss(MSE): 21.4947 \t Train RMSE: 4.6362\n",
      "Train Epoch: 75 [12008/22024] \t Train Loss(MSE): 3.6199 \t Train RMSE: 1.9026\n",
      "Train Epoch: 75 [14008/22024] \t Train Loss(MSE): 1.7371 \t Train RMSE: 1.3180\n",
      "Train Epoch: 75 [16008/22024] \t Train Loss(MSE): 6.0129 \t Train RMSE: 2.4521\n",
      "Train Epoch: 75 [18008/22024] \t Train Loss(MSE): 1.1650 \t Train RMSE: 1.0794\n",
      "Train Epoch: 75 [20008/22024] \t Train Loss(MSE): 1.2680 \t Train RMSE: 1.1260\n",
      "Train Epoch: 75 [22008/22024] \t Train Loss(MSE): 1.2764 \t Train RMSE: 1.1298\n",
      "[Epoch: 75 \t Valid MSE: 20.4582 \t Valid RMSE: 3.9211]\n",
      "Train Epoch: 76 [8/22024] \t Train Loss(MSE): 7.7269 \t Train RMSE: 2.7797\n",
      "Train Epoch: 76 [2008/22024] \t Train Loss(MSE): 3.7075 \t Train RMSE: 1.9255\n",
      "Train Epoch: 76 [4008/22024] \t Train Loss(MSE): 5.7723 \t Train RMSE: 2.4026\n",
      "Train Epoch: 76 [6008/22024] \t Train Loss(MSE): 10.5607 \t Train RMSE: 3.2497\n",
      "Train Epoch: 76 [8008/22024] \t Train Loss(MSE): 14.9826 \t Train RMSE: 3.8707\n",
      "Train Epoch: 76 [10008/22024] \t Train Loss(MSE): 33.5846 \t Train RMSE: 5.7952\n",
      "Train Epoch: 76 [12008/22024] \t Train Loss(MSE): 10.6342 \t Train RMSE: 3.2610\n",
      "Train Epoch: 76 [14008/22024] \t Train Loss(MSE): 18.5686 \t Train RMSE: 4.3091\n",
      "Train Epoch: 76 [16008/22024] \t Train Loss(MSE): 14.2730 \t Train RMSE: 3.7780\n",
      "Train Epoch: 76 [18008/22024] \t Train Loss(MSE): 2.9313 \t Train RMSE: 1.7121\n",
      "Train Epoch: 76 [20008/22024] \t Train Loss(MSE): 3.8674 \t Train RMSE: 1.9666\n",
      "Train Epoch: 76 [22008/22024] \t Train Loss(MSE): 4.3924 \t Train RMSE: 2.0958\n",
      "[Epoch: 76 \t Valid MSE: 19.3119 \t Valid RMSE: 3.8758]\n",
      "Train Epoch: 77 [8/22024] \t Train Loss(MSE): 1.7592 \t Train RMSE: 1.3264\n",
      "Train Epoch: 77 [2008/22024] \t Train Loss(MSE): 3.6118 \t Train RMSE: 1.9005\n",
      "Train Epoch: 77 [4008/22024] \t Train Loss(MSE): 16.4648 \t Train RMSE: 4.0577\n",
      "Train Epoch: 77 [6008/22024] \t Train Loss(MSE): 9.5782 \t Train RMSE: 3.0949\n",
      "Train Epoch: 77 [8008/22024] \t Train Loss(MSE): 4.9004 \t Train RMSE: 2.2137\n",
      "Train Epoch: 77 [10008/22024] \t Train Loss(MSE): 32.6564 \t Train RMSE: 5.7146\n",
      "Train Epoch: 77 [12008/22024] \t Train Loss(MSE): 10.1269 \t Train RMSE: 3.1823\n",
      "Train Epoch: 77 [14008/22024] \t Train Loss(MSE): 16.5666 \t Train RMSE: 4.0702\n",
      "Train Epoch: 77 [16008/22024] \t Train Loss(MSE): 4.2094 \t Train RMSE: 2.0517\n",
      "Train Epoch: 77 [18008/22024] \t Train Loss(MSE): 2.9594 \t Train RMSE: 1.7203\n",
      "Train Epoch: 77 [20008/22024] \t Train Loss(MSE): 17.3209 \t Train RMSE: 4.1618\n",
      "Train Epoch: 77 [22008/22024] \t Train Loss(MSE): 88.0357 \t Train RMSE: 9.3827\n",
      "[Epoch: 77 \t Valid MSE: 20.8306 \t Valid RMSE: 3.9606]\n",
      "Train Epoch: 78 [8/22024] \t Train Loss(MSE): 7.0438 \t Train RMSE: 2.6540\n",
      "Train Epoch: 78 [2008/22024] \t Train Loss(MSE): 18.4221 \t Train RMSE: 4.2921\n",
      "Train Epoch: 78 [4008/22024] \t Train Loss(MSE): 4.8541 \t Train RMSE: 2.2032\n",
      "Train Epoch: 78 [6008/22024] \t Train Loss(MSE): 1.3147 \t Train RMSE: 1.1466\n",
      "Train Epoch: 78 [8008/22024] \t Train Loss(MSE): 2.0334 \t Train RMSE: 1.4260\n",
      "Train Epoch: 78 [10008/22024] \t Train Loss(MSE): 4.7495 \t Train RMSE: 2.1793\n",
      "Train Epoch: 78 [12008/22024] \t Train Loss(MSE): 1.5210 \t Train RMSE: 1.2333\n",
      "Train Epoch: 78 [14008/22024] \t Train Loss(MSE): 0.9607 \t Train RMSE: 0.9801\n",
      "Train Epoch: 78 [16008/22024] \t Train Loss(MSE): 2.8430 \t Train RMSE: 1.6861\n",
      "Train Epoch: 78 [18008/22024] \t Train Loss(MSE): 9.6512 \t Train RMSE: 3.1066\n",
      "Train Epoch: 78 [20008/22024] \t Train Loss(MSE): 0.3762 \t Train RMSE: 0.6133\n",
      "Train Epoch: 78 [22008/22024] \t Train Loss(MSE): 6.3582 \t Train RMSE: 2.5215\n",
      "[Epoch: 78 \t Valid MSE: 20.3185 \t Valid RMSE: 3.9185]\n",
      "Train Epoch: 79 [8/22024] \t Train Loss(MSE): 4.9422 \t Train RMSE: 2.2231\n",
      "Train Epoch: 79 [2008/22024] \t Train Loss(MSE): 3.9480 \t Train RMSE: 1.9870\n",
      "Train Epoch: 79 [4008/22024] \t Train Loss(MSE): 7.9042 \t Train RMSE: 2.8114\n",
      "Train Epoch: 79 [6008/22024] \t Train Loss(MSE): 10.5990 \t Train RMSE: 3.2556\n",
      "Train Epoch: 79 [8008/22024] \t Train Loss(MSE): 22.2339 \t Train RMSE: 4.7153\n",
      "Train Epoch: 79 [10008/22024] \t Train Loss(MSE): 9.1343 \t Train RMSE: 3.0223\n",
      "Train Epoch: 79 [12008/22024] \t Train Loss(MSE): 1.5603 \t Train RMSE: 1.2491\n",
      "Train Epoch: 79 [14008/22024] \t Train Loss(MSE): 31.3631 \t Train RMSE: 5.6003\n",
      "Train Epoch: 79 [16008/22024] \t Train Loss(MSE): 4.0253 \t Train RMSE: 2.0063\n",
      "Train Epoch: 79 [18008/22024] \t Train Loss(MSE): 7.3659 \t Train RMSE: 2.7140\n",
      "Train Epoch: 79 [20008/22024] \t Train Loss(MSE): 36.0838 \t Train RMSE: 6.0070\n",
      "Train Epoch: 79 [22008/22024] \t Train Loss(MSE): 12.3840 \t Train RMSE: 3.5191\n",
      "[Epoch: 79 \t Valid MSE: 19.9732 \t Valid RMSE: 3.8788]\n",
      "Train Epoch: 80 [8/22024] \t Train Loss(MSE): 1.6559 \t Train RMSE: 1.2868\n",
      "Train Epoch: 80 [2008/22024] \t Train Loss(MSE): 19.4477 \t Train RMSE: 4.4099\n",
      "Train Epoch: 80 [4008/22024] \t Train Loss(MSE): 16.1997 \t Train RMSE: 4.0249\n",
      "Train Epoch: 80 [6008/22024] \t Train Loss(MSE): 4.5743 \t Train RMSE: 2.1388\n",
      "Train Epoch: 80 [8008/22024] \t Train Loss(MSE): 7.0429 \t Train RMSE: 2.6538\n",
      "Train Epoch: 80 [10008/22024] \t Train Loss(MSE): 2.4313 \t Train RMSE: 1.5593\n",
      "Train Epoch: 80 [12008/22024] \t Train Loss(MSE): 15.0033 \t Train RMSE: 3.8734\n",
      "Train Epoch: 80 [14008/22024] \t Train Loss(MSE): 23.4113 \t Train RMSE: 4.8385\n",
      "Train Epoch: 80 [16008/22024] \t Train Loss(MSE): 8.3314 \t Train RMSE: 2.8864\n",
      "Train Epoch: 80 [18008/22024] \t Train Loss(MSE): 8.3401 \t Train RMSE: 2.8879\n",
      "Train Epoch: 80 [20008/22024] \t Train Loss(MSE): 3.4667 \t Train RMSE: 1.8619\n",
      "Train Epoch: 80 [22008/22024] \t Train Loss(MSE): 7.2285 \t Train RMSE: 2.6886\n",
      "[Epoch: 80 \t Valid MSE: 23.2127 \t Valid RMSE: 4.2587]\n",
      "Train Epoch: 81 [8/22024] \t Train Loss(MSE): 4.2912 \t Train RMSE: 2.0715\n",
      "Train Epoch: 81 [2008/22024] \t Train Loss(MSE): 8.3178 \t Train RMSE: 2.8841\n",
      "Train Epoch: 81 [4008/22024] \t Train Loss(MSE): 56.0041 \t Train RMSE: 7.4836\n",
      "Train Epoch: 81 [6008/22024] \t Train Loss(MSE): 17.0391 \t Train RMSE: 4.1278\n",
      "Train Epoch: 81 [8008/22024] \t Train Loss(MSE): 1.5754 \t Train RMSE: 1.2551\n",
      "Train Epoch: 81 [10008/22024] \t Train Loss(MSE): 4.2310 \t Train RMSE: 2.0569\n",
      "Train Epoch: 81 [12008/22024] \t Train Loss(MSE): 4.3539 \t Train RMSE: 2.0866\n",
      "Train Epoch: 81 [14008/22024] \t Train Loss(MSE): 2.6740 \t Train RMSE: 1.6352\n",
      "Train Epoch: 81 [16008/22024] \t Train Loss(MSE): 4.5643 \t Train RMSE: 2.1364\n",
      "Train Epoch: 81 [18008/22024] \t Train Loss(MSE): 0.9650 \t Train RMSE: 0.9824\n",
      "Train Epoch: 81 [20008/22024] \t Train Loss(MSE): 5.5519 \t Train RMSE: 2.3562\n",
      "Train Epoch: 81 [22008/22024] \t Train Loss(MSE): 1.9235 \t Train RMSE: 1.3869\n",
      "[Epoch: 81 \t Valid MSE: 17.7501 \t Valid RMSE: 3.5421]\n",
      "Train Epoch: 82 [8/22024] \t Train Loss(MSE): 0.4979 \t Train RMSE: 0.7056\n",
      "Train Epoch: 82 [2008/22024] \t Train Loss(MSE): 2.0384 \t Train RMSE: 1.4277\n",
      "Train Epoch: 82 [4008/22024] \t Train Loss(MSE): 16.0087 \t Train RMSE: 4.0011\n",
      "Train Epoch: 82 [6008/22024] \t Train Loss(MSE): 10.3619 \t Train RMSE: 3.2190\n",
      "Train Epoch: 82 [8008/22024] \t Train Loss(MSE): 1.3609 \t Train RMSE: 1.1666\n",
      "Train Epoch: 82 [10008/22024] \t Train Loss(MSE): 20.2898 \t Train RMSE: 4.5044\n",
      "Train Epoch: 82 [12008/22024] \t Train Loss(MSE): 6.9856 \t Train RMSE: 2.6430\n",
      "Train Epoch: 82 [14008/22024] \t Train Loss(MSE): 20.7429 \t Train RMSE: 4.5544\n",
      "Train Epoch: 82 [16008/22024] \t Train Loss(MSE): 24.2039 \t Train RMSE: 4.9198\n",
      "Train Epoch: 82 [18008/22024] \t Train Loss(MSE): 4.7041 \t Train RMSE: 2.1689\n",
      "Train Epoch: 82 [20008/22024] \t Train Loss(MSE): 5.1275 \t Train RMSE: 2.2644\n",
      "Train Epoch: 82 [22008/22024] \t Train Loss(MSE): 4.6310 \t Train RMSE: 2.1520\n",
      "[Epoch: 82 \t Valid MSE: 19.6757 \t Valid RMSE: 3.8433]\n",
      "Train Epoch: 83 [8/22024] \t Train Loss(MSE): 7.9918 \t Train RMSE: 2.8270\n",
      "Train Epoch: 83 [2008/22024] \t Train Loss(MSE): 18.9840 \t Train RMSE: 4.3571\n",
      "Train Epoch: 83 [4008/22024] \t Train Loss(MSE): 19.8153 \t Train RMSE: 4.4514\n",
      "Train Epoch: 83 [6008/22024] \t Train Loss(MSE): 14.2179 \t Train RMSE: 3.7707\n",
      "Train Epoch: 83 [8008/22024] \t Train Loss(MSE): 10.3869 \t Train RMSE: 3.2229\n",
      "Train Epoch: 83 [10008/22024] \t Train Loss(MSE): 7.8025 \t Train RMSE: 2.7933\n",
      "Train Epoch: 83 [12008/22024] \t Train Loss(MSE): 2.3032 \t Train RMSE: 1.5176\n",
      "Train Epoch: 83 [14008/22024] \t Train Loss(MSE): 2.0281 \t Train RMSE: 1.4241\n",
      "Train Epoch: 83 [16008/22024] \t Train Loss(MSE): 20.5499 \t Train RMSE: 4.5332\n",
      "Train Epoch: 83 [18008/22024] \t Train Loss(MSE): 0.7198 \t Train RMSE: 0.8484\n",
      "Train Epoch: 83 [20008/22024] \t Train Loss(MSE): 7.1878 \t Train RMSE: 2.6810\n",
      "Train Epoch: 83 [22008/22024] \t Train Loss(MSE): 4.7089 \t Train RMSE: 2.1700\n",
      "[Epoch: 83 \t Valid MSE: 19.9070 \t Valid RMSE: 3.8723]\n",
      "Train Epoch: 84 [8/22024] \t Train Loss(MSE): 3.1663 \t Train RMSE: 1.7794\n",
      "Train Epoch: 84 [2008/22024] \t Train Loss(MSE): 31.2375 \t Train RMSE: 5.5891\n",
      "Train Epoch: 84 [4008/22024] \t Train Loss(MSE): 30.6211 \t Train RMSE: 5.5336\n",
      "Train Epoch: 84 [6008/22024] \t Train Loss(MSE): 6.5235 \t Train RMSE: 2.5541\n",
      "Train Epoch: 84 [8008/22024] \t Train Loss(MSE): 4.8402 \t Train RMSE: 2.2001\n",
      "Train Epoch: 84 [10008/22024] \t Train Loss(MSE): 6.9136 \t Train RMSE: 2.6294\n",
      "Train Epoch: 84 [12008/22024] \t Train Loss(MSE): 2.5607 \t Train RMSE: 1.6002\n",
      "Train Epoch: 84 [14008/22024] \t Train Loss(MSE): 2.3867 \t Train RMSE: 1.5449\n",
      "Train Epoch: 84 [16008/22024] \t Train Loss(MSE): 2.7042 \t Train RMSE: 1.6444\n",
      "Train Epoch: 84 [18008/22024] \t Train Loss(MSE): 19.6962 \t Train RMSE: 4.4380\n",
      "Train Epoch: 84 [20008/22024] \t Train Loss(MSE): 2.6640 \t Train RMSE: 1.6322\n",
      "Train Epoch: 84 [22008/22024] \t Train Loss(MSE): 27.7105 \t Train RMSE: 5.2641\n",
      "[Epoch: 84 \t Valid MSE: 19.6090 \t Valid RMSE: 3.8255]\n",
      "Train Epoch: 85 [8/22024] \t Train Loss(MSE): 12.8348 \t Train RMSE: 3.5826\n",
      "Train Epoch: 85 [2008/22024] \t Train Loss(MSE): 17.1816 \t Train RMSE: 4.1451\n",
      "Train Epoch: 85 [4008/22024] \t Train Loss(MSE): 33.9457 \t Train RMSE: 5.8263\n",
      "Train Epoch: 85 [6008/22024] \t Train Loss(MSE): 4.7977 \t Train RMSE: 2.1904\n",
      "Train Epoch: 85 [8008/22024] \t Train Loss(MSE): 38.7901 \t Train RMSE: 6.2282\n",
      "Train Epoch: 85 [10008/22024] \t Train Loss(MSE): 7.8753 \t Train RMSE: 2.8063\n",
      "Train Epoch: 85 [12008/22024] \t Train Loss(MSE): 2.4497 \t Train RMSE: 1.5652\n",
      "Train Epoch: 85 [14008/22024] \t Train Loss(MSE): 13.0955 \t Train RMSE: 3.6188\n",
      "Train Epoch: 85 [16008/22024] \t Train Loss(MSE): 3.2558 \t Train RMSE: 1.8044\n",
      "Train Epoch: 85 [18008/22024] \t Train Loss(MSE): 3.1381 \t Train RMSE: 1.7715\n",
      "Train Epoch: 85 [20008/22024] \t Train Loss(MSE): 12.3330 \t Train RMSE: 3.5118\n",
      "Train Epoch: 85 [22008/22024] \t Train Loss(MSE): 14.2254 \t Train RMSE: 3.7717\n",
      "[Epoch: 85 \t Valid MSE: 21.4667 \t Valid RMSE: 4.0880]\n",
      "Train Epoch: 86 [8/22024] \t Train Loss(MSE): 3.9918 \t Train RMSE: 1.9979\n",
      "Train Epoch: 86 [2008/22024] \t Train Loss(MSE): 2.8219 \t Train RMSE: 1.6798\n",
      "Train Epoch: 86 [4008/22024] \t Train Loss(MSE): 12.8698 \t Train RMSE: 3.5875\n",
      "Train Epoch: 86 [6008/22024] \t Train Loss(MSE): 1.8108 \t Train RMSE: 1.3456\n",
      "Train Epoch: 86 [8008/22024] \t Train Loss(MSE): 2.9591 \t Train RMSE: 1.7202\n",
      "Train Epoch: 86 [10008/22024] \t Train Loss(MSE): 4.0708 \t Train RMSE: 2.0176\n",
      "Train Epoch: 86 [12008/22024] \t Train Loss(MSE): 9.7537 \t Train RMSE: 3.1231\n",
      "Train Epoch: 86 [14008/22024] \t Train Loss(MSE): 9.1322 \t Train RMSE: 3.0220\n",
      "Train Epoch: 86 [16008/22024] \t Train Loss(MSE): 1.7618 \t Train RMSE: 1.3273\n",
      "Train Epoch: 86 [18008/22024] \t Train Loss(MSE): 3.5258 \t Train RMSE: 1.8777\n",
      "Train Epoch: 86 [20008/22024] \t Train Loss(MSE): 10.1062 \t Train RMSE: 3.1790\n",
      "Train Epoch: 86 [22008/22024] \t Train Loss(MSE): 10.1547 \t Train RMSE: 3.1866\n",
      "[Epoch: 86 \t Valid MSE: 19.2278 \t Valid RMSE: 3.7979]\n",
      "Train Epoch: 87 [8/22024] \t Train Loss(MSE): 16.8154 \t Train RMSE: 4.1007\n",
      "Train Epoch: 87 [2008/22024] \t Train Loss(MSE): 28.9704 \t Train RMSE: 5.3824\n",
      "Train Epoch: 87 [4008/22024] \t Train Loss(MSE): 6.4543 \t Train RMSE: 2.5405\n",
      "Train Epoch: 87 [6008/22024] \t Train Loss(MSE): 3.4771 \t Train RMSE: 1.8647\n",
      "Train Epoch: 87 [8008/22024] \t Train Loss(MSE): 4.4051 \t Train RMSE: 2.0988\n",
      "Train Epoch: 87 [10008/22024] \t Train Loss(MSE): 3.4366 \t Train RMSE: 1.8538\n",
      "Train Epoch: 87 [12008/22024] \t Train Loss(MSE): 13.7277 \t Train RMSE: 3.7051\n",
      "Train Epoch: 87 [14008/22024] \t Train Loss(MSE): 6.1762 \t Train RMSE: 2.4852\n",
      "Train Epoch: 87 [16008/22024] \t Train Loss(MSE): 13.7980 \t Train RMSE: 3.7146\n",
      "Train Epoch: 87 [18008/22024] \t Train Loss(MSE): 7.5729 \t Train RMSE: 2.7519\n",
      "Train Epoch: 87 [20008/22024] \t Train Loss(MSE): 5.3737 \t Train RMSE: 2.3181\n",
      "Train Epoch: 87 [22008/22024] \t Train Loss(MSE): 3.7803 \t Train RMSE: 1.9443\n",
      "[Epoch: 87 \t Valid MSE: 29.6457 \t Valid RMSE: 4.9695]\n",
      "Train Epoch: 88 [8/22024] \t Train Loss(MSE): 5.6391 \t Train RMSE: 2.3747\n",
      "Train Epoch: 88 [2008/22024] \t Train Loss(MSE): 4.5126 \t Train RMSE: 2.1243\n",
      "Train Epoch: 88 [4008/22024] \t Train Loss(MSE): 4.4293 \t Train RMSE: 2.1046\n",
      "Train Epoch: 88 [6008/22024] \t Train Loss(MSE): 5.4139 \t Train RMSE: 2.3268\n",
      "Train Epoch: 88 [8008/22024] \t Train Loss(MSE): 1.9708 \t Train RMSE: 1.4038\n",
      "Train Epoch: 88 [10008/22024] \t Train Loss(MSE): 7.9143 \t Train RMSE: 2.8132\n",
      "Train Epoch: 88 [12008/22024] \t Train Loss(MSE): 4.8995 \t Train RMSE: 2.2135\n",
      "Train Epoch: 88 [14008/22024] \t Train Loss(MSE): 5.3469 \t Train RMSE: 2.3123\n",
      "Train Epoch: 88 [16008/22024] \t Train Loss(MSE): 2.9169 \t Train RMSE: 1.7079\n",
      "Train Epoch: 88 [18008/22024] \t Train Loss(MSE): 3.3709 \t Train RMSE: 1.8360\n",
      "Train Epoch: 88 [20008/22024] \t Train Loss(MSE): 3.3887 \t Train RMSE: 1.8409\n",
      "Train Epoch: 88 [22008/22024] \t Train Loss(MSE): 23.9112 \t Train RMSE: 4.8899\n",
      "[Epoch: 88 \t Valid MSE: 19.2113 \t Valid RMSE: 3.7597]\n",
      "Train Epoch: 89 [8/22024] \t Train Loss(MSE): 4.8502 \t Train RMSE: 2.2023\n",
      "Train Epoch: 89 [2008/22024] \t Train Loss(MSE): 9.2617 \t Train RMSE: 3.0433\n",
      "Train Epoch: 89 [4008/22024] \t Train Loss(MSE): 4.0525 \t Train RMSE: 2.0131\n",
      "Train Epoch: 89 [6008/22024] \t Train Loss(MSE): 1.5676 \t Train RMSE: 1.2520\n",
      "Train Epoch: 89 [8008/22024] \t Train Loss(MSE): 0.9211 \t Train RMSE: 0.9598\n",
      "Train Epoch: 89 [10008/22024] \t Train Loss(MSE): 5.6429 \t Train RMSE: 2.3755\n",
      "Train Epoch: 89 [12008/22024] \t Train Loss(MSE): 13.2271 \t Train RMSE: 3.6369\n",
      "Train Epoch: 89 [14008/22024] \t Train Loss(MSE): 91.5152 \t Train RMSE: 9.5664\n",
      "Train Epoch: 89 [16008/22024] \t Train Loss(MSE): 27.8162 \t Train RMSE: 5.2741\n",
      "Train Epoch: 89 [18008/22024] \t Train Loss(MSE): 4.5876 \t Train RMSE: 2.1419\n",
      "Train Epoch: 89 [20008/22024] \t Train Loss(MSE): 1.4344 \t Train RMSE: 1.1977\n",
      "Train Epoch: 89 [22008/22024] \t Train Loss(MSE): 6.1456 \t Train RMSE: 2.4790\n",
      "[Epoch: 89 \t Valid MSE: 20.2807 \t Valid RMSE: 3.9018]\n",
      "Train Epoch: 90 [8/22024] \t Train Loss(MSE): 6.8323 \t Train RMSE: 2.6139\n",
      "Train Epoch: 90 [2008/22024] \t Train Loss(MSE): 4.4299 \t Train RMSE: 2.1047\n",
      "Train Epoch: 90 [4008/22024] \t Train Loss(MSE): 3.6136 \t Train RMSE: 1.9009\n",
      "Train Epoch: 90 [6008/22024] \t Train Loss(MSE): 19.3282 \t Train RMSE: 4.3964\n",
      "Train Epoch: 90 [8008/22024] \t Train Loss(MSE): 3.7959 \t Train RMSE: 1.9483\n",
      "Train Epoch: 90 [10008/22024] \t Train Loss(MSE): 17.6084 \t Train RMSE: 4.1962\n",
      "Train Epoch: 90 [12008/22024] \t Train Loss(MSE): 0.8277 \t Train RMSE: 0.9098\n",
      "Train Epoch: 90 [14008/22024] \t Train Loss(MSE): 2.2209 \t Train RMSE: 1.4903\n",
      "Train Epoch: 90 [16008/22024] \t Train Loss(MSE): 19.1772 \t Train RMSE: 4.3792\n",
      "Train Epoch: 90 [18008/22024] \t Train Loss(MSE): 3.8006 \t Train RMSE: 1.9495\n",
      "Train Epoch: 90 [20008/22024] \t Train Loss(MSE): 6.7193 \t Train RMSE: 2.5922\n",
      "Train Epoch: 90 [22008/22024] \t Train Loss(MSE): 5.0852 \t Train RMSE: 2.2550\n",
      "[Epoch: 90 \t Valid MSE: 22.0640 \t Valid RMSE: 4.0154]\n",
      "Train Epoch: 91 [8/22024] \t Train Loss(MSE): 2.7710 \t Train RMSE: 1.6646\n",
      "Train Epoch: 91 [2008/22024] \t Train Loss(MSE): 7.5264 \t Train RMSE: 2.7434\n",
      "Train Epoch: 91 [4008/22024] \t Train Loss(MSE): 11.4838 \t Train RMSE: 3.3888\n",
      "Train Epoch: 91 [6008/22024] \t Train Loss(MSE): 1.7856 \t Train RMSE: 1.3362\n",
      "Train Epoch: 91 [8008/22024] \t Train Loss(MSE): 19.2107 \t Train RMSE: 4.3830\n",
      "Train Epoch: 91 [10008/22024] \t Train Loss(MSE): 41.8820 \t Train RMSE: 6.4716\n",
      "Train Epoch: 91 [12008/22024] \t Train Loss(MSE): 8.0500 \t Train RMSE: 2.8373\n",
      "Train Epoch: 91 [14008/22024] \t Train Loss(MSE): 31.4255 \t Train RMSE: 5.6058\n",
      "Train Epoch: 91 [16008/22024] \t Train Loss(MSE): 4.0951 \t Train RMSE: 2.0236\n",
      "Train Epoch: 91 [18008/22024] \t Train Loss(MSE): 28.4368 \t Train RMSE: 5.3326\n",
      "Train Epoch: 91 [20008/22024] \t Train Loss(MSE): 3.6870 \t Train RMSE: 1.9202\n",
      "Train Epoch: 91 [22008/22024] \t Train Loss(MSE): 9.7143 \t Train RMSE: 3.1168\n",
      "[Epoch: 91 \t Valid MSE: 21.7063 \t Valid RMSE: 4.0130]\n",
      "Train Epoch: 92 [8/22024] \t Train Loss(MSE): 27.5432 \t Train RMSE: 5.2482\n",
      "Train Epoch: 92 [2008/22024] \t Train Loss(MSE): 1.8475 \t Train RMSE: 1.3592\n",
      "Train Epoch: 92 [4008/22024] \t Train Loss(MSE): 1.4385 \t Train RMSE: 1.1994\n",
      "Train Epoch: 92 [6008/22024] \t Train Loss(MSE): 6.3577 \t Train RMSE: 2.5214\n",
      "Train Epoch: 92 [8008/22024] \t Train Loss(MSE): 4.3441 \t Train RMSE: 2.0843\n",
      "Train Epoch: 92 [10008/22024] \t Train Loss(MSE): 3.9815 \t Train RMSE: 1.9954\n",
      "Train Epoch: 92 [12008/22024] \t Train Loss(MSE): 4.7446 \t Train RMSE: 2.1782\n",
      "Train Epoch: 92 [14008/22024] \t Train Loss(MSE): 39.0145 \t Train RMSE: 6.2462\n",
      "Train Epoch: 92 [16008/22024] \t Train Loss(MSE): 12.6363 \t Train RMSE: 3.5548\n",
      "Train Epoch: 92 [18008/22024] \t Train Loss(MSE): 1.2580 \t Train RMSE: 1.1216\n",
      "Train Epoch: 92 [20008/22024] \t Train Loss(MSE): 4.4352 \t Train RMSE: 2.1060\n",
      "Train Epoch: 92 [22008/22024] \t Train Loss(MSE): 11.8545 \t Train RMSE: 3.4430\n",
      "[Epoch: 92 \t Valid MSE: 21.6575 \t Valid RMSE: 4.0990]\n",
      "Train Epoch: 93 [8/22024] \t Train Loss(MSE): 7.1590 \t Train RMSE: 2.6756\n",
      "Train Epoch: 93 [2008/22024] \t Train Loss(MSE): 8.8152 \t Train RMSE: 2.9690\n",
      "Train Epoch: 93 [4008/22024] \t Train Loss(MSE): 0.5229 \t Train RMSE: 0.7231\n",
      "Train Epoch: 93 [6008/22024] \t Train Loss(MSE): 14.7765 \t Train RMSE: 3.8440\n",
      "Train Epoch: 93 [8008/22024] \t Train Loss(MSE): 8.5972 \t Train RMSE: 2.9321\n",
      "Train Epoch: 93 [10008/22024] \t Train Loss(MSE): 15.1840 \t Train RMSE: 3.8967\n",
      "Train Epoch: 93 [12008/22024] \t Train Loss(MSE): 12.3844 \t Train RMSE: 3.5191\n",
      "Train Epoch: 93 [14008/22024] \t Train Loss(MSE): 59.1207 \t Train RMSE: 7.6890\n",
      "Train Epoch: 93 [16008/22024] \t Train Loss(MSE): 5.7377 \t Train RMSE: 2.3954\n",
      "Train Epoch: 93 [18008/22024] \t Train Loss(MSE): 10.2166 \t Train RMSE: 3.1963\n",
      "Train Epoch: 93 [20008/22024] \t Train Loss(MSE): 7.2219 \t Train RMSE: 2.6874\n",
      "Train Epoch: 93 [22008/22024] \t Train Loss(MSE): 10.9154 \t Train RMSE: 3.3039\n",
      "[Epoch: 93 \t Valid MSE: 25.0747 \t Valid RMSE: 4.5617]\n",
      "Train Epoch: 94 [8/22024] \t Train Loss(MSE): 10.0259 \t Train RMSE: 3.1664\n",
      "Train Epoch: 94 [2008/22024] \t Train Loss(MSE): 52.3389 \t Train RMSE: 7.2346\n",
      "Train Epoch: 94 [4008/22024] \t Train Loss(MSE): 6.9548 \t Train RMSE: 2.6372\n",
      "Train Epoch: 94 [6008/22024] \t Train Loss(MSE): 5.6934 \t Train RMSE: 2.3861\n",
      "Train Epoch: 94 [8008/22024] \t Train Loss(MSE): 27.6374 \t Train RMSE: 5.2571\n",
      "Train Epoch: 94 [10008/22024] \t Train Loss(MSE): 10.3077 \t Train RMSE: 3.2106\n",
      "Train Epoch: 94 [12008/22024] \t Train Loss(MSE): 7.9088 \t Train RMSE: 2.8123\n",
      "Train Epoch: 94 [14008/22024] \t Train Loss(MSE): 4.1068 \t Train RMSE: 2.0265\n",
      "Train Epoch: 94 [16008/22024] \t Train Loss(MSE): 4.2841 \t Train RMSE: 2.0698\n",
      "Train Epoch: 94 [18008/22024] \t Train Loss(MSE): 6.0079 \t Train RMSE: 2.4511\n",
      "Train Epoch: 94 [20008/22024] \t Train Loss(MSE): 21.8836 \t Train RMSE: 4.6780\n",
      "Train Epoch: 94 [22008/22024] \t Train Loss(MSE): 24.5732 \t Train RMSE: 4.9571\n",
      "[Epoch: 94 \t Valid MSE: 20.6855 \t Valid RMSE: 3.9533]\n",
      "Train Epoch: 95 [8/22024] \t Train Loss(MSE): 5.4373 \t Train RMSE: 2.3318\n",
      "Train Epoch: 95 [2008/22024] \t Train Loss(MSE): 4.2435 \t Train RMSE: 2.0600\n",
      "Train Epoch: 95 [4008/22024] \t Train Loss(MSE): 2.6586 \t Train RMSE: 1.6305\n",
      "Train Epoch: 95 [6008/22024] \t Train Loss(MSE): 28.9663 \t Train RMSE: 5.3820\n",
      "Train Epoch: 95 [8008/22024] \t Train Loss(MSE): 0.4155 \t Train RMSE: 0.6446\n",
      "Train Epoch: 95 [10008/22024] \t Train Loss(MSE): 6.6976 \t Train RMSE: 2.5880\n",
      "Train Epoch: 95 [12008/22024] \t Train Loss(MSE): 6.3639 \t Train RMSE: 2.5227\n",
      "Train Epoch: 95 [14008/22024] \t Train Loss(MSE): 13.9768 \t Train RMSE: 3.7386\n",
      "Train Epoch: 95 [16008/22024] \t Train Loss(MSE): 5.3319 \t Train RMSE: 2.3091\n",
      "Train Epoch: 95 [18008/22024] \t Train Loss(MSE): 11.3413 \t Train RMSE: 3.3677\n",
      "Train Epoch: 95 [20008/22024] \t Train Loss(MSE): 19.5168 \t Train RMSE: 4.4178\n",
      "Train Epoch: 95 [22008/22024] \t Train Loss(MSE): 10.4906 \t Train RMSE: 3.2389\n",
      "[Epoch: 95 \t Valid MSE: 19.8635 \t Valid RMSE: 3.7609]\n",
      "Train Epoch: 96 [8/22024] \t Train Loss(MSE): 3.6115 \t Train RMSE: 1.9004\n",
      "Train Epoch: 96 [2008/22024] \t Train Loss(MSE): 3.4240 \t Train RMSE: 1.8504\n",
      "Train Epoch: 96 [4008/22024] \t Train Loss(MSE): 29.7926 \t Train RMSE: 5.4583\n",
      "Train Epoch: 96 [6008/22024] \t Train Loss(MSE): 13.1094 \t Train RMSE: 3.6207\n",
      "Train Epoch: 96 [8008/22024] \t Train Loss(MSE): 5.3366 \t Train RMSE: 2.3101\n",
      "Train Epoch: 96 [10008/22024] \t Train Loss(MSE): 26.2827 \t Train RMSE: 5.1267\n",
      "Train Epoch: 96 [12008/22024] \t Train Loss(MSE): 6.9637 \t Train RMSE: 2.6389\n",
      "Train Epoch: 96 [14008/22024] \t Train Loss(MSE): 1.9874 \t Train RMSE: 1.4098\n",
      "Train Epoch: 96 [16008/22024] \t Train Loss(MSE): 2.2010 \t Train RMSE: 1.4836\n",
      "Train Epoch: 96 [18008/22024] \t Train Loss(MSE): 6.6471 \t Train RMSE: 2.5782\n",
      "Train Epoch: 96 [20008/22024] \t Train Loss(MSE): 12.3526 \t Train RMSE: 3.5146\n",
      "Train Epoch: 96 [22008/22024] \t Train Loss(MSE): 1.8190 \t Train RMSE: 1.3487\n",
      "[Epoch: 96 \t Valid MSE: 21.3167 \t Valid RMSE: 4.0183]\n",
      "Train Epoch: 97 [8/22024] \t Train Loss(MSE): 3.0203 \t Train RMSE: 1.7379\n",
      "Train Epoch: 97 [2008/22024] \t Train Loss(MSE): 18.9838 \t Train RMSE: 4.3570\n",
      "Train Epoch: 97 [4008/22024] \t Train Loss(MSE): 2.9711 \t Train RMSE: 1.7237\n",
      "Train Epoch: 97 [6008/22024] \t Train Loss(MSE): 6.6304 \t Train RMSE: 2.5750\n",
      "Train Epoch: 97 [8008/22024] \t Train Loss(MSE): 5.8177 \t Train RMSE: 2.4120\n",
      "Train Epoch: 97 [10008/22024] \t Train Loss(MSE): 21.2086 \t Train RMSE: 4.6053\n",
      "Train Epoch: 97 [12008/22024] \t Train Loss(MSE): 4.7570 \t Train RMSE: 2.1810\n",
      "Train Epoch: 97 [14008/22024] \t Train Loss(MSE): 22.1993 \t Train RMSE: 4.7116\n",
      "Train Epoch: 97 [16008/22024] \t Train Loss(MSE): 3.8577 \t Train RMSE: 1.9641\n",
      "Train Epoch: 97 [18008/22024] \t Train Loss(MSE): 1.9739 \t Train RMSE: 1.4049\n",
      "Train Epoch: 97 [20008/22024] \t Train Loss(MSE): 5.8988 \t Train RMSE: 2.4287\n",
      "Train Epoch: 97 [22008/22024] \t Train Loss(MSE): 2.4482 \t Train RMSE: 1.5647\n",
      "[Epoch: 97 \t Valid MSE: 19.6671 \t Valid RMSE: 3.7726]\n",
      "Train Epoch: 98 [8/22024] \t Train Loss(MSE): 13.5425 \t Train RMSE: 3.6800\n",
      "Train Epoch: 98 [2008/22024] \t Train Loss(MSE): 17.3882 \t Train RMSE: 4.1699\n",
      "Train Epoch: 98 [4008/22024] \t Train Loss(MSE): 16.1223 \t Train RMSE: 4.0153\n",
      "Train Epoch: 98 [6008/22024] \t Train Loss(MSE): 15.3768 \t Train RMSE: 3.9213\n",
      "Train Epoch: 98 [8008/22024] \t Train Loss(MSE): 8.1113 \t Train RMSE: 2.8480\n",
      "Train Epoch: 98 [10008/22024] \t Train Loss(MSE): 3.7005 \t Train RMSE: 1.9237\n",
      "Train Epoch: 98 [12008/22024] \t Train Loss(MSE): 1.2848 \t Train RMSE: 1.1335\n",
      "Train Epoch: 98 [14008/22024] \t Train Loss(MSE): 18.3984 \t Train RMSE: 4.2893\n",
      "Train Epoch: 98 [16008/22024] \t Train Loss(MSE): 7.2973 \t Train RMSE: 2.7013\n",
      "Train Epoch: 98 [18008/22024] \t Train Loss(MSE): 14.9143 \t Train RMSE: 3.8619\n",
      "Train Epoch: 98 [20008/22024] \t Train Loss(MSE): 40.8908 \t Train RMSE: 6.3946\n",
      "Train Epoch: 98 [22008/22024] \t Train Loss(MSE): 4.3431 \t Train RMSE: 2.0840\n",
      "[Epoch: 98 \t Valid MSE: 18.8963 \t Valid RMSE: 3.6392]\n",
      "Train Epoch: 99 [8/22024] \t Train Loss(MSE): 9.3230 \t Train RMSE: 3.0534\n",
      "Train Epoch: 99 [2008/22024] \t Train Loss(MSE): 2.9769 \t Train RMSE: 1.7254\n",
      "Train Epoch: 99 [4008/22024] \t Train Loss(MSE): 6.0245 \t Train RMSE: 2.4545\n",
      "Train Epoch: 99 [6008/22024] \t Train Loss(MSE): 23.7948 \t Train RMSE: 4.8780\n",
      "Train Epoch: 99 [8008/22024] \t Train Loss(MSE): 5.7046 \t Train RMSE: 2.3884\n",
      "Train Epoch: 99 [10008/22024] \t Train Loss(MSE): 7.1916 \t Train RMSE: 2.6817\n",
      "Train Epoch: 99 [12008/22024] \t Train Loss(MSE): 26.9581 \t Train RMSE: 5.1921\n",
      "Train Epoch: 99 [14008/22024] \t Train Loss(MSE): 17.3836 \t Train RMSE: 4.1694\n",
      "Train Epoch: 99 [16008/22024] \t Train Loss(MSE): 26.7905 \t Train RMSE: 5.1760\n",
      "Train Epoch: 99 [18008/22024] \t Train Loss(MSE): 2.3223 \t Train RMSE: 1.5239\n",
      "Train Epoch: 99 [20008/22024] \t Train Loss(MSE): 2.8248 \t Train RMSE: 1.6807\n",
      "Train Epoch: 99 [22008/22024] \t Train Loss(MSE): 0.5951 \t Train RMSE: 0.7714\n",
      "[Epoch: 99 \t Valid MSE: 19.4310 \t Valid RMSE: 3.7805]\n",
      "Train Epoch: 100 [8/22024] \t Train Loss(MSE): 60.0781 \t Train RMSE: 7.7510\n",
      "Train Epoch: 100 [2008/22024] \t Train Loss(MSE): 14.8026 \t Train RMSE: 3.8474\n",
      "Train Epoch: 100 [4008/22024] \t Train Loss(MSE): 11.1193 \t Train RMSE: 3.3346\n",
      "Train Epoch: 100 [6008/22024] \t Train Loss(MSE): 5.3528 \t Train RMSE: 2.3136\n",
      "Train Epoch: 100 [8008/22024] \t Train Loss(MSE): 1.3407 \t Train RMSE: 1.1579\n",
      "Train Epoch: 100 [10008/22024] \t Train Loss(MSE): 19.0023 \t Train RMSE: 4.3592\n",
      "Train Epoch: 100 [12008/22024] \t Train Loss(MSE): 19.7046 \t Train RMSE: 4.4390\n",
      "Train Epoch: 100 [14008/22024] \t Train Loss(MSE): 43.7608 \t Train RMSE: 6.6152\n",
      "Train Epoch: 100 [16008/22024] \t Train Loss(MSE): 0.7648 \t Train RMSE: 0.8746\n",
      "Train Epoch: 100 [18008/22024] \t Train Loss(MSE): 12.0672 \t Train RMSE: 3.4738\n",
      "Train Epoch: 100 [20008/22024] \t Train Loss(MSE): 6.7289 \t Train RMSE: 2.5940\n",
      "Train Epoch: 100 [22008/22024] \t Train Loss(MSE): 1.6425 \t Train RMSE: 1.2816\n",
      "[Epoch: 100 \t Valid MSE: 19.2951 \t Valid RMSE: 3.7893]\n",
      "Train Epoch: 101 [8/22024] \t Train Loss(MSE): 2.8759 \t Train RMSE: 1.6958\n",
      "Train Epoch: 101 [2008/22024] \t Train Loss(MSE): 18.4176 \t Train RMSE: 4.2916\n",
      "Train Epoch: 101 [4008/22024] \t Train Loss(MSE): 1.4513 \t Train RMSE: 1.2047\n",
      "Train Epoch: 101 [6008/22024] \t Train Loss(MSE): 6.4320 \t Train RMSE: 2.5361\n",
      "Train Epoch: 101 [8008/22024] \t Train Loss(MSE): 1.0354 \t Train RMSE: 1.0176\n",
      "Train Epoch: 101 [10008/22024] \t Train Loss(MSE): 3.1412 \t Train RMSE: 1.7723\n",
      "Train Epoch: 101 [12008/22024] \t Train Loss(MSE): 2.7767 \t Train RMSE: 1.6663\n",
      "Train Epoch: 101 [14008/22024] \t Train Loss(MSE): 0.9151 \t Train RMSE: 0.9566\n",
      "Train Epoch: 101 [16008/22024] \t Train Loss(MSE): 21.5487 \t Train RMSE: 4.6421\n",
      "Train Epoch: 101 [18008/22024] \t Train Loss(MSE): 15.8408 \t Train RMSE: 3.9800\n",
      "Train Epoch: 101 [20008/22024] \t Train Loss(MSE): 1.5331 \t Train RMSE: 1.2382\n",
      "Train Epoch: 101 [22008/22024] \t Train Loss(MSE): 2.7166 \t Train RMSE: 1.6482\n",
      "[Epoch: 101 \t Valid MSE: 20.1006 \t Valid RMSE: 3.8965]\n",
      "Train Epoch: 102 [8/22024] \t Train Loss(MSE): 5.9493 \t Train RMSE: 2.4391\n",
      "Train Epoch: 102 [2008/22024] \t Train Loss(MSE): 4.7551 \t Train RMSE: 2.1806\n",
      "Train Epoch: 102 [4008/22024] \t Train Loss(MSE): 9.6596 \t Train RMSE: 3.1080\n",
      "Train Epoch: 102 [6008/22024] \t Train Loss(MSE): 6.6066 \t Train RMSE: 2.5703\n",
      "Train Epoch: 102 [8008/22024] \t Train Loss(MSE): 2.2390 \t Train RMSE: 1.4963\n",
      "Train Epoch: 102 [10008/22024] \t Train Loss(MSE): 1.2473 \t Train RMSE: 1.1168\n",
      "Train Epoch: 102 [12008/22024] \t Train Loss(MSE): 17.4671 \t Train RMSE: 4.1794\n",
      "Train Epoch: 102 [14008/22024] \t Train Loss(MSE): 2.2653 \t Train RMSE: 1.5051\n",
      "Train Epoch: 102 [16008/22024] \t Train Loss(MSE): 70.3268 \t Train RMSE: 8.3861\n",
      "Train Epoch: 102 [18008/22024] \t Train Loss(MSE): 6.2907 \t Train RMSE: 2.5081\n",
      "Train Epoch: 102 [20008/22024] \t Train Loss(MSE): 1.1182 \t Train RMSE: 1.0575\n",
      "Train Epoch: 102 [22008/22024] \t Train Loss(MSE): 3.3911 \t Train RMSE: 1.8415\n",
      "[Epoch: 102 \t Valid MSE: 23.0437 \t Valid RMSE: 4.3218]\n",
      "Train Epoch: 103 [8/22024] \t Train Loss(MSE): 4.5035 \t Train RMSE: 2.1221\n",
      "Train Epoch: 103 [2008/22024] \t Train Loss(MSE): 0.6567 \t Train RMSE: 0.8103\n",
      "Train Epoch: 103 [4008/22024] \t Train Loss(MSE): 2.3131 \t Train RMSE: 1.5209\n",
      "Train Epoch: 103 [6008/22024] \t Train Loss(MSE): 6.2942 \t Train RMSE: 2.5088\n",
      "Train Epoch: 103 [8008/22024] \t Train Loss(MSE): 79.6319 \t Train RMSE: 8.9237\n",
      "Train Epoch: 103 [10008/22024] \t Train Loss(MSE): 16.9408 \t Train RMSE: 4.1159\n",
      "Train Epoch: 103 [12008/22024] \t Train Loss(MSE): 2.8298 \t Train RMSE: 1.6822\n",
      "Train Epoch: 103 [14008/22024] \t Train Loss(MSE): 15.0116 \t Train RMSE: 3.8745\n",
      "Train Epoch: 103 [16008/22024] \t Train Loss(MSE): 0.8223 \t Train RMSE: 0.9068\n",
      "Train Epoch: 103 [18008/22024] \t Train Loss(MSE): 22.2544 \t Train RMSE: 4.7175\n",
      "Train Epoch: 103 [20008/22024] \t Train Loss(MSE): 2.5237 \t Train RMSE: 1.5886\n",
      "Train Epoch: 103 [22008/22024] \t Train Loss(MSE): 2.8803 \t Train RMSE: 1.6971\n",
      "[Epoch: 103 \t Valid MSE: 23.2405 \t Valid RMSE: 4.2300]\n",
      "Train Epoch: 104 [8/22024] \t Train Loss(MSE): 6.4061 \t Train RMSE: 2.5310\n",
      "Train Epoch: 104 [2008/22024] \t Train Loss(MSE): 9.2090 \t Train RMSE: 3.0346\n",
      "Train Epoch: 104 [4008/22024] \t Train Loss(MSE): 47.2922 \t Train RMSE: 6.8769\n",
      "Train Epoch: 104 [6008/22024] \t Train Loss(MSE): 7.3582 \t Train RMSE: 2.7126\n",
      "Train Epoch: 104 [8008/22024] \t Train Loss(MSE): 24.2451 \t Train RMSE: 4.9239\n",
      "Train Epoch: 104 [10008/22024] \t Train Loss(MSE): 3.2703 \t Train RMSE: 1.8084\n",
      "Train Epoch: 104 [12008/22024] \t Train Loss(MSE): 2.3910 \t Train RMSE: 1.5463\n",
      "Train Epoch: 104 [14008/22024] \t Train Loss(MSE): 43.3709 \t Train RMSE: 6.5857\n",
      "Train Epoch: 104 [16008/22024] \t Train Loss(MSE): 43.9695 \t Train RMSE: 6.6310\n",
      "Train Epoch: 104 [18008/22024] \t Train Loss(MSE): 10.0370 \t Train RMSE: 3.1681\n",
      "Train Epoch: 104 [20008/22024] \t Train Loss(MSE): 104.0430 \t Train RMSE: 10.2001\n",
      "Train Epoch: 104 [22008/22024] \t Train Loss(MSE): 86.8952 \t Train RMSE: 9.3218\n",
      "[Epoch: 104 \t Valid MSE: 20.3001 \t Valid RMSE: 3.8890]\n",
      "Train Epoch: 105 [8/22024] \t Train Loss(MSE): 8.6365 \t Train RMSE: 2.9388\n",
      "Train Epoch: 105 [2008/22024] \t Train Loss(MSE): 4.3346 \t Train RMSE: 2.0820\n",
      "Train Epoch: 105 [4008/22024] \t Train Loss(MSE): 23.5982 \t Train RMSE: 4.8578\n",
      "Train Epoch: 105 [6008/22024] \t Train Loss(MSE): 13.6995 \t Train RMSE: 3.7013\n",
      "Train Epoch: 105 [8008/22024] \t Train Loss(MSE): 14.1429 \t Train RMSE: 3.7607\n",
      "Train Epoch: 105 [10008/22024] \t Train Loss(MSE): 0.9768 \t Train RMSE: 0.9884\n",
      "Train Epoch: 105 [12008/22024] \t Train Loss(MSE): 13.1918 \t Train RMSE: 3.6321\n",
      "Train Epoch: 105 [14008/22024] \t Train Loss(MSE): 7.8215 \t Train RMSE: 2.7967\n",
      "Train Epoch: 105 [16008/22024] \t Train Loss(MSE): 13.6032 \t Train RMSE: 3.6883\n",
      "Train Epoch: 105 [18008/22024] \t Train Loss(MSE): 5.2455 \t Train RMSE: 2.2903\n",
      "Train Epoch: 105 [20008/22024] \t Train Loss(MSE): 45.5590 \t Train RMSE: 6.7497\n",
      "Train Epoch: 105 [22008/22024] \t Train Loss(MSE): 3.6797 \t Train RMSE: 1.9183\n",
      "[Epoch: 105 \t Valid MSE: 24.5063 \t Valid RMSE: 4.4199]\n",
      "Train Epoch: 106 [8/22024] \t Train Loss(MSE): 2.8994 \t Train RMSE: 1.7028\n",
      "Train Epoch: 106 [2008/22024] \t Train Loss(MSE): 24.0034 \t Train RMSE: 4.8993\n",
      "Train Epoch: 106 [4008/22024] \t Train Loss(MSE): 7.0576 \t Train RMSE: 2.6566\n",
      "Train Epoch: 106 [6008/22024] \t Train Loss(MSE): 2.2319 \t Train RMSE: 1.4940\n",
      "Train Epoch: 106 [8008/22024] \t Train Loss(MSE): 15.1795 \t Train RMSE: 3.8961\n",
      "Train Epoch: 106 [10008/22024] \t Train Loss(MSE): 5.0895 \t Train RMSE: 2.2560\n",
      "Train Epoch: 106 [12008/22024] \t Train Loss(MSE): 8.1188 \t Train RMSE: 2.8493\n",
      "Train Epoch: 106 [14008/22024] \t Train Loss(MSE): 8.1678 \t Train RMSE: 2.8579\n",
      "Train Epoch: 106 [16008/22024] \t Train Loss(MSE): 1.2400 \t Train RMSE: 1.1136\n",
      "Train Epoch: 106 [18008/22024] \t Train Loss(MSE): 24.9641 \t Train RMSE: 4.9964\n",
      "Train Epoch: 106 [20008/22024] \t Train Loss(MSE): 17.7744 \t Train RMSE: 4.2160\n",
      "Train Epoch: 106 [22008/22024] \t Train Loss(MSE): 1.3695 \t Train RMSE: 1.1703\n",
      "[Epoch: 106 \t Valid MSE: 19.1038 \t Valid RMSE: 3.6599]\n",
      "Train Epoch: 107 [8/22024] \t Train Loss(MSE): 4.7857 \t Train RMSE: 2.1876\n",
      "Train Epoch: 107 [2008/22024] \t Train Loss(MSE): 49.0348 \t Train RMSE: 7.0025\n",
      "Train Epoch: 107 [4008/22024] \t Train Loss(MSE): 39.4735 \t Train RMSE: 6.2828\n",
      "Train Epoch: 107 [6008/22024] \t Train Loss(MSE): 3.9272 \t Train RMSE: 1.9817\n",
      "Train Epoch: 107 [8008/22024] \t Train Loss(MSE): 4.9225 \t Train RMSE: 2.2187\n",
      "Train Epoch: 107 [10008/22024] \t Train Loss(MSE): 20.5470 \t Train RMSE: 4.5329\n",
      "Train Epoch: 107 [12008/22024] \t Train Loss(MSE): 15.9093 \t Train RMSE: 3.9886\n",
      "Train Epoch: 107 [14008/22024] \t Train Loss(MSE): 14.0938 \t Train RMSE: 3.7542\n",
      "Train Epoch: 107 [16008/22024] \t Train Loss(MSE): 15.4184 \t Train RMSE: 3.9266\n",
      "Train Epoch: 107 [18008/22024] \t Train Loss(MSE): 3.6841 \t Train RMSE: 1.9194\n",
      "Train Epoch: 107 [20008/22024] \t Train Loss(MSE): 2.8188 \t Train RMSE: 1.6789\n",
      "Train Epoch: 107 [22008/22024] \t Train Loss(MSE): 0.2529 \t Train RMSE: 0.5029\n",
      "[Epoch: 107 \t Valid MSE: 19.8497 \t Valid RMSE: 3.7787]\n",
      "Train Epoch: 108 [8/22024] \t Train Loss(MSE): 11.9321 \t Train RMSE: 3.4543\n",
      "Train Epoch: 108 [2008/22024] \t Train Loss(MSE): 1.0222 \t Train RMSE: 1.0110\n",
      "Train Epoch: 108 [4008/22024] \t Train Loss(MSE): 10.6566 \t Train RMSE: 3.2645\n",
      "Train Epoch: 108 [6008/22024] \t Train Loss(MSE): 0.8332 \t Train RMSE: 0.9128\n",
      "Train Epoch: 108 [8008/22024] \t Train Loss(MSE): 6.3884 \t Train RMSE: 2.5275\n",
      "Train Epoch: 108 [10008/22024] \t Train Loss(MSE): 8.4608 \t Train RMSE: 2.9088\n",
      "Train Epoch: 108 [12008/22024] \t Train Loss(MSE): 59.3687 \t Train RMSE: 7.7051\n",
      "Train Epoch: 108 [14008/22024] \t Train Loss(MSE): 1.1156 \t Train RMSE: 1.0562\n",
      "Train Epoch: 108 [16008/22024] \t Train Loss(MSE): 8.2392 \t Train RMSE: 2.8704\n",
      "Train Epoch: 108 [18008/22024] \t Train Loss(MSE): 10.2735 \t Train RMSE: 3.2052\n",
      "Train Epoch: 108 [20008/22024] \t Train Loss(MSE): 4.3186 \t Train RMSE: 2.0781\n",
      "Train Epoch: 108 [22008/22024] \t Train Loss(MSE): 4.4949 \t Train RMSE: 2.1201\n",
      "[Epoch: 108 \t Valid MSE: 19.0424 \t Valid RMSE: 3.7223]\n",
      "Train Epoch: 109 [8/22024] \t Train Loss(MSE): 80.7596 \t Train RMSE: 8.9866\n",
      "Train Epoch: 109 [2008/22024] \t Train Loss(MSE): 4.7958 \t Train RMSE: 2.1899\n",
      "Train Epoch: 109 [4008/22024] \t Train Loss(MSE): 1.4015 \t Train RMSE: 1.1839\n",
      "Train Epoch: 109 [6008/22024] \t Train Loss(MSE): 17.6593 \t Train RMSE: 4.2023\n",
      "Train Epoch: 109 [8008/22024] \t Train Loss(MSE): 2.2904 \t Train RMSE: 1.5134\n",
      "Train Epoch: 109 [10008/22024] \t Train Loss(MSE): 1.5933 \t Train RMSE: 1.2623\n",
      "Train Epoch: 109 [12008/22024] \t Train Loss(MSE): 4.2914 \t Train RMSE: 2.0716\n",
      "Train Epoch: 109 [14008/22024] \t Train Loss(MSE): 4.0993 \t Train RMSE: 2.0247\n",
      "Train Epoch: 109 [16008/22024] \t Train Loss(MSE): 14.8483 \t Train RMSE: 3.8534\n",
      "Train Epoch: 109 [18008/22024] \t Train Loss(MSE): 3.7101 \t Train RMSE: 1.9262\n",
      "Train Epoch: 109 [20008/22024] \t Train Loss(MSE): 3.4155 \t Train RMSE: 1.8481\n",
      "Train Epoch: 109 [22008/22024] \t Train Loss(MSE): 9.1906 \t Train RMSE: 3.0316\n",
      "[Epoch: 109 \t Valid MSE: 21.9805 \t Valid RMSE: 4.0701]\n",
      "Train Epoch: 110 [8/22024] \t Train Loss(MSE): 6.1593 \t Train RMSE: 2.4818\n",
      "Train Epoch: 110 [2008/22024] \t Train Loss(MSE): 6.0397 \t Train RMSE: 2.4576\n",
      "Train Epoch: 110 [4008/22024] \t Train Loss(MSE): 14.7740 \t Train RMSE: 3.8437\n",
      "Train Epoch: 110 [6008/22024] \t Train Loss(MSE): 18.2494 \t Train RMSE: 4.2719\n",
      "Train Epoch: 110 [8008/22024] \t Train Loss(MSE): 3.5657 \t Train RMSE: 1.8883\n",
      "Train Epoch: 110 [10008/22024] \t Train Loss(MSE): 3.3601 \t Train RMSE: 1.8331\n",
      "Train Epoch: 110 [12008/22024] \t Train Loss(MSE): 4.6010 \t Train RMSE: 2.1450\n",
      "Train Epoch: 110 [14008/22024] \t Train Loss(MSE): 3.1401 \t Train RMSE: 1.7720\n",
      "Train Epoch: 110 [16008/22024] \t Train Loss(MSE): 9.4780 \t Train RMSE: 3.0786\n",
      "Train Epoch: 110 [18008/22024] \t Train Loss(MSE): 17.6634 \t Train RMSE: 4.2028\n",
      "Train Epoch: 110 [20008/22024] \t Train Loss(MSE): 1.2457 \t Train RMSE: 1.1161\n",
      "Train Epoch: 110 [22008/22024] \t Train Loss(MSE): 14.6644 \t Train RMSE: 3.8294\n",
      "[Epoch: 110 \t Valid MSE: 21.9302 \t Valid RMSE: 4.0825]\n",
      "Train Epoch: 111 [8/22024] \t Train Loss(MSE): 7.2819 \t Train RMSE: 2.6985\n",
      "Train Epoch: 111 [2008/22024] \t Train Loss(MSE): 5.2951 \t Train RMSE: 2.3011\n",
      "Train Epoch: 111 [4008/22024] \t Train Loss(MSE): 12.9355 \t Train RMSE: 3.5966\n",
      "Train Epoch: 111 [6008/22024] \t Train Loss(MSE): 43.8462 \t Train RMSE: 6.6216\n",
      "Train Epoch: 111 [8008/22024] \t Train Loss(MSE): 7.4131 \t Train RMSE: 2.7227\n",
      "Train Epoch: 111 [10008/22024] \t Train Loss(MSE): 14.8923 \t Train RMSE: 3.8590\n",
      "Train Epoch: 111 [12008/22024] \t Train Loss(MSE): 29.4365 \t Train RMSE: 5.4255\n",
      "Train Epoch: 111 [14008/22024] \t Train Loss(MSE): 12.9120 \t Train RMSE: 3.5933\n",
      "Train Epoch: 111 [16008/22024] \t Train Loss(MSE): 4.7232 \t Train RMSE: 2.1733\n",
      "Train Epoch: 111 [18008/22024] \t Train Loss(MSE): 13.4303 \t Train RMSE: 3.6647\n",
      "Train Epoch: 111 [20008/22024] \t Train Loss(MSE): 29.0095 \t Train RMSE: 5.3861\n",
      "Train Epoch: 111 [22008/22024] \t Train Loss(MSE): 10.7081 \t Train RMSE: 3.2723\n",
      "[Epoch: 111 \t Valid MSE: 19.0909 \t Valid RMSE: 3.7773]\n",
      "Train Epoch: 112 [8/22024] \t Train Loss(MSE): 14.6533 \t Train RMSE: 3.8280\n",
      "Train Epoch: 112 [2008/22024] \t Train Loss(MSE): 6.9817 \t Train RMSE: 2.6423\n",
      "Train Epoch: 112 [4008/22024] \t Train Loss(MSE): 59.3282 \t Train RMSE: 7.7025\n",
      "Train Epoch: 112 [6008/22024] \t Train Loss(MSE): 1.3121 \t Train RMSE: 1.1455\n",
      "Train Epoch: 112 [8008/22024] \t Train Loss(MSE): 24.7067 \t Train RMSE: 4.9706\n",
      "Train Epoch: 112 [10008/22024] \t Train Loss(MSE): 23.8300 \t Train RMSE: 4.8816\n",
      "Train Epoch: 112 [12008/22024] \t Train Loss(MSE): 7.0378 \t Train RMSE: 2.6529\n",
      "Train Epoch: 112 [14008/22024] \t Train Loss(MSE): 8.2151 \t Train RMSE: 2.8662\n",
      "Train Epoch: 112 [16008/22024] \t Train Loss(MSE): 60.6488 \t Train RMSE: 7.7877\n",
      "Train Epoch: 112 [18008/22024] \t Train Loss(MSE): 3.1770 \t Train RMSE: 1.7824\n",
      "Train Epoch: 112 [20008/22024] \t Train Loss(MSE): 1.7934 \t Train RMSE: 1.3392\n",
      "Train Epoch: 112 [22008/22024] \t Train Loss(MSE): 13.6752 \t Train RMSE: 3.6980\n",
      "[Epoch: 112 \t Valid MSE: 23.3470 \t Valid RMSE: 4.2542]\n",
      "Train Epoch: 113 [8/22024] \t Train Loss(MSE): 23.0993 \t Train RMSE: 4.8062\n",
      "Train Epoch: 113 [2008/22024] \t Train Loss(MSE): 1.2687 \t Train RMSE: 1.1263\n",
      "Train Epoch: 113 [4008/22024] \t Train Loss(MSE): 19.6457 \t Train RMSE: 4.4323\n",
      "Train Epoch: 113 [6008/22024] \t Train Loss(MSE): 36.7626 \t Train RMSE: 6.0632\n",
      "Train Epoch: 113 [8008/22024] \t Train Loss(MSE): 23.3986 \t Train RMSE: 4.8372\n",
      "Train Epoch: 113 [10008/22024] \t Train Loss(MSE): 38.0813 \t Train RMSE: 6.1710\n",
      "Train Epoch: 113 [12008/22024] \t Train Loss(MSE): 4.7869 \t Train RMSE: 2.1879\n",
      "Train Epoch: 113 [14008/22024] \t Train Loss(MSE): 5.5713 \t Train RMSE: 2.3604\n",
      "Train Epoch: 113 [16008/22024] \t Train Loss(MSE): 8.3694 \t Train RMSE: 2.8930\n",
      "Train Epoch: 113 [18008/22024] \t Train Loss(MSE): 14.3226 \t Train RMSE: 3.7845\n",
      "Train Epoch: 113 [20008/22024] \t Train Loss(MSE): 4.6408 \t Train RMSE: 2.1543\n",
      "Train Epoch: 113 [22008/22024] \t Train Loss(MSE): 2.8242 \t Train RMSE: 1.6805\n",
      "[Epoch: 113 \t Valid MSE: 20.1232 \t Valid RMSE: 3.8959]\n",
      "Train Epoch: 114 [8/22024] \t Train Loss(MSE): 8.1990 \t Train RMSE: 2.8634\n",
      "Train Epoch: 114 [2008/22024] \t Train Loss(MSE): 16.5691 \t Train RMSE: 4.0705\n",
      "Train Epoch: 114 [4008/22024] \t Train Loss(MSE): 41.1824 \t Train RMSE: 6.4174\n",
      "Train Epoch: 114 [6008/22024] \t Train Loss(MSE): 6.1179 \t Train RMSE: 2.4734\n",
      "Train Epoch: 114 [8008/22024] \t Train Loss(MSE): 29.6619 \t Train RMSE: 5.4463\n",
      "Train Epoch: 114 [10008/22024] \t Train Loss(MSE): 1.0728 \t Train RMSE: 1.0358\n",
      "Train Epoch: 114 [12008/22024] \t Train Loss(MSE): 24.0028 \t Train RMSE: 4.8993\n",
      "Train Epoch: 114 [14008/22024] \t Train Loss(MSE): 13.2453 \t Train RMSE: 3.6394\n",
      "Train Epoch: 114 [16008/22024] \t Train Loss(MSE): 6.4490 \t Train RMSE: 2.5395\n",
      "Train Epoch: 114 [18008/22024] \t Train Loss(MSE): 0.8162 \t Train RMSE: 0.9035\n",
      "Train Epoch: 114 [20008/22024] \t Train Loss(MSE): 31.8243 \t Train RMSE: 5.6413\n",
      "Train Epoch: 114 [22008/22024] \t Train Loss(MSE): 33.7443 \t Train RMSE: 5.8090\n",
      "[Epoch: 114 \t Valid MSE: 24.7690 \t Valid RMSE: 4.2727]\n",
      "Train Epoch: 115 [8/22024] \t Train Loss(MSE): 5.1158 \t Train RMSE: 2.2618\n",
      "Train Epoch: 115 [2008/22024] \t Train Loss(MSE): 30.1582 \t Train RMSE: 5.4916\n",
      "Train Epoch: 115 [4008/22024] \t Train Loss(MSE): 2.9736 \t Train RMSE: 1.7244\n",
      "Train Epoch: 115 [6008/22024] \t Train Loss(MSE): 49.6001 \t Train RMSE: 7.0427\n",
      "Train Epoch: 115 [8008/22024] \t Train Loss(MSE): 4.5359 \t Train RMSE: 2.1298\n",
      "Train Epoch: 115 [10008/22024] \t Train Loss(MSE): 35.1967 \t Train RMSE: 5.9327\n",
      "Train Epoch: 115 [12008/22024] \t Train Loss(MSE): 37.3979 \t Train RMSE: 6.1154\n",
      "Train Epoch: 115 [14008/22024] \t Train Loss(MSE): 5.2149 \t Train RMSE: 2.2836\n",
      "Train Epoch: 115 [16008/22024] \t Train Loss(MSE): 0.4553 \t Train RMSE: 0.6748\n",
      "Train Epoch: 115 [18008/22024] \t Train Loss(MSE): 165.2691 \t Train RMSE: 12.8557\n",
      "Train Epoch: 115 [20008/22024] \t Train Loss(MSE): 6.3312 \t Train RMSE: 2.5162\n",
      "Train Epoch: 115 [22008/22024] \t Train Loss(MSE): 17.2271 \t Train RMSE: 4.1506\n",
      "[Epoch: 115 \t Valid MSE: 24.8865 \t Valid RMSE: 4.4208]\n",
      "Train Epoch: 116 [8/22024] \t Train Loss(MSE): 14.3831 \t Train RMSE: 3.7925\n",
      "Train Epoch: 116 [2008/22024] \t Train Loss(MSE): 6.5924 \t Train RMSE: 2.5676\n",
      "Train Epoch: 116 [4008/22024] \t Train Loss(MSE): 2.4775 \t Train RMSE: 1.5740\n",
      "Train Epoch: 116 [6008/22024] \t Train Loss(MSE): 29.5660 \t Train RMSE: 5.4375\n",
      "Train Epoch: 116 [8008/22024] \t Train Loss(MSE): 5.2016 \t Train RMSE: 2.2807\n",
      "Train Epoch: 116 [10008/22024] \t Train Loss(MSE): 1.8940 \t Train RMSE: 1.3762\n",
      "Train Epoch: 116 [12008/22024] \t Train Loss(MSE): 11.6674 \t Train RMSE: 3.4158\n",
      "Train Epoch: 116 [14008/22024] \t Train Loss(MSE): 2.9918 \t Train RMSE: 1.7297\n",
      "Train Epoch: 116 [16008/22024] \t Train Loss(MSE): 7.4359 \t Train RMSE: 2.7269\n",
      "Train Epoch: 116 [18008/22024] \t Train Loss(MSE): 5.2637 \t Train RMSE: 2.2943\n",
      "Train Epoch: 116 [20008/22024] \t Train Loss(MSE): 3.4746 \t Train RMSE: 1.8640\n",
      "Train Epoch: 116 [22008/22024] \t Train Loss(MSE): 3.0347 \t Train RMSE: 1.7420\n",
      "[Epoch: 116 \t Valid MSE: 23.5202 \t Valid RMSE: 4.3118]\n",
      "Train Epoch: 117 [8/22024] \t Train Loss(MSE): 2.1452 \t Train RMSE: 1.4647\n",
      "Train Epoch: 117 [2008/22024] \t Train Loss(MSE): 3.8184 \t Train RMSE: 1.9541\n",
      "Train Epoch: 117 [4008/22024] \t Train Loss(MSE): 12.5645 \t Train RMSE: 3.5447\n",
      "Train Epoch: 117 [6008/22024] \t Train Loss(MSE): 12.9279 \t Train RMSE: 3.5955\n",
      "Train Epoch: 117 [8008/22024] \t Train Loss(MSE): 2.4405 \t Train RMSE: 1.5622\n",
      "Train Epoch: 117 [10008/22024] \t Train Loss(MSE): 5.0946 \t Train RMSE: 2.2571\n",
      "Train Epoch: 117 [12008/22024] \t Train Loss(MSE): 4.8411 \t Train RMSE: 2.2002\n",
      "Train Epoch: 117 [14008/22024] \t Train Loss(MSE): 9.3790 \t Train RMSE: 3.0625\n",
      "Train Epoch: 117 [16008/22024] \t Train Loss(MSE): 31.2416 \t Train RMSE: 5.5894\n",
      "Train Epoch: 117 [18008/22024] \t Train Loss(MSE): 11.3780 \t Train RMSE: 3.3731\n",
      "Train Epoch: 117 [20008/22024] \t Train Loss(MSE): 2.7106 \t Train RMSE: 1.6464\n",
      "Train Epoch: 117 [22008/22024] \t Train Loss(MSE): 4.3466 \t Train RMSE: 2.0849\n",
      "[Epoch: 117 \t Valid MSE: 19.6886 \t Valid RMSE: 3.8511]\n",
      "Train Epoch: 118 [8/22024] \t Train Loss(MSE): 9.6417 \t Train RMSE: 3.1051\n",
      "Train Epoch: 118 [2008/22024] \t Train Loss(MSE): 53.2322 \t Train RMSE: 7.2960\n",
      "Train Epoch: 118 [4008/22024] \t Train Loss(MSE): 1.8651 \t Train RMSE: 1.3657\n",
      "Train Epoch: 118 [6008/22024] \t Train Loss(MSE): 1.4222 \t Train RMSE: 1.1926\n",
      "Train Epoch: 118 [8008/22024] \t Train Loss(MSE): 19.8251 \t Train RMSE: 4.4525\n",
      "Train Epoch: 118 [10008/22024] \t Train Loss(MSE): 1.4676 \t Train RMSE: 1.2114\n",
      "Train Epoch: 118 [12008/22024] \t Train Loss(MSE): 24.2806 \t Train RMSE: 4.9275\n",
      "Train Epoch: 118 [14008/22024] \t Train Loss(MSE): 7.8136 \t Train RMSE: 2.7953\n",
      "Train Epoch: 118 [16008/22024] \t Train Loss(MSE): 37.1196 \t Train RMSE: 6.0926\n",
      "Train Epoch: 118 [18008/22024] \t Train Loss(MSE): 9.3814 \t Train RMSE: 3.0629\n",
      "Train Epoch: 118 [20008/22024] \t Train Loss(MSE): 30.3232 \t Train RMSE: 5.5066\n",
      "Train Epoch: 118 [22008/22024] \t Train Loss(MSE): 11.8833 \t Train RMSE: 3.4472\n",
      "[Epoch: 118 \t Valid MSE: 19.3651 \t Valid RMSE: 3.7995]\n",
      "Train Epoch: 119 [8/22024] \t Train Loss(MSE): 1.7914 \t Train RMSE: 1.3384\n",
      "Train Epoch: 119 [2008/22024] \t Train Loss(MSE): 3.3202 \t Train RMSE: 1.8222\n",
      "Train Epoch: 119 [4008/22024] \t Train Loss(MSE): 11.4055 \t Train RMSE: 3.3772\n",
      "Train Epoch: 119 [6008/22024] \t Train Loss(MSE): 5.5841 \t Train RMSE: 2.3631\n",
      "Train Epoch: 119 [8008/22024] \t Train Loss(MSE): 37.6262 \t Train RMSE: 6.1340\n",
      "Train Epoch: 119 [10008/22024] \t Train Loss(MSE): 5.2906 \t Train RMSE: 2.3001\n",
      "Train Epoch: 119 [12008/22024] \t Train Loss(MSE): 14.5524 \t Train RMSE: 3.8148\n",
      "Train Epoch: 119 [14008/22024] \t Train Loss(MSE): 8.1118 \t Train RMSE: 2.8481\n",
      "Train Epoch: 119 [16008/22024] \t Train Loss(MSE): 2.5265 \t Train RMSE: 1.5895\n",
      "Train Epoch: 119 [18008/22024] \t Train Loss(MSE): 11.6882 \t Train RMSE: 3.4188\n",
      "Train Epoch: 119 [20008/22024] \t Train Loss(MSE): 8.5357 \t Train RMSE: 2.9216\n",
      "Train Epoch: 119 [22008/22024] \t Train Loss(MSE): 19.2467 \t Train RMSE: 4.3871\n",
      "[Epoch: 119 \t Valid MSE: 21.9673 \t Valid RMSE: 4.1264]\n",
      "Train Epoch: 120 [8/22024] \t Train Loss(MSE): 2.6039 \t Train RMSE: 1.6136\n",
      "Train Epoch: 120 [2008/22024] \t Train Loss(MSE): 1.9573 \t Train RMSE: 1.3990\n",
      "Train Epoch: 120 [4008/22024] \t Train Loss(MSE): 75.6659 \t Train RMSE: 8.6986\n",
      "Train Epoch: 120 [6008/22024] \t Train Loss(MSE): 6.2936 \t Train RMSE: 2.5087\n",
      "Train Epoch: 120 [8008/22024] \t Train Loss(MSE): 1.5149 \t Train RMSE: 1.2308\n",
      "Train Epoch: 120 [10008/22024] \t Train Loss(MSE): 2.4616 \t Train RMSE: 1.5690\n",
      "Train Epoch: 120 [12008/22024] \t Train Loss(MSE): 17.8288 \t Train RMSE: 4.2224\n",
      "Train Epoch: 120 [14008/22024] \t Train Loss(MSE): 43.1006 \t Train RMSE: 6.5651\n",
      "Train Epoch: 120 [16008/22024] \t Train Loss(MSE): 2.4698 \t Train RMSE: 1.5716\n",
      "Train Epoch: 120 [18008/22024] \t Train Loss(MSE): 198.5503 \t Train RMSE: 14.0908\n",
      "Train Epoch: 120 [20008/22024] \t Train Loss(MSE): 27.0614 \t Train RMSE: 5.2021\n",
      "Train Epoch: 120 [22008/22024] \t Train Loss(MSE): 7.4459 \t Train RMSE: 2.7287\n",
      "[Epoch: 120 \t Valid MSE: 17.8013 \t Valid RMSE: 3.5213]\n",
      "Train Epoch: 121 [8/22024] \t Train Loss(MSE): 0.9487 \t Train RMSE: 0.9740\n",
      "Train Epoch: 121 [2008/22024] \t Train Loss(MSE): 11.5795 \t Train RMSE: 3.4029\n",
      "Train Epoch: 121 [4008/22024] \t Train Loss(MSE): 3.2791 \t Train RMSE: 1.8108\n",
      "Train Epoch: 121 [6008/22024] \t Train Loss(MSE): 15.4612 \t Train RMSE: 3.9321\n",
      "Train Epoch: 121 [8008/22024] \t Train Loss(MSE): 24.3333 \t Train RMSE: 4.9329\n",
      "Train Epoch: 121 [10008/22024] \t Train Loss(MSE): 2.4813 \t Train RMSE: 1.5752\n",
      "Train Epoch: 121 [12008/22024] \t Train Loss(MSE): 8.8566 \t Train RMSE: 2.9760\n",
      "Train Epoch: 121 [14008/22024] \t Train Loss(MSE): 8.1423 \t Train RMSE: 2.8535\n",
      "Train Epoch: 121 [16008/22024] \t Train Loss(MSE): 2.6021 \t Train RMSE: 1.6131\n",
      "Train Epoch: 121 [18008/22024] \t Train Loss(MSE): 25.9449 \t Train RMSE: 5.0936\n",
      "Train Epoch: 121 [20008/22024] \t Train Loss(MSE): 16.9224 \t Train RMSE: 4.1137\n",
      "Train Epoch: 121 [22008/22024] \t Train Loss(MSE): 2.8876 \t Train RMSE: 1.6993\n",
      "[Epoch: 121 \t Valid MSE: 18.9353 \t Valid RMSE: 3.6392]\n",
      "Train Epoch: 122 [8/22024] \t Train Loss(MSE): 5.7498 \t Train RMSE: 2.3979\n",
      "Train Epoch: 122 [2008/22024] \t Train Loss(MSE): 4.1766 \t Train RMSE: 2.0437\n",
      "Train Epoch: 122 [4008/22024] \t Train Loss(MSE): 2.3827 \t Train RMSE: 1.5436\n",
      "Train Epoch: 122 [6008/22024] \t Train Loss(MSE): 8.9696 \t Train RMSE: 2.9949\n",
      "Train Epoch: 122 [8008/22024] \t Train Loss(MSE): 7.7432 \t Train RMSE: 2.7827\n",
      "Train Epoch: 122 [10008/22024] \t Train Loss(MSE): 0.6281 \t Train RMSE: 0.7925\n",
      "Train Epoch: 122 [12008/22024] \t Train Loss(MSE): 13.6174 \t Train RMSE: 3.6902\n",
      "Train Epoch: 122 [14008/22024] \t Train Loss(MSE): 7.9097 \t Train RMSE: 2.8124\n",
      "Train Epoch: 122 [16008/22024] \t Train Loss(MSE): 8.3589 \t Train RMSE: 2.8912\n",
      "Train Epoch: 122 [18008/22024] \t Train Loss(MSE): 3.9140 \t Train RMSE: 1.9784\n",
      "Train Epoch: 122 [20008/22024] \t Train Loss(MSE): 5.3443 \t Train RMSE: 2.3118\n",
      "Train Epoch: 122 [22008/22024] \t Train Loss(MSE): 7.9450 \t Train RMSE: 2.8187\n",
      "[Epoch: 122 \t Valid MSE: 24.0695 \t Valid RMSE: 4.3489]\n",
      "Train Epoch: 123 [8/22024] \t Train Loss(MSE): 14.5171 \t Train RMSE: 3.8101\n",
      "Train Epoch: 123 [2008/22024] \t Train Loss(MSE): 6.8986 \t Train RMSE: 2.6265\n",
      "Train Epoch: 123 [4008/22024] \t Train Loss(MSE): 1.1223 \t Train RMSE: 1.0594\n",
      "Train Epoch: 123 [6008/22024] \t Train Loss(MSE): 17.3282 \t Train RMSE: 4.1627\n",
      "Train Epoch: 123 [8008/22024] \t Train Loss(MSE): 2.5723 \t Train RMSE: 1.6039\n",
      "Train Epoch: 123 [10008/22024] \t Train Loss(MSE): 6.8364 \t Train RMSE: 2.6146\n",
      "Train Epoch: 123 [12008/22024] \t Train Loss(MSE): 1.7277 \t Train RMSE: 1.3144\n",
      "Train Epoch: 123 [14008/22024] \t Train Loss(MSE): 19.5767 \t Train RMSE: 4.4246\n",
      "Train Epoch: 123 [16008/22024] \t Train Loss(MSE): 16.7054 \t Train RMSE: 4.0872\n",
      "Train Epoch: 123 [18008/22024] \t Train Loss(MSE): 12.8075 \t Train RMSE: 3.5788\n",
      "Train Epoch: 123 [20008/22024] \t Train Loss(MSE): 23.6562 \t Train RMSE: 4.8638\n",
      "Train Epoch: 123 [22008/22024] \t Train Loss(MSE): 11.6367 \t Train RMSE: 3.4113\n",
      "[Epoch: 123 \t Valid MSE: 23.0629 \t Valid RMSE: 4.2724]\n",
      "Train Epoch: 124 [8/22024] \t Train Loss(MSE): 30.0153 \t Train RMSE: 5.4786\n",
      "Train Epoch: 124 [2008/22024] \t Train Loss(MSE): 6.8879 \t Train RMSE: 2.6245\n",
      "Train Epoch: 124 [4008/22024] \t Train Loss(MSE): 4.8473 \t Train RMSE: 2.2016\n",
      "Train Epoch: 124 [6008/22024] \t Train Loss(MSE): 5.0166 \t Train RMSE: 2.2398\n",
      "Train Epoch: 124 [8008/22024] \t Train Loss(MSE): 4.6818 \t Train RMSE: 2.1638\n",
      "Train Epoch: 124 [10008/22024] \t Train Loss(MSE): 0.4914 \t Train RMSE: 0.7010\n",
      "Train Epoch: 124 [12008/22024] \t Train Loss(MSE): 7.4587 \t Train RMSE: 2.7311\n",
      "Train Epoch: 124 [14008/22024] \t Train Loss(MSE): 2.7824 \t Train RMSE: 1.6680\n",
      "Train Epoch: 124 [16008/22024] \t Train Loss(MSE): 20.5558 \t Train RMSE: 4.5338\n",
      "Train Epoch: 124 [18008/22024] \t Train Loss(MSE): 7.3560 \t Train RMSE: 2.7122\n",
      "Train Epoch: 124 [20008/22024] \t Train Loss(MSE): 7.1127 \t Train RMSE: 2.6670\n",
      "Train Epoch: 124 [22008/22024] \t Train Loss(MSE): 16.2625 \t Train RMSE: 4.0327\n",
      "[Epoch: 124 \t Valid MSE: 20.5982 \t Valid RMSE: 3.8721]\n",
      "Train Epoch: 125 [8/22024] \t Train Loss(MSE): 1.4673 \t Train RMSE: 1.2113\n",
      "Train Epoch: 125 [2008/22024] \t Train Loss(MSE): 4.7205 \t Train RMSE: 2.1727\n",
      "Train Epoch: 125 [4008/22024] \t Train Loss(MSE): 3.0368 \t Train RMSE: 1.7427\n",
      "Train Epoch: 125 [6008/22024] \t Train Loss(MSE): 14.9004 \t Train RMSE: 3.8601\n",
      "Train Epoch: 125 [8008/22024] \t Train Loss(MSE): 32.4240 \t Train RMSE: 5.6942\n",
      "Train Epoch: 125 [10008/22024] \t Train Loss(MSE): 4.0020 \t Train RMSE: 2.0005\n",
      "Train Epoch: 125 [12008/22024] \t Train Loss(MSE): 46.9860 \t Train RMSE: 6.8546\n",
      "Train Epoch: 125 [14008/22024] \t Train Loss(MSE): 2.5179 \t Train RMSE: 1.5868\n",
      "Train Epoch: 125 [16008/22024] \t Train Loss(MSE): 5.4354 \t Train RMSE: 2.3314\n",
      "Train Epoch: 125 [18008/22024] \t Train Loss(MSE): 1.8275 \t Train RMSE: 1.3518\n",
      "Train Epoch: 125 [20008/22024] \t Train Loss(MSE): 3.8048 \t Train RMSE: 1.9506\n",
      "Train Epoch: 125 [22008/22024] \t Train Loss(MSE): 2.5186 \t Train RMSE: 1.5870\n",
      "[Epoch: 125 \t Valid MSE: 20.7879 \t Valid RMSE: 3.9147]\n",
      "Train Epoch: 126 [8/22024] \t Train Loss(MSE): 15.8304 \t Train RMSE: 3.9787\n",
      "Train Epoch: 126 [2008/22024] \t Train Loss(MSE): 2.4155 \t Train RMSE: 1.5542\n",
      "Train Epoch: 126 [4008/22024] \t Train Loss(MSE): 67.7516 \t Train RMSE: 8.2311\n",
      "Train Epoch: 126 [6008/22024] \t Train Loss(MSE): 13.9616 \t Train RMSE: 3.7365\n",
      "Train Epoch: 126 [8008/22024] \t Train Loss(MSE): 0.6384 \t Train RMSE: 0.7990\n",
      "Train Epoch: 126 [10008/22024] \t Train Loss(MSE): 35.6438 \t Train RMSE: 5.9702\n",
      "Train Epoch: 126 [12008/22024] \t Train Loss(MSE): 1.4002 \t Train RMSE: 1.1833\n",
      "Train Epoch: 126 [14008/22024] \t Train Loss(MSE): 6.2433 \t Train RMSE: 2.4987\n",
      "Train Epoch: 126 [16008/22024] \t Train Loss(MSE): 21.3430 \t Train RMSE: 4.6198\n",
      "Train Epoch: 126 [18008/22024] \t Train Loss(MSE): 2.3496 \t Train RMSE: 1.5328\n",
      "Train Epoch: 126 [20008/22024] \t Train Loss(MSE): 10.7484 \t Train RMSE: 3.2785\n",
      "Train Epoch: 126 [22008/22024] \t Train Loss(MSE): 9.0397 \t Train RMSE: 3.0066\n",
      "[Epoch: 126 \t Valid MSE: 25.8670 \t Valid RMSE: 4.6060]\n",
      "Train Epoch: 127 [8/22024] \t Train Loss(MSE): 19.7629 \t Train RMSE: 4.4456\n",
      "Train Epoch: 127 [2008/22024] \t Train Loss(MSE): 5.1620 \t Train RMSE: 2.2720\n",
      "Train Epoch: 127 [4008/22024] \t Train Loss(MSE): 3.8653 \t Train RMSE: 1.9660\n",
      "Train Epoch: 127 [6008/22024] \t Train Loss(MSE): 10.2701 \t Train RMSE: 3.2047\n",
      "Train Epoch: 127 [8008/22024] \t Train Loss(MSE): 0.5284 \t Train RMSE: 0.7269\n",
      "Train Epoch: 127 [10008/22024] \t Train Loss(MSE): 12.0225 \t Train RMSE: 3.4674\n",
      "Train Epoch: 127 [12008/22024] \t Train Loss(MSE): 16.4184 \t Train RMSE: 4.0520\n",
      "Train Epoch: 127 [14008/22024] \t Train Loss(MSE): 3.4381 \t Train RMSE: 1.8542\n",
      "Train Epoch: 127 [16008/22024] \t Train Loss(MSE): 19.6600 \t Train RMSE: 4.4340\n",
      "Train Epoch: 127 [18008/22024] \t Train Loss(MSE): 10.3672 \t Train RMSE: 3.2198\n",
      "Train Epoch: 127 [20008/22024] \t Train Loss(MSE): 43.9770 \t Train RMSE: 6.6315\n",
      "Train Epoch: 127 [22008/22024] \t Train Loss(MSE): 24.4564 \t Train RMSE: 4.9453\n",
      "[Epoch: 127 \t Valid MSE: 22.9824 \t Valid RMSE: 4.2581]\n",
      "Train Epoch: 128 [8/22024] \t Train Loss(MSE): 1.4145 \t Train RMSE: 1.1893\n",
      "Train Epoch: 128 [2008/22024] \t Train Loss(MSE): 6.5824 \t Train RMSE: 2.5656\n",
      "Train Epoch: 128 [4008/22024] \t Train Loss(MSE): 3.3362 \t Train RMSE: 1.8265\n",
      "Train Epoch: 128 [6008/22024] \t Train Loss(MSE): 23.6641 \t Train RMSE: 4.8646\n",
      "Train Epoch: 128 [8008/22024] \t Train Loss(MSE): 5.0193 \t Train RMSE: 2.2404\n",
      "Train Epoch: 128 [10008/22024] \t Train Loss(MSE): 4.3712 \t Train RMSE: 2.0907\n",
      "Train Epoch: 128 [12008/22024] \t Train Loss(MSE): 1.9769 \t Train RMSE: 1.4060\n",
      "Train Epoch: 128 [14008/22024] \t Train Loss(MSE): 6.4383 \t Train RMSE: 2.5374\n",
      "Train Epoch: 128 [16008/22024] \t Train Loss(MSE): 1.3594 \t Train RMSE: 1.1659\n",
      "Train Epoch: 128 [18008/22024] \t Train Loss(MSE): 6.4622 \t Train RMSE: 2.5421\n",
      "Train Epoch: 128 [20008/22024] \t Train Loss(MSE): 3.2670 \t Train RMSE: 1.8075\n",
      "Train Epoch: 128 [22008/22024] \t Train Loss(MSE): 1.7382 \t Train RMSE: 1.3184\n",
      "[Epoch: 128 \t Valid MSE: 22.0249 \t Valid RMSE: 4.1211]\n",
      "Train Epoch: 129 [8/22024] \t Train Loss(MSE): 10.2092 \t Train RMSE: 3.1952\n",
      "Train Epoch: 129 [2008/22024] \t Train Loss(MSE): 2.9683 \t Train RMSE: 1.7229\n",
      "Train Epoch: 129 [4008/22024] \t Train Loss(MSE): 1.9838 \t Train RMSE: 1.4085\n",
      "Train Epoch: 129 [6008/22024] \t Train Loss(MSE): 16.1430 \t Train RMSE: 4.0178\n",
      "Train Epoch: 129 [8008/22024] \t Train Loss(MSE): 12.9608 \t Train RMSE: 3.6001\n",
      "Train Epoch: 129 [10008/22024] \t Train Loss(MSE): 8.2329 \t Train RMSE: 2.8693\n",
      "Train Epoch: 129 [12008/22024] \t Train Loss(MSE): 16.6522 \t Train RMSE: 4.0807\n",
      "Train Epoch: 129 [14008/22024] \t Train Loss(MSE): 7.3946 \t Train RMSE: 2.7193\n",
      "Train Epoch: 129 [16008/22024] \t Train Loss(MSE): 13.8490 \t Train RMSE: 3.7214\n",
      "Train Epoch: 129 [18008/22024] \t Train Loss(MSE): 0.9250 \t Train RMSE: 0.9618\n",
      "Train Epoch: 129 [20008/22024] \t Train Loss(MSE): 10.4279 \t Train RMSE: 3.2292\n",
      "Train Epoch: 129 [22008/22024] \t Train Loss(MSE): 2.3119 \t Train RMSE: 1.5205\n",
      "[Epoch: 129 \t Valid MSE: 20.0963 \t Valid RMSE: 3.8779]\n",
      "Train Epoch: 130 [8/22024] \t Train Loss(MSE): 5.2823 \t Train RMSE: 2.2983\n",
      "Train Epoch: 130 [2008/22024] \t Train Loss(MSE): 26.8833 \t Train RMSE: 5.1849\n",
      "Train Epoch: 130 [4008/22024] \t Train Loss(MSE): 1.3813 \t Train RMSE: 1.1753\n",
      "Train Epoch: 130 [6008/22024] \t Train Loss(MSE): 6.0214 \t Train RMSE: 2.4539\n",
      "Train Epoch: 130 [8008/22024] \t Train Loss(MSE): 5.1505 \t Train RMSE: 2.2695\n",
      "Train Epoch: 130 [10008/22024] \t Train Loss(MSE): 4.7660 \t Train RMSE: 2.1831\n",
      "Train Epoch: 130 [12008/22024] \t Train Loss(MSE): 4.9988 \t Train RMSE: 2.2358\n",
      "Train Epoch: 130 [14008/22024] \t Train Loss(MSE): 13.0908 \t Train RMSE: 3.6181\n",
      "Train Epoch: 130 [16008/22024] \t Train Loss(MSE): 2.6004 \t Train RMSE: 1.6126\n",
      "Train Epoch: 130 [18008/22024] \t Train Loss(MSE): 0.8064 \t Train RMSE: 0.8980\n",
      "Train Epoch: 130 [20008/22024] \t Train Loss(MSE): 3.2855 \t Train RMSE: 1.8126\n",
      "Train Epoch: 130 [22008/22024] \t Train Loss(MSE): 3.5168 \t Train RMSE: 1.8753\n",
      "[Epoch: 130 \t Valid MSE: 19.5533 \t Valid RMSE: 3.8215]\n",
      "Train Epoch: 131 [8/22024] \t Train Loss(MSE): 6.0830 \t Train RMSE: 2.4664\n",
      "Train Epoch: 131 [2008/22024] \t Train Loss(MSE): 2.8655 \t Train RMSE: 1.6928\n",
      "Train Epoch: 131 [4008/22024] \t Train Loss(MSE): 4.0697 \t Train RMSE: 2.0174\n",
      "Train Epoch: 131 [6008/22024] \t Train Loss(MSE): 8.7552 \t Train RMSE: 2.9589\n",
      "Train Epoch: 131 [8008/22024] \t Train Loss(MSE): 12.4478 \t Train RMSE: 3.5281\n",
      "Train Epoch: 131 [10008/22024] \t Train Loss(MSE): 13.3316 \t Train RMSE: 3.6512\n",
      "Train Epoch: 131 [12008/22024] \t Train Loss(MSE): 11.1245 \t Train RMSE: 3.3353\n",
      "Train Epoch: 131 [14008/22024] \t Train Loss(MSE): 4.5102 \t Train RMSE: 2.1237\n",
      "Train Epoch: 131 [16008/22024] \t Train Loss(MSE): 3.3121 \t Train RMSE: 1.8199\n",
      "Train Epoch: 131 [18008/22024] \t Train Loss(MSE): 3.2721 \t Train RMSE: 1.8089\n",
      "Train Epoch: 131 [20008/22024] \t Train Loss(MSE): 1.1432 \t Train RMSE: 1.0692\n",
      "Train Epoch: 131 [22008/22024] \t Train Loss(MSE): 3.0652 \t Train RMSE: 1.7508\n",
      "[Epoch: 131 \t Valid MSE: 19.5032 \t Valid RMSE: 3.7932]\n",
      "Train Epoch: 132 [8/22024] \t Train Loss(MSE): 7.3782 \t Train RMSE: 2.7163\n",
      "Train Epoch: 132 [2008/22024] \t Train Loss(MSE): 9.0329 \t Train RMSE: 3.0055\n",
      "Train Epoch: 132 [4008/22024] \t Train Loss(MSE): 11.8401 \t Train RMSE: 3.4409\n",
      "Train Epoch: 132 [6008/22024] \t Train Loss(MSE): 28.7861 \t Train RMSE: 5.3653\n",
      "Train Epoch: 132 [8008/22024] \t Train Loss(MSE): 4.6907 \t Train RMSE: 2.1658\n",
      "Train Epoch: 132 [10008/22024] \t Train Loss(MSE): 4.0121 \t Train RMSE: 2.0030\n",
      "Train Epoch: 132 [12008/22024] \t Train Loss(MSE): 2.3920 \t Train RMSE: 1.5466\n",
      "Train Epoch: 132 [14008/22024] \t Train Loss(MSE): 2.8678 \t Train RMSE: 1.6935\n",
      "Train Epoch: 132 [16008/22024] \t Train Loss(MSE): 1.1595 \t Train RMSE: 1.0768\n",
      "Train Epoch: 132 [18008/22024] \t Train Loss(MSE): 7.5396 \t Train RMSE: 2.7458\n",
      "Train Epoch: 132 [20008/22024] \t Train Loss(MSE): 7.2756 \t Train RMSE: 2.6973\n",
      "Train Epoch: 132 [22008/22024] \t Train Loss(MSE): 144.2377 \t Train RMSE: 12.0099\n",
      "[Epoch: 132 \t Valid MSE: 18.1661 \t Valid RMSE: 3.6020]\n",
      "Train Epoch: 133 [8/22024] \t Train Loss(MSE): 4.0356 \t Train RMSE: 2.0089\n",
      "Train Epoch: 133 [2008/22024] \t Train Loss(MSE): 1.9460 \t Train RMSE: 1.3950\n",
      "Train Epoch: 133 [4008/22024] \t Train Loss(MSE): 16.4852 \t Train RMSE: 4.0602\n",
      "Train Epoch: 133 [6008/22024] \t Train Loss(MSE): 3.4111 \t Train RMSE: 1.8469\n",
      "Train Epoch: 133 [8008/22024] \t Train Loss(MSE): 11.0974 \t Train RMSE: 3.3313\n",
      "Train Epoch: 133 [10008/22024] \t Train Loss(MSE): 2.2596 \t Train RMSE: 1.5032\n",
      "Train Epoch: 133 [12008/22024] \t Train Loss(MSE): 12.8824 \t Train RMSE: 3.5892\n",
      "Train Epoch: 133 [14008/22024] \t Train Loss(MSE): 137.5619 \t Train RMSE: 11.7287\n",
      "Train Epoch: 133 [16008/22024] \t Train Loss(MSE): 4.2884 \t Train RMSE: 2.0709\n",
      "Train Epoch: 133 [18008/22024] \t Train Loss(MSE): 5.3506 \t Train RMSE: 2.3131\n",
      "Train Epoch: 133 [20008/22024] \t Train Loss(MSE): 45.0146 \t Train RMSE: 6.7093\n",
      "Train Epoch: 133 [22008/22024] \t Train Loss(MSE): 0.2923 \t Train RMSE: 0.5407\n",
      "[Epoch: 133 \t Valid MSE: 19.5509 \t Valid RMSE: 3.8170]\n",
      "Train Epoch: 134 [8/22024] \t Train Loss(MSE): 4.6084 \t Train RMSE: 2.1467\n",
      "Train Epoch: 134 [2008/22024] \t Train Loss(MSE): 0.9688 \t Train RMSE: 0.9843\n",
      "Train Epoch: 134 [4008/22024] \t Train Loss(MSE): 2.6477 \t Train RMSE: 1.6272\n",
      "Train Epoch: 134 [6008/22024] \t Train Loss(MSE): 2.1299 \t Train RMSE: 1.4594\n",
      "Train Epoch: 134 [8008/22024] \t Train Loss(MSE): 21.3608 \t Train RMSE: 4.6218\n",
      "Train Epoch: 134 [10008/22024] \t Train Loss(MSE): 14.4688 \t Train RMSE: 3.8038\n",
      "Train Epoch: 134 [12008/22024] \t Train Loss(MSE): 8.5590 \t Train RMSE: 2.9256\n",
      "Train Epoch: 134 [14008/22024] \t Train Loss(MSE): 17.0158 \t Train RMSE: 4.1250\n",
      "Train Epoch: 134 [16008/22024] \t Train Loss(MSE): 5.1040 \t Train RMSE: 2.2592\n",
      "Train Epoch: 134 [18008/22024] \t Train Loss(MSE): 37.3469 \t Train RMSE: 6.1112\n",
      "Train Epoch: 134 [20008/22024] \t Train Loss(MSE): 7.4769 \t Train RMSE: 2.7344\n",
      "Train Epoch: 134 [22008/22024] \t Train Loss(MSE): 2.0313 \t Train RMSE: 1.4253\n",
      "[Epoch: 134 \t Valid MSE: 20.5558 \t Valid RMSE: 3.9487]\n",
      "Train Epoch: 135 [8/22024] \t Train Loss(MSE): 11.9626 \t Train RMSE: 3.4587\n",
      "Train Epoch: 135 [2008/22024] \t Train Loss(MSE): 12.5806 \t Train RMSE: 3.5469\n",
      "Train Epoch: 135 [4008/22024] \t Train Loss(MSE): 0.3642 \t Train RMSE: 0.6035\n",
      "Train Epoch: 135 [6008/22024] \t Train Loss(MSE): 3.8841 \t Train RMSE: 1.9708\n",
      "Train Epoch: 135 [8008/22024] \t Train Loss(MSE): 2.6825 \t Train RMSE: 1.6378\n",
      "Train Epoch: 135 [10008/22024] \t Train Loss(MSE): 4.3505 \t Train RMSE: 2.0858\n",
      "Train Epoch: 135 [12008/22024] \t Train Loss(MSE): 2.2409 \t Train RMSE: 1.4970\n",
      "Train Epoch: 135 [14008/22024] \t Train Loss(MSE): 3.7722 \t Train RMSE: 1.9422\n",
      "Train Epoch: 135 [16008/22024] \t Train Loss(MSE): 46.3843 \t Train RMSE: 6.8106\n",
      "Train Epoch: 135 [18008/22024] \t Train Loss(MSE): 14.2555 \t Train RMSE: 3.7756\n",
      "Train Epoch: 135 [20008/22024] \t Train Loss(MSE): 1.3013 \t Train RMSE: 1.1408\n",
      "Train Epoch: 135 [22008/22024] \t Train Loss(MSE): 4.9041 \t Train RMSE: 2.2145\n",
      "[Epoch: 135 \t Valid MSE: 22.4819 \t Valid RMSE: 4.2014]\n",
      "Train Epoch: 136 [8/22024] \t Train Loss(MSE): 0.9110 \t Train RMSE: 0.9545\n",
      "Train Epoch: 136 [2008/22024] \t Train Loss(MSE): 1.5092 \t Train RMSE: 1.2285\n",
      "Train Epoch: 136 [4008/22024] \t Train Loss(MSE): 33.6745 \t Train RMSE: 5.8030\n",
      "Train Epoch: 136 [6008/22024] \t Train Loss(MSE): 9.1621 \t Train RMSE: 3.0269\n",
      "Train Epoch: 136 [8008/22024] \t Train Loss(MSE): 8.6468 \t Train RMSE: 2.9405\n",
      "Train Epoch: 136 [10008/22024] \t Train Loss(MSE): 13.9796 \t Train RMSE: 3.7389\n",
      "Train Epoch: 136 [12008/22024] \t Train Loss(MSE): 4.7030 \t Train RMSE: 2.1686\n",
      "Train Epoch: 136 [14008/22024] \t Train Loss(MSE): 3.5288 \t Train RMSE: 1.8785\n",
      "Train Epoch: 136 [16008/22024] \t Train Loss(MSE): 22.9612 \t Train RMSE: 4.7918\n",
      "Train Epoch: 136 [18008/22024] \t Train Loss(MSE): 20.8951 \t Train RMSE: 4.5711\n",
      "Train Epoch: 136 [20008/22024] \t Train Loss(MSE): 12.3975 \t Train RMSE: 3.5210\n",
      "Train Epoch: 136 [22008/22024] \t Train Loss(MSE): 3.9608 \t Train RMSE: 1.9902\n",
      "[Epoch: 136 \t Valid MSE: 19.5040 \t Valid RMSE: 3.8336]\n",
      "Train Epoch: 137 [8/22024] \t Train Loss(MSE): 5.5275 \t Train RMSE: 2.3511\n",
      "Train Epoch: 137 [2008/22024] \t Train Loss(MSE): 118.5702 \t Train RMSE: 10.8890\n",
      "Train Epoch: 137 [4008/22024] \t Train Loss(MSE): 18.7394 \t Train RMSE: 4.3289\n",
      "Train Epoch: 137 [6008/22024] \t Train Loss(MSE): 11.0461 \t Train RMSE: 3.3236\n",
      "Train Epoch: 137 [8008/22024] \t Train Loss(MSE): 9.2768 \t Train RMSE: 3.0458\n",
      "Train Epoch: 137 [10008/22024] \t Train Loss(MSE): 4.6581 \t Train RMSE: 2.1583\n",
      "Train Epoch: 137 [12008/22024] \t Train Loss(MSE): 9.1064 \t Train RMSE: 3.0177\n",
      "Train Epoch: 137 [14008/22024] \t Train Loss(MSE): 18.8210 \t Train RMSE: 4.3383\n",
      "Train Epoch: 137 [16008/22024] \t Train Loss(MSE): 1.7952 \t Train RMSE: 1.3399\n",
      "Train Epoch: 137 [18008/22024] \t Train Loss(MSE): 0.8983 \t Train RMSE: 0.9478\n",
      "Train Epoch: 137 [20008/22024] \t Train Loss(MSE): 3.5703 \t Train RMSE: 1.8895\n",
      "Train Epoch: 137 [22008/22024] \t Train Loss(MSE): 5.4539 \t Train RMSE: 2.3354\n",
      "[Epoch: 137 \t Valid MSE: 19.7714 \t Valid RMSE: 3.7988]\n",
      "Train Epoch: 138 [8/22024] \t Train Loss(MSE): 3.3016 \t Train RMSE: 1.8170\n",
      "Train Epoch: 138 [2008/22024] \t Train Loss(MSE): 4.0223 \t Train RMSE: 2.0056\n",
      "Train Epoch: 138 [4008/22024] \t Train Loss(MSE): 34.8496 \t Train RMSE: 5.9034\n",
      "Train Epoch: 138 [6008/22024] \t Train Loss(MSE): 24.0002 \t Train RMSE: 4.8990\n",
      "Train Epoch: 138 [8008/22024] \t Train Loss(MSE): 2.4867 \t Train RMSE: 1.5769\n",
      "Train Epoch: 138 [10008/22024] \t Train Loss(MSE): 8.0153 \t Train RMSE: 2.8311\n",
      "Train Epoch: 138 [12008/22024] \t Train Loss(MSE): 6.2235 \t Train RMSE: 2.4947\n",
      "Train Epoch: 138 [14008/22024] \t Train Loss(MSE): 0.6953 \t Train RMSE: 0.8339\n",
      "Train Epoch: 138 [16008/22024] \t Train Loss(MSE): 79.8390 \t Train RMSE: 8.9353\n",
      "Train Epoch: 138 [18008/22024] \t Train Loss(MSE): 1.4741 \t Train RMSE: 1.2141\n",
      "Train Epoch: 138 [20008/22024] \t Train Loss(MSE): 5.4573 \t Train RMSE: 2.3361\n",
      "Train Epoch: 138 [22008/22024] \t Train Loss(MSE): 18.2476 \t Train RMSE: 4.2717\n",
      "[Epoch: 138 \t Valid MSE: 20.0836 \t Valid RMSE: 3.8774]\n",
      "Train Epoch: 139 [8/22024] \t Train Loss(MSE): 1.2565 \t Train RMSE: 1.1209\n",
      "Train Epoch: 139 [2008/22024] \t Train Loss(MSE): 12.3971 \t Train RMSE: 3.5209\n",
      "Train Epoch: 139 [4008/22024] \t Train Loss(MSE): 31.1773 \t Train RMSE: 5.5837\n",
      "Train Epoch: 139 [6008/22024] \t Train Loss(MSE): 10.6712 \t Train RMSE: 3.2667\n",
      "Train Epoch: 139 [8008/22024] \t Train Loss(MSE): 9.4967 \t Train RMSE: 3.0817\n",
      "Train Epoch: 139 [10008/22024] \t Train Loss(MSE): 75.3876 \t Train RMSE: 8.6826\n",
      "Train Epoch: 139 [12008/22024] \t Train Loss(MSE): 9.7818 \t Train RMSE: 3.1276\n",
      "Train Epoch: 139 [14008/22024] \t Train Loss(MSE): 30.6307 \t Train RMSE: 5.5345\n",
      "Train Epoch: 139 [16008/22024] \t Train Loss(MSE): 7.6938 \t Train RMSE: 2.7738\n",
      "Train Epoch: 139 [18008/22024] \t Train Loss(MSE): 6.3680 \t Train RMSE: 2.5235\n",
      "Train Epoch: 139 [20008/22024] \t Train Loss(MSE): 2.1677 \t Train RMSE: 1.4723\n",
      "Train Epoch: 139 [22008/22024] \t Train Loss(MSE): 3.4728 \t Train RMSE: 1.8635\n",
      "[Epoch: 139 \t Valid MSE: 18.4646 \t Valid RMSE: 3.6491]\n",
      "Train Epoch: 140 [8/22024] \t Train Loss(MSE): 17.7349 \t Train RMSE: 4.2113\n",
      "Train Epoch: 140 [2008/22024] \t Train Loss(MSE): 5.9266 \t Train RMSE: 2.4345\n",
      "Train Epoch: 140 [4008/22024] \t Train Loss(MSE): 7.9942 \t Train RMSE: 2.8274\n",
      "Train Epoch: 140 [6008/22024] \t Train Loss(MSE): 185.2876 \t Train RMSE: 13.6120\n",
      "Train Epoch: 140 [8008/22024] \t Train Loss(MSE): 16.6788 \t Train RMSE: 4.0840\n",
      "Train Epoch: 140 [10008/22024] \t Train Loss(MSE): 9.4646 \t Train RMSE: 3.0765\n",
      "Train Epoch: 140 [12008/22024] \t Train Loss(MSE): 1.1774 \t Train RMSE: 1.0851\n",
      "Train Epoch: 140 [14008/22024] \t Train Loss(MSE): 1.4321 \t Train RMSE: 1.1967\n",
      "Train Epoch: 140 [16008/22024] \t Train Loss(MSE): 1.7888 \t Train RMSE: 1.3375\n",
      "Train Epoch: 140 [18008/22024] \t Train Loss(MSE): 1.9760 \t Train RMSE: 1.4057\n",
      "Train Epoch: 140 [20008/22024] \t Train Loss(MSE): 4.5691 \t Train RMSE: 2.1376\n",
      "Train Epoch: 140 [22008/22024] \t Train Loss(MSE): 6.9865 \t Train RMSE: 2.6432\n",
      "[Epoch: 140 \t Valid MSE: 20.7245 \t Valid RMSE: 4.0023]\n",
      "Train Epoch: 141 [8/22024] \t Train Loss(MSE): 7.9027 \t Train RMSE: 2.8112\n",
      "Train Epoch: 141 [2008/22024] \t Train Loss(MSE): 6.1588 \t Train RMSE: 2.4817\n",
      "Train Epoch: 141 [4008/22024] \t Train Loss(MSE): 7.4995 \t Train RMSE: 2.7385\n",
      "Train Epoch: 141 [6008/22024] \t Train Loss(MSE): 9.7722 \t Train RMSE: 3.1260\n",
      "Train Epoch: 141 [8008/22024] \t Train Loss(MSE): 8.7930 \t Train RMSE: 2.9653\n",
      "Train Epoch: 141 [10008/22024] \t Train Loss(MSE): 8.1250 \t Train RMSE: 2.8504\n",
      "Train Epoch: 141 [12008/22024] \t Train Loss(MSE): 10.6061 \t Train RMSE: 3.2567\n",
      "Train Epoch: 141 [14008/22024] \t Train Loss(MSE): 2.5129 \t Train RMSE: 1.5852\n",
      "Train Epoch: 141 [16008/22024] \t Train Loss(MSE): 16.6853 \t Train RMSE: 4.0848\n",
      "Train Epoch: 141 [18008/22024] \t Train Loss(MSE): 2.2074 \t Train RMSE: 1.4857\n",
      "Train Epoch: 141 [20008/22024] \t Train Loss(MSE): 3.1680 \t Train RMSE: 1.7799\n",
      "Train Epoch: 141 [22008/22024] \t Train Loss(MSE): 3.6916 \t Train RMSE: 1.9214\n",
      "[Epoch: 141 \t Valid MSE: 24.3726 \t Valid RMSE: 4.4317]\n",
      "Train Epoch: 142 [8/22024] \t Train Loss(MSE): 17.8687 \t Train RMSE: 4.2271\n",
      "Train Epoch: 142 [2008/22024] \t Train Loss(MSE): 2.9213 \t Train RMSE: 1.7092\n",
      "Train Epoch: 142 [4008/22024] \t Train Loss(MSE): 7.2569 \t Train RMSE: 2.6939\n",
      "Train Epoch: 142 [6008/22024] \t Train Loss(MSE): 14.3513 \t Train RMSE: 3.7883\n",
      "Train Epoch: 142 [8008/22024] \t Train Loss(MSE): 8.1267 \t Train RMSE: 2.8507\n",
      "Train Epoch: 142 [10008/22024] \t Train Loss(MSE): 1.0029 \t Train RMSE: 1.0014\n",
      "Train Epoch: 142 [12008/22024] \t Train Loss(MSE): 3.9088 \t Train RMSE: 1.9771\n",
      "Train Epoch: 142 [14008/22024] \t Train Loss(MSE): 6.2214 \t Train RMSE: 2.4943\n",
      "Train Epoch: 142 [16008/22024] \t Train Loss(MSE): 2.5200 \t Train RMSE: 1.5875\n",
      "Train Epoch: 142 [18008/22024] \t Train Loss(MSE): 5.7636 \t Train RMSE: 2.4007\n",
      "Train Epoch: 142 [20008/22024] \t Train Loss(MSE): 9.0513 \t Train RMSE: 3.0085\n",
      "Train Epoch: 142 [22008/22024] \t Train Loss(MSE): 11.5801 \t Train RMSE: 3.4030\n",
      "[Epoch: 142 \t Valid MSE: 21.6986 \t Valid RMSE: 4.1281]\n",
      "Train Epoch: 143 [8/22024] \t Train Loss(MSE): 4.5576 \t Train RMSE: 2.1349\n",
      "Train Epoch: 143 [2008/22024] \t Train Loss(MSE): 8.6567 \t Train RMSE: 2.9422\n",
      "Train Epoch: 143 [4008/22024] \t Train Loss(MSE): 4.4746 \t Train RMSE: 2.1153\n",
      "Train Epoch: 143 [6008/22024] \t Train Loss(MSE): 1.0005 \t Train RMSE: 1.0003\n",
      "Train Epoch: 143 [8008/22024] \t Train Loss(MSE): 3.4100 \t Train RMSE: 1.8466\n",
      "Train Epoch: 143 [10008/22024] \t Train Loss(MSE): 20.7907 \t Train RMSE: 4.5597\n",
      "Train Epoch: 143 [12008/22024] \t Train Loss(MSE): 1.3533 \t Train RMSE: 1.1633\n",
      "Train Epoch: 143 [14008/22024] \t Train Loss(MSE): 15.6198 \t Train RMSE: 3.9522\n",
      "Train Epoch: 143 [16008/22024] \t Train Loss(MSE): 6.0612 \t Train RMSE: 2.4619\n",
      "Train Epoch: 143 [18008/22024] \t Train Loss(MSE): 6.8771 \t Train RMSE: 2.6224\n",
      "Train Epoch: 143 [20008/22024] \t Train Loss(MSE): 8.6937 \t Train RMSE: 2.9485\n",
      "Train Epoch: 143 [22008/22024] \t Train Loss(MSE): 2.4067 \t Train RMSE: 1.5513\n",
      "[Epoch: 143 \t Valid MSE: 22.2534 \t Valid RMSE: 4.1108]\n",
      "Train Epoch: 144 [8/22024] \t Train Loss(MSE): 6.4847 \t Train RMSE: 2.5465\n",
      "Train Epoch: 144 [2008/22024] \t Train Loss(MSE): 17.2638 \t Train RMSE: 4.1550\n",
      "Train Epoch: 144 [4008/22024] \t Train Loss(MSE): 19.9086 \t Train RMSE: 4.4619\n",
      "Train Epoch: 144 [6008/22024] \t Train Loss(MSE): 11.0644 \t Train RMSE: 3.3263\n",
      "Train Epoch: 144 [8008/22024] \t Train Loss(MSE): 6.2902 \t Train RMSE: 2.5080\n",
      "Train Epoch: 144 [10008/22024] \t Train Loss(MSE): 9.0156 \t Train RMSE: 3.0026\n",
      "Train Epoch: 144 [12008/22024] \t Train Loss(MSE): 10.9183 \t Train RMSE: 3.3043\n",
      "Train Epoch: 144 [14008/22024] \t Train Loss(MSE): 12.2256 \t Train RMSE: 3.4965\n",
      "Train Epoch: 144 [16008/22024] \t Train Loss(MSE): 5.9773 \t Train RMSE: 2.4449\n",
      "Train Epoch: 144 [18008/22024] \t Train Loss(MSE): 6.4559 \t Train RMSE: 2.5408\n",
      "Train Epoch: 144 [20008/22024] \t Train Loss(MSE): 4.3308 \t Train RMSE: 2.0811\n",
      "Train Epoch: 144 [22008/22024] \t Train Loss(MSE): 50.1248 \t Train RMSE: 7.0799\n",
      "[Epoch: 144 \t Valid MSE: 20.5771 \t Valid RMSE: 3.8825]\n",
      "Train Epoch: 145 [8/22024] \t Train Loss(MSE): 7.9400 \t Train RMSE: 2.8178\n",
      "Train Epoch: 145 [2008/22024] \t Train Loss(MSE): 3.5461 \t Train RMSE: 1.8831\n",
      "Train Epoch: 145 [4008/22024] \t Train Loss(MSE): 31.4472 \t Train RMSE: 5.6078\n",
      "Train Epoch: 145 [6008/22024] \t Train Loss(MSE): 1.4866 \t Train RMSE: 1.2193\n",
      "Train Epoch: 145 [8008/22024] \t Train Loss(MSE): 6.1844 \t Train RMSE: 2.4869\n",
      "Train Epoch: 145 [10008/22024] \t Train Loss(MSE): 4.1468 \t Train RMSE: 2.0364\n",
      "Train Epoch: 145 [12008/22024] \t Train Loss(MSE): 0.7469 \t Train RMSE: 0.8642\n",
      "Train Epoch: 145 [14008/22024] \t Train Loss(MSE): 10.5628 \t Train RMSE: 3.2500\n",
      "Train Epoch: 145 [16008/22024] \t Train Loss(MSE): 16.5658 \t Train RMSE: 4.0701\n",
      "Train Epoch: 145 [18008/22024] \t Train Loss(MSE): 2.5191 \t Train RMSE: 1.5872\n",
      "Train Epoch: 145 [20008/22024] \t Train Loss(MSE): 14.3428 \t Train RMSE: 3.7872\n",
      "Train Epoch: 145 [22008/22024] \t Train Loss(MSE): 16.8511 \t Train RMSE: 4.1050\n",
      "[Epoch: 145 \t Valid MSE: 19.0302 \t Valid RMSE: 3.7268]\n",
      "Train Epoch: 146 [8/22024] \t Train Loss(MSE): 102.2608 \t Train RMSE: 10.1124\n",
      "Train Epoch: 146 [2008/22024] \t Train Loss(MSE): 2.9562 \t Train RMSE: 1.7194\n",
      "Train Epoch: 146 [4008/22024] \t Train Loss(MSE): 4.7484 \t Train RMSE: 2.1791\n",
      "Train Epoch: 146 [6008/22024] \t Train Loss(MSE): 2.6390 \t Train RMSE: 1.6245\n",
      "Train Epoch: 146 [8008/22024] \t Train Loss(MSE): 6.3542 \t Train RMSE: 2.5208\n",
      "Train Epoch: 146 [10008/22024] \t Train Loss(MSE): 10.3323 \t Train RMSE: 3.2144\n",
      "Train Epoch: 146 [12008/22024] \t Train Loss(MSE): 6.2172 \t Train RMSE: 2.4934\n",
      "Train Epoch: 146 [14008/22024] \t Train Loss(MSE): 3.0898 \t Train RMSE: 1.7578\n",
      "Train Epoch: 146 [16008/22024] \t Train Loss(MSE): 3.2640 \t Train RMSE: 1.8066\n",
      "Train Epoch: 146 [18008/22024] \t Train Loss(MSE): 4.5237 \t Train RMSE: 2.1269\n",
      "Train Epoch: 146 [20008/22024] \t Train Loss(MSE): 5.0833 \t Train RMSE: 2.2546\n",
      "Train Epoch: 146 [22008/22024] \t Train Loss(MSE): 6.3477 \t Train RMSE: 2.5195\n",
      "[Epoch: 146 \t Valid MSE: 22.3622 \t Valid RMSE: 4.1840]\n",
      "Train Epoch: 147 [8/22024] \t Train Loss(MSE): 2.1276 \t Train RMSE: 1.4586\n",
      "Train Epoch: 147 [2008/22024] \t Train Loss(MSE): 5.4421 \t Train RMSE: 2.3328\n",
      "Train Epoch: 147 [4008/22024] \t Train Loss(MSE): 3.0690 \t Train RMSE: 1.7518\n",
      "Train Epoch: 147 [6008/22024] \t Train Loss(MSE): 40.4888 \t Train RMSE: 6.3631\n",
      "Train Epoch: 147 [8008/22024] \t Train Loss(MSE): 1.5701 \t Train RMSE: 1.2530\n",
      "Train Epoch: 147 [10008/22024] \t Train Loss(MSE): 1.8417 \t Train RMSE: 1.3571\n",
      "Train Epoch: 147 [12008/22024] \t Train Loss(MSE): 1.0594 \t Train RMSE: 1.0293\n",
      "Train Epoch: 147 [14008/22024] \t Train Loss(MSE): 3.1963 \t Train RMSE: 1.7878\n",
      "Train Epoch: 147 [16008/22024] \t Train Loss(MSE): 45.0772 \t Train RMSE: 6.7140\n",
      "Train Epoch: 147 [18008/22024] \t Train Loss(MSE): 12.5275 \t Train RMSE: 3.5394\n",
      "Train Epoch: 147 [20008/22024] \t Train Loss(MSE): 3.1227 \t Train RMSE: 1.7671\n",
      "Train Epoch: 147 [22008/22024] \t Train Loss(MSE): 2.4617 \t Train RMSE: 1.5690\n",
      "[Epoch: 147 \t Valid MSE: 19.3307 \t Valid RMSE: 3.7217]\n",
      "Train Epoch: 148 [8/22024] \t Train Loss(MSE): 0.3480 \t Train RMSE: 0.5899\n",
      "Train Epoch: 148 [2008/22024] \t Train Loss(MSE): 2.6745 \t Train RMSE: 1.6354\n",
      "Train Epoch: 148 [4008/22024] \t Train Loss(MSE): 3.4730 \t Train RMSE: 1.8636\n",
      "Train Epoch: 148 [6008/22024] \t Train Loss(MSE): 4.1469 \t Train RMSE: 2.0364\n",
      "Train Epoch: 148 [8008/22024] \t Train Loss(MSE): 4.5686 \t Train RMSE: 2.1374\n",
      "Train Epoch: 148 [10008/22024] \t Train Loss(MSE): 2.3271 \t Train RMSE: 1.5255\n",
      "Train Epoch: 148 [12008/22024] \t Train Loss(MSE): 31.5741 \t Train RMSE: 5.6191\n",
      "Train Epoch: 148 [14008/22024] \t Train Loss(MSE): 2.8467 \t Train RMSE: 1.6872\n",
      "Train Epoch: 148 [16008/22024] \t Train Loss(MSE): 7.7080 \t Train RMSE: 2.7763\n",
      "Train Epoch: 148 [18008/22024] \t Train Loss(MSE): 4.2654 \t Train RMSE: 2.0653\n",
      "Train Epoch: 148 [20008/22024] \t Train Loss(MSE): 15.5478 \t Train RMSE: 3.9431\n",
      "Train Epoch: 148 [22008/22024] \t Train Loss(MSE): 6.5047 \t Train RMSE: 2.5504\n",
      "[Epoch: 148 \t Valid MSE: 20.9777 \t Valid RMSE: 3.9673]\n",
      "Train Epoch: 149 [8/22024] \t Train Loss(MSE): 16.6451 \t Train RMSE: 4.0798\n",
      "Train Epoch: 149 [2008/22024] \t Train Loss(MSE): 14.7388 \t Train RMSE: 3.8391\n",
      "Train Epoch: 149 [4008/22024] \t Train Loss(MSE): 23.4516 \t Train RMSE: 4.8427\n",
      "Train Epoch: 149 [6008/22024] \t Train Loss(MSE): 24.9910 \t Train RMSE: 4.9991\n",
      "Train Epoch: 149 [8008/22024] \t Train Loss(MSE): 59.5019 \t Train RMSE: 7.7137\n",
      "Train Epoch: 149 [10008/22024] \t Train Loss(MSE): 7.0298 \t Train RMSE: 2.6514\n",
      "Train Epoch: 149 [12008/22024] \t Train Loss(MSE): 12.6871 \t Train RMSE: 3.5619\n",
      "Train Epoch: 149 [14008/22024] \t Train Loss(MSE): 19.2676 \t Train RMSE: 4.3895\n",
      "Train Epoch: 149 [16008/22024] \t Train Loss(MSE): 4.0798 \t Train RMSE: 2.0199\n",
      "Train Epoch: 149 [18008/22024] \t Train Loss(MSE): 5.0256 \t Train RMSE: 2.2418\n",
      "Train Epoch: 149 [20008/22024] \t Train Loss(MSE): 22.8485 \t Train RMSE: 4.7800\n",
      "Train Epoch: 149 [22008/22024] \t Train Loss(MSE): 1.4690 \t Train RMSE: 1.2120\n",
      "[Epoch: 149 \t Valid MSE: 20.5305 \t Valid RMSE: 3.9025]\n",
      "Train Epoch: 150 [8/22024] \t Train Loss(MSE): 4.3940 \t Train RMSE: 2.0962\n",
      "Train Epoch: 150 [2008/22024] \t Train Loss(MSE): 3.6404 \t Train RMSE: 1.9080\n",
      "Train Epoch: 150 [4008/22024] \t Train Loss(MSE): 27.4926 \t Train RMSE: 5.2433\n",
      "Train Epoch: 150 [6008/22024] \t Train Loss(MSE): 7.3349 \t Train RMSE: 2.7083\n",
      "Train Epoch: 150 [8008/22024] \t Train Loss(MSE): 2.3333 \t Train RMSE: 1.5275\n",
      "Train Epoch: 150 [10008/22024] \t Train Loss(MSE): 7.9854 \t Train RMSE: 2.8258\n",
      "Train Epoch: 150 [12008/22024] \t Train Loss(MSE): 12.1696 \t Train RMSE: 3.4885\n",
      "Train Epoch: 150 [14008/22024] \t Train Loss(MSE): 10.3635 \t Train RMSE: 3.2192\n",
      "Train Epoch: 150 [16008/22024] \t Train Loss(MSE): 11.7755 \t Train RMSE: 3.4316\n",
      "Train Epoch: 150 [18008/22024] \t Train Loss(MSE): 22.3066 \t Train RMSE: 4.7230\n",
      "Train Epoch: 150 [20008/22024] \t Train Loss(MSE): 127.1624 \t Train RMSE: 11.2766\n",
      "Train Epoch: 150 [22008/22024] \t Train Loss(MSE): 3.7589 \t Train RMSE: 1.9388\n",
      "[Epoch: 150 \t Valid MSE: 22.1712 \t Valid RMSE: 4.1728]\n",
      "Train Epoch: 151 [8/22024] \t Train Loss(MSE): 11.7872 \t Train RMSE: 3.4333\n",
      "Train Epoch: 151 [2008/22024] \t Train Loss(MSE): 10.4535 \t Train RMSE: 3.2332\n",
      "Train Epoch: 151 [4008/22024] \t Train Loss(MSE): 2.0240 \t Train RMSE: 1.4227\n",
      "Train Epoch: 151 [6008/22024] \t Train Loss(MSE): 10.6541 \t Train RMSE: 3.2641\n",
      "Train Epoch: 151 [8008/22024] \t Train Loss(MSE): 86.1288 \t Train RMSE: 9.2806\n",
      "Train Epoch: 151 [10008/22024] \t Train Loss(MSE): 9.3200 \t Train RMSE: 3.0529\n",
      "Train Epoch: 151 [12008/22024] \t Train Loss(MSE): 4.1231 \t Train RMSE: 2.0305\n",
      "Train Epoch: 151 [14008/22024] \t Train Loss(MSE): 1.0386 \t Train RMSE: 1.0191\n",
      "Train Epoch: 151 [16008/22024] \t Train Loss(MSE): 12.4910 \t Train RMSE: 3.5343\n",
      "Train Epoch: 151 [18008/22024] \t Train Loss(MSE): 8.8282 \t Train RMSE: 2.9712\n",
      "Train Epoch: 151 [20008/22024] \t Train Loss(MSE): 9.0083 \t Train RMSE: 3.0014\n",
      "Train Epoch: 151 [22008/22024] \t Train Loss(MSE): 11.5741 \t Train RMSE: 3.4021\n",
      "[Epoch: 151 \t Valid MSE: 21.2436 \t Valid RMSE: 4.0082]\n",
      "Train Epoch: 152 [8/22024] \t Train Loss(MSE): 7.6903 \t Train RMSE: 2.7731\n",
      "Train Epoch: 152 [2008/22024] \t Train Loss(MSE): 0.5009 \t Train RMSE: 0.7077\n",
      "Train Epoch: 152 [4008/22024] \t Train Loss(MSE): 14.1531 \t Train RMSE: 3.7621\n",
      "Train Epoch: 152 [6008/22024] \t Train Loss(MSE): 8.6202 \t Train RMSE: 2.9360\n",
      "Train Epoch: 152 [8008/22024] \t Train Loss(MSE): 2.5404 \t Train RMSE: 1.5939\n",
      "Train Epoch: 152 [10008/22024] \t Train Loss(MSE): 3.2251 \t Train RMSE: 1.7958\n",
      "Train Epoch: 152 [12008/22024] \t Train Loss(MSE): 1.3557 \t Train RMSE: 1.1644\n",
      "Train Epoch: 152 [14008/22024] \t Train Loss(MSE): 3.4776 \t Train RMSE: 1.8648\n",
      "Train Epoch: 152 [16008/22024] \t Train Loss(MSE): 12.3737 \t Train RMSE: 3.5176\n",
      "Train Epoch: 152 [18008/22024] \t Train Loss(MSE): 2.1550 \t Train RMSE: 1.4680\n",
      "Train Epoch: 152 [20008/22024] \t Train Loss(MSE): 11.6957 \t Train RMSE: 3.4199\n",
      "Train Epoch: 152 [22008/22024] \t Train Loss(MSE): 5.5663 \t Train RMSE: 2.3593\n",
      "[Epoch: 152 \t Valid MSE: 21.8723 \t Valid RMSE: 4.1792]\n",
      "Train Epoch: 153 [8/22024] \t Train Loss(MSE): 0.8248 \t Train RMSE: 0.9082\n",
      "Train Epoch: 153 [2008/22024] \t Train Loss(MSE): 9.9813 \t Train RMSE: 3.1593\n",
      "Train Epoch: 153 [4008/22024] \t Train Loss(MSE): 5.1681 \t Train RMSE: 2.2733\n",
      "Train Epoch: 153 [6008/22024] \t Train Loss(MSE): 5.7400 \t Train RMSE: 2.3958\n",
      "Train Epoch: 153 [8008/22024] \t Train Loss(MSE): 7.2343 \t Train RMSE: 2.6897\n",
      "Train Epoch: 153 [10008/22024] \t Train Loss(MSE): 11.6426 \t Train RMSE: 3.4121\n",
      "Train Epoch: 153 [12008/22024] \t Train Loss(MSE): 1.1618 \t Train RMSE: 1.0778\n",
      "Train Epoch: 153 [14008/22024] \t Train Loss(MSE): 8.3222 \t Train RMSE: 2.8848\n",
      "Train Epoch: 153 [16008/22024] \t Train Loss(MSE): 1.3632 \t Train RMSE: 1.1676\n",
      "Train Epoch: 153 [18008/22024] \t Train Loss(MSE): 68.9325 \t Train RMSE: 8.3026\n",
      "Train Epoch: 153 [20008/22024] \t Train Loss(MSE): 2.2318 \t Train RMSE: 1.4939\n",
      "Train Epoch: 153 [22008/22024] \t Train Loss(MSE): 27.5636 \t Train RMSE: 5.2501\n",
      "[Epoch: 153 \t Valid MSE: 19.4675 \t Valid RMSE: 3.8195]\n",
      "Train Epoch: 154 [8/22024] \t Train Loss(MSE): 4.1872 \t Train RMSE: 2.0463\n",
      "Train Epoch: 154 [2008/22024] \t Train Loss(MSE): 1.9331 \t Train RMSE: 1.3904\n",
      "Train Epoch: 154 [4008/22024] \t Train Loss(MSE): 4.2366 \t Train RMSE: 2.0583\n",
      "Train Epoch: 154 [6008/22024] \t Train Loss(MSE): 22.5529 \t Train RMSE: 4.7490\n",
      "Train Epoch: 154 [8008/22024] \t Train Loss(MSE): 3.4960 \t Train RMSE: 1.8698\n",
      "Train Epoch: 154 [10008/22024] \t Train Loss(MSE): 19.3344 \t Train RMSE: 4.3971\n",
      "Train Epoch: 154 [12008/22024] \t Train Loss(MSE): 30.6004 \t Train RMSE: 5.5318\n",
      "Train Epoch: 154 [14008/22024] \t Train Loss(MSE): 1.3485 \t Train RMSE: 1.1613\n",
      "Train Epoch: 154 [16008/22024] \t Train Loss(MSE): 5.5012 \t Train RMSE: 2.3455\n",
      "Train Epoch: 154 [18008/22024] \t Train Loss(MSE): 10.8760 \t Train RMSE: 3.2979\n",
      "Train Epoch: 154 [20008/22024] \t Train Loss(MSE): 19.9823 \t Train RMSE: 4.4702\n",
      "Train Epoch: 154 [22008/22024] \t Train Loss(MSE): 14.6171 \t Train RMSE: 3.8232\n",
      "[Epoch: 154 \t Valid MSE: 22.8010 \t Valid RMSE: 4.2296]\n",
      "Train Epoch: 155 [8/22024] \t Train Loss(MSE): 55.2749 \t Train RMSE: 7.4347\n",
      "Train Epoch: 155 [2008/22024] \t Train Loss(MSE): 1.4737 \t Train RMSE: 1.2140\n",
      "Train Epoch: 155 [4008/22024] \t Train Loss(MSE): 9.3393 \t Train RMSE: 3.0560\n",
      "Train Epoch: 155 [6008/22024] \t Train Loss(MSE): 3.2547 \t Train RMSE: 1.8041\n",
      "Train Epoch: 155 [8008/22024] \t Train Loss(MSE): 13.1409 \t Train RMSE: 3.6250\n",
      "Train Epoch: 155 [10008/22024] \t Train Loss(MSE): 7.0241 \t Train RMSE: 2.6503\n",
      "Train Epoch: 155 [12008/22024] \t Train Loss(MSE): 12.5444 \t Train RMSE: 3.5418\n",
      "Train Epoch: 155 [14008/22024] \t Train Loss(MSE): 6.9565 \t Train RMSE: 2.6375\n",
      "Train Epoch: 155 [16008/22024] \t Train Loss(MSE): 4.0187 \t Train RMSE: 2.0047\n",
      "Train Epoch: 155 [18008/22024] \t Train Loss(MSE): 5.0739 \t Train RMSE: 2.2525\n",
      "Train Epoch: 155 [20008/22024] \t Train Loss(MSE): 84.3364 \t Train RMSE: 9.1835\n",
      "Train Epoch: 155 [22008/22024] \t Train Loss(MSE): 5.8426 \t Train RMSE: 2.4171\n",
      "[Epoch: 155 \t Valid MSE: 27.6771 \t Valid RMSE: 4.7445]\n",
      "Train Epoch: 156 [8/22024] \t Train Loss(MSE): 5.2625 \t Train RMSE: 2.2940\n",
      "Train Epoch: 156 [2008/22024] \t Train Loss(MSE): 0.7413 \t Train RMSE: 0.8610\n",
      "Train Epoch: 156 [4008/22024] \t Train Loss(MSE): 2.4121 \t Train RMSE: 1.5531\n",
      "Train Epoch: 156 [6008/22024] \t Train Loss(MSE): 1.6452 \t Train RMSE: 1.2826\n",
      "Train Epoch: 156 [8008/22024] \t Train Loss(MSE): 13.7273 \t Train RMSE: 3.7050\n",
      "Train Epoch: 156 [10008/22024] \t Train Loss(MSE): 1.9447 \t Train RMSE: 1.3945\n",
      "Train Epoch: 156 [12008/22024] \t Train Loss(MSE): 18.5864 \t Train RMSE: 4.3112\n",
      "Train Epoch: 156 [14008/22024] \t Train Loss(MSE): 4.9058 \t Train RMSE: 2.2149\n",
      "Train Epoch: 156 [16008/22024] \t Train Loss(MSE): 8.4833 \t Train RMSE: 2.9126\n",
      "Train Epoch: 156 [18008/22024] \t Train Loss(MSE): 13.4661 \t Train RMSE: 3.6696\n",
      "Train Epoch: 156 [20008/22024] \t Train Loss(MSE): 39.7215 \t Train RMSE: 6.3025\n",
      "Train Epoch: 156 [22008/22024] \t Train Loss(MSE): 1.7162 \t Train RMSE: 1.3100\n",
      "[Epoch: 156 \t Valid MSE: 21.3773 \t Valid RMSE: 3.9935]\n",
      "Train Epoch: 157 [8/22024] \t Train Loss(MSE): 1.8464 \t Train RMSE: 1.3588\n",
      "Train Epoch: 157 [2008/22024] \t Train Loss(MSE): 3.5984 \t Train RMSE: 1.8969\n",
      "Train Epoch: 157 [4008/22024] \t Train Loss(MSE): 12.0852 \t Train RMSE: 3.4764\n",
      "Train Epoch: 157 [6008/22024] \t Train Loss(MSE): 17.1030 \t Train RMSE: 4.1356\n",
      "Train Epoch: 157 [8008/22024] \t Train Loss(MSE): 17.0135 \t Train RMSE: 4.1247\n",
      "Train Epoch: 157 [10008/22024] \t Train Loss(MSE): 9.9316 \t Train RMSE: 3.1514\n",
      "Train Epoch: 157 [12008/22024] \t Train Loss(MSE): 5.2088 \t Train RMSE: 2.2823\n",
      "Train Epoch: 157 [14008/22024] \t Train Loss(MSE): 11.5996 \t Train RMSE: 3.4058\n",
      "Train Epoch: 157 [16008/22024] \t Train Loss(MSE): 4.4915 \t Train RMSE: 2.1193\n",
      "Train Epoch: 157 [18008/22024] \t Train Loss(MSE): 0.5640 \t Train RMSE: 0.7510\n",
      "Train Epoch: 157 [20008/22024] \t Train Loss(MSE): 16.3516 \t Train RMSE: 4.0437\n",
      "Train Epoch: 157 [22008/22024] \t Train Loss(MSE): 4.8305 \t Train RMSE: 2.1978\n",
      "[Epoch: 157 \t Valid MSE: 20.6760 \t Valid RMSE: 3.9905]\n",
      "Train Epoch: 158 [8/22024] \t Train Loss(MSE): 2.9182 \t Train RMSE: 1.7083\n",
      "Train Epoch: 158 [2008/22024] \t Train Loss(MSE): 2.6139 \t Train RMSE: 1.6167\n",
      "Train Epoch: 158 [4008/22024] \t Train Loss(MSE): 2.2153 \t Train RMSE: 1.4884\n",
      "Train Epoch: 158 [6008/22024] \t Train Loss(MSE): 2.8043 \t Train RMSE: 1.6746\n",
      "Train Epoch: 158 [8008/22024] \t Train Loss(MSE): 1.9994 \t Train RMSE: 1.4140\n",
      "Train Epoch: 158 [10008/22024] \t Train Loss(MSE): 2.3657 \t Train RMSE: 1.5381\n",
      "Train Epoch: 158 [12008/22024] \t Train Loss(MSE): 1.1835 \t Train RMSE: 1.0879\n",
      "Train Epoch: 158 [14008/22024] \t Train Loss(MSE): 6.4251 \t Train RMSE: 2.5348\n",
      "Train Epoch: 158 [16008/22024] \t Train Loss(MSE): 11.2807 \t Train RMSE: 3.3587\n",
      "Train Epoch: 158 [18008/22024] \t Train Loss(MSE): 10.7759 \t Train RMSE: 3.2827\n",
      "Train Epoch: 158 [20008/22024] \t Train Loss(MSE): 33.0967 \t Train RMSE: 5.7530\n",
      "Train Epoch: 158 [22008/22024] \t Train Loss(MSE): 2.9239 \t Train RMSE: 1.7099\n",
      "[Epoch: 158 \t Valid MSE: 21.9683 \t Valid RMSE: 4.1154]\n",
      "Train Epoch: 159 [8/22024] \t Train Loss(MSE): 10.5036 \t Train RMSE: 3.2409\n",
      "Train Epoch: 159 [2008/22024] \t Train Loss(MSE): 7.2012 \t Train RMSE: 2.6835\n",
      "Train Epoch: 159 [4008/22024] \t Train Loss(MSE): 1.2994 \t Train RMSE: 1.1399\n",
      "Train Epoch: 159 [6008/22024] \t Train Loss(MSE): 5.5400 \t Train RMSE: 2.3537\n",
      "Train Epoch: 159 [8008/22024] \t Train Loss(MSE): 5.0347 \t Train RMSE: 2.2438\n",
      "Train Epoch: 159 [10008/22024] \t Train Loss(MSE): 1.5426 \t Train RMSE: 1.2420\n",
      "Train Epoch: 159 [12008/22024] \t Train Loss(MSE): 84.8848 \t Train RMSE: 9.2133\n",
      "Train Epoch: 159 [14008/22024] \t Train Loss(MSE): 14.2209 \t Train RMSE: 3.7711\n",
      "Train Epoch: 159 [16008/22024] \t Train Loss(MSE): 10.1226 \t Train RMSE: 3.1816\n",
      "Train Epoch: 159 [18008/22024] \t Train Loss(MSE): 1.2567 \t Train RMSE: 1.1210\n",
      "Train Epoch: 159 [20008/22024] \t Train Loss(MSE): 7.0759 \t Train RMSE: 2.6601\n",
      "Train Epoch: 159 [22008/22024] \t Train Loss(MSE): 5.2898 \t Train RMSE: 2.3000\n",
      "[Epoch: 159 \t Valid MSE: 22.0259 \t Valid RMSE: 4.1766]\n",
      "Train Epoch: 160 [8/22024] \t Train Loss(MSE): 0.7975 \t Train RMSE: 0.8930\n",
      "Train Epoch: 160 [2008/22024] \t Train Loss(MSE): 10.4347 \t Train RMSE: 3.2303\n",
      "Train Epoch: 160 [4008/22024] \t Train Loss(MSE): 2.3003 \t Train RMSE: 1.5167\n",
      "Train Epoch: 160 [6008/22024] \t Train Loss(MSE): 6.8391 \t Train RMSE: 2.6152\n",
      "Train Epoch: 160 [8008/22024] \t Train Loss(MSE): 2.7529 \t Train RMSE: 1.6592\n",
      "Train Epoch: 160 [10008/22024] \t Train Loss(MSE): 1.9669 \t Train RMSE: 1.4024\n",
      "Train Epoch: 160 [12008/22024] \t Train Loss(MSE): 86.3460 \t Train RMSE: 9.2923\n",
      "Train Epoch: 160 [14008/22024] \t Train Loss(MSE): 4.5063 \t Train RMSE: 2.1228\n",
      "Train Epoch: 160 [16008/22024] \t Train Loss(MSE): 5.3964 \t Train RMSE: 2.3230\n",
      "Train Epoch: 160 [18008/22024] \t Train Loss(MSE): 1.6258 \t Train RMSE: 1.2751\n",
      "Train Epoch: 160 [20008/22024] \t Train Loss(MSE): 14.5600 \t Train RMSE: 3.8158\n",
      "Train Epoch: 160 [22008/22024] \t Train Loss(MSE): 35.4485 \t Train RMSE: 5.9539\n",
      "[Epoch: 160 \t Valid MSE: 23.9331 \t Valid RMSE: 4.4025]\n",
      "Train Epoch: 161 [8/22024] \t Train Loss(MSE): 4.8644 \t Train RMSE: 2.2055\n",
      "Train Epoch: 161 [2008/22024] \t Train Loss(MSE): 10.8967 \t Train RMSE: 3.3010\n",
      "Train Epoch: 161 [4008/22024] \t Train Loss(MSE): 4.3626 \t Train RMSE: 2.0887\n",
      "Train Epoch: 161 [6008/22024] \t Train Loss(MSE): 56.9537 \t Train RMSE: 7.5468\n",
      "Train Epoch: 161 [8008/22024] \t Train Loss(MSE): 11.6342 \t Train RMSE: 3.4109\n",
      "Train Epoch: 161 [10008/22024] \t Train Loss(MSE): 1.6997 \t Train RMSE: 1.3037\n",
      "Train Epoch: 161 [12008/22024] \t Train Loss(MSE): 18.7741 \t Train RMSE: 4.3329\n",
      "Train Epoch: 161 [14008/22024] \t Train Loss(MSE): 9.8884 \t Train RMSE: 3.1446\n",
      "Train Epoch: 161 [16008/22024] \t Train Loss(MSE): 13.9964 \t Train RMSE: 3.7412\n",
      "Train Epoch: 161 [18008/22024] \t Train Loss(MSE): 15.8293 \t Train RMSE: 3.9786\n",
      "Train Epoch: 161 [20008/22024] \t Train Loss(MSE): 6.3002 \t Train RMSE: 2.5100\n",
      "Train Epoch: 161 [22008/22024] \t Train Loss(MSE): 8.8824 \t Train RMSE: 2.9803\n",
      "[Epoch: 161 \t Valid MSE: 19.4341 \t Valid RMSE: 3.7985]\n",
      "Train Epoch: 162 [8/22024] \t Train Loss(MSE): 20.8727 \t Train RMSE: 4.5687\n",
      "Train Epoch: 162 [2008/22024] \t Train Loss(MSE): 12.0927 \t Train RMSE: 3.4775\n",
      "Train Epoch: 162 [4008/22024] \t Train Loss(MSE): 12.3593 \t Train RMSE: 3.5156\n",
      "Train Epoch: 162 [6008/22024] \t Train Loss(MSE): 12.2787 \t Train RMSE: 3.5041\n",
      "Train Epoch: 162 [8008/22024] \t Train Loss(MSE): 3.0066 \t Train RMSE: 1.7339\n",
      "Train Epoch: 162 [10008/22024] \t Train Loss(MSE): 2.4711 \t Train RMSE: 1.5720\n",
      "Train Epoch: 162 [12008/22024] \t Train Loss(MSE): 4.0541 \t Train RMSE: 2.0135\n",
      "Train Epoch: 162 [14008/22024] \t Train Loss(MSE): 12.6936 \t Train RMSE: 3.5628\n",
      "Train Epoch: 162 [16008/22024] \t Train Loss(MSE): 4.7806 \t Train RMSE: 2.1865\n",
      "Train Epoch: 162 [18008/22024] \t Train Loss(MSE): 5.2796 \t Train RMSE: 2.2977\n",
      "Train Epoch: 162 [20008/22024] \t Train Loss(MSE): 4.5675 \t Train RMSE: 2.1372\n",
      "Train Epoch: 162 [22008/22024] \t Train Loss(MSE): 2.4049 \t Train RMSE: 1.5508\n",
      "[Epoch: 162 \t Valid MSE: 21.1948 \t Valid RMSE: 4.0263]\n",
      "Train Epoch: 163 [8/22024] \t Train Loss(MSE): 9.6044 \t Train RMSE: 3.0991\n",
      "Train Epoch: 163 [2008/22024] \t Train Loss(MSE): 32.5979 \t Train RMSE: 5.7095\n",
      "Train Epoch: 163 [4008/22024] \t Train Loss(MSE): 60.6095 \t Train RMSE: 7.7852\n",
      "Train Epoch: 163 [6008/22024] \t Train Loss(MSE): 11.6086 \t Train RMSE: 3.4071\n",
      "Train Epoch: 163 [8008/22024] \t Train Loss(MSE): 38.2032 \t Train RMSE: 6.1809\n",
      "Train Epoch: 163 [10008/22024] \t Train Loss(MSE): 4.9149 \t Train RMSE: 2.2170\n",
      "Train Epoch: 163 [12008/22024] \t Train Loss(MSE): 17.1287 \t Train RMSE: 4.1387\n",
      "Train Epoch: 163 [14008/22024] \t Train Loss(MSE): 5.6360 \t Train RMSE: 2.3740\n",
      "Train Epoch: 163 [16008/22024] \t Train Loss(MSE): 17.4435 \t Train RMSE: 4.1765\n",
      "Train Epoch: 163 [18008/22024] \t Train Loss(MSE): 2.2934 \t Train RMSE: 1.5144\n",
      "Train Epoch: 163 [20008/22024] \t Train Loss(MSE): 53.4316 \t Train RMSE: 7.3097\n",
      "Train Epoch: 163 [22008/22024] \t Train Loss(MSE): 4.9405 \t Train RMSE: 2.2227\n",
      "[Epoch: 163 \t Valid MSE: 19.8406 \t Valid RMSE: 3.7979]\n",
      "Train Epoch: 164 [8/22024] \t Train Loss(MSE): 22.9043 \t Train RMSE: 4.7858\n",
      "Train Epoch: 164 [2008/22024] \t Train Loss(MSE): 10.0268 \t Train RMSE: 3.1665\n",
      "Train Epoch: 164 [4008/22024] \t Train Loss(MSE): 36.2940 \t Train RMSE: 6.0244\n",
      "Train Epoch: 164 [6008/22024] \t Train Loss(MSE): 9.8167 \t Train RMSE: 3.1332\n",
      "Train Epoch: 164 [8008/22024] \t Train Loss(MSE): 33.9565 \t Train RMSE: 5.8272\n",
      "Train Epoch: 164 [10008/22024] \t Train Loss(MSE): 5.7942 \t Train RMSE: 2.4071\n",
      "Train Epoch: 164 [12008/22024] \t Train Loss(MSE): 5.8178 \t Train RMSE: 2.4120\n",
      "Train Epoch: 164 [14008/22024] \t Train Loss(MSE): 7.3334 \t Train RMSE: 2.7080\n",
      "Train Epoch: 164 [16008/22024] \t Train Loss(MSE): 0.2966 \t Train RMSE: 0.5446\n",
      "Train Epoch: 164 [18008/22024] \t Train Loss(MSE): 28.9515 \t Train RMSE: 5.3807\n",
      "Train Epoch: 164 [20008/22024] \t Train Loss(MSE): 2.0845 \t Train RMSE: 1.4438\n",
      "Train Epoch: 164 [22008/22024] \t Train Loss(MSE): 6.8934 \t Train RMSE: 2.6255\n",
      "[Epoch: 164 \t Valid MSE: 20.4160 \t Valid RMSE: 3.8566]\n",
      "Train Epoch: 165 [8/22024] \t Train Loss(MSE): 7.6018 \t Train RMSE: 2.7571\n",
      "Train Epoch: 165 [2008/22024] \t Train Loss(MSE): 4.8763 \t Train RMSE: 2.2082\n",
      "Train Epoch: 165 [4008/22024] \t Train Loss(MSE): 3.1000 \t Train RMSE: 1.7607\n",
      "Train Epoch: 165 [6008/22024] \t Train Loss(MSE): 4.4545 \t Train RMSE: 2.1106\n",
      "Train Epoch: 165 [8008/22024] \t Train Loss(MSE): 3.5457 \t Train RMSE: 1.8830\n",
      "Train Epoch: 165 [10008/22024] \t Train Loss(MSE): 2.6655 \t Train RMSE: 1.6326\n",
      "Train Epoch: 165 [12008/22024] \t Train Loss(MSE): 5.8154 \t Train RMSE: 2.4115\n",
      "Train Epoch: 165 [14008/22024] \t Train Loss(MSE): 20.4493 \t Train RMSE: 4.5221\n",
      "Train Epoch: 165 [16008/22024] \t Train Loss(MSE): 72.8765 \t Train RMSE: 8.5368\n",
      "Train Epoch: 165 [18008/22024] \t Train Loss(MSE): 12.3549 \t Train RMSE: 3.5150\n",
      "Train Epoch: 165 [20008/22024] \t Train Loss(MSE): 1.2185 \t Train RMSE: 1.1039\n",
      "Train Epoch: 165 [22008/22024] \t Train Loss(MSE): 7.0777 \t Train RMSE: 2.6604\n",
      "[Epoch: 165 \t Valid MSE: 20.6925 \t Valid RMSE: 3.9346]\n",
      "Train Epoch: 166 [8/22024] \t Train Loss(MSE): 3.6522 \t Train RMSE: 1.9111\n",
      "Train Epoch: 166 [2008/22024] \t Train Loss(MSE): 2.7171 \t Train RMSE: 1.6484\n",
      "Train Epoch: 166 [4008/22024] \t Train Loss(MSE): 1.3847 \t Train RMSE: 1.1767\n",
      "Train Epoch: 166 [6008/22024] \t Train Loss(MSE): 6.0374 \t Train RMSE: 2.4571\n",
      "Train Epoch: 166 [8008/22024] \t Train Loss(MSE): 1.1472 \t Train RMSE: 1.0711\n",
      "Train Epoch: 166 [10008/22024] \t Train Loss(MSE): 0.7884 \t Train RMSE: 0.8879\n",
      "Train Epoch: 166 [12008/22024] \t Train Loss(MSE): 2.2633 \t Train RMSE: 1.5044\n",
      "Train Epoch: 166 [14008/22024] \t Train Loss(MSE): 3.5036 \t Train RMSE: 1.8718\n",
      "Train Epoch: 166 [16008/22024] \t Train Loss(MSE): 8.2788 \t Train RMSE: 2.8773\n",
      "Train Epoch: 166 [18008/22024] \t Train Loss(MSE): 1.0222 \t Train RMSE: 1.0110\n",
      "Train Epoch: 166 [20008/22024] \t Train Loss(MSE): 7.6542 \t Train RMSE: 2.7666\n",
      "Train Epoch: 166 [22008/22024] \t Train Loss(MSE): 3.1712 \t Train RMSE: 1.7808\n",
      "[Epoch: 166 \t Valid MSE: 19.2386 \t Valid RMSE: 3.7374]\n",
      "Train Epoch: 167 [8/22024] \t Train Loss(MSE): 11.9836 \t Train RMSE: 3.4617\n",
      "Train Epoch: 167 [2008/22024] \t Train Loss(MSE): 3.7447 \t Train RMSE: 1.9351\n",
      "Train Epoch: 167 [4008/22024] \t Train Loss(MSE): 15.6558 \t Train RMSE: 3.9567\n",
      "Train Epoch: 167 [6008/22024] \t Train Loss(MSE): 35.8153 \t Train RMSE: 5.9846\n",
      "Train Epoch: 167 [8008/22024] \t Train Loss(MSE): 5.5285 \t Train RMSE: 2.3513\n",
      "Train Epoch: 167 [10008/22024] \t Train Loss(MSE): 2.6039 \t Train RMSE: 1.6137\n",
      "Train Epoch: 167 [12008/22024] \t Train Loss(MSE): 7.2584 \t Train RMSE: 2.6941\n",
      "Train Epoch: 167 [14008/22024] \t Train Loss(MSE): 10.0678 \t Train RMSE: 3.1730\n",
      "Train Epoch: 167 [16008/22024] \t Train Loss(MSE): 1.7320 \t Train RMSE: 1.3161\n",
      "Train Epoch: 167 [18008/22024] \t Train Loss(MSE): 3.1921 \t Train RMSE: 1.7866\n",
      "Train Epoch: 167 [20008/22024] \t Train Loss(MSE): 6.1923 \t Train RMSE: 2.4884\n",
      "Train Epoch: 167 [22008/22024] \t Train Loss(MSE): 2.9381 \t Train RMSE: 1.7141\n",
      "[Epoch: 167 \t Valid MSE: 23.6157 \t Valid RMSE: 4.3820]\n",
      "Train Epoch: 168 [8/22024] \t Train Loss(MSE): 14.9537 \t Train RMSE: 3.8670\n",
      "Train Epoch: 168 [2008/22024] \t Train Loss(MSE): 9.5505 \t Train RMSE: 3.0904\n",
      "Train Epoch: 168 [4008/22024] \t Train Loss(MSE): 14.5219 \t Train RMSE: 3.8108\n",
      "Train Epoch: 168 [6008/22024] \t Train Loss(MSE): 1.3437 \t Train RMSE: 1.1592\n",
      "Train Epoch: 168 [8008/22024] \t Train Loss(MSE): 6.6486 \t Train RMSE: 2.5785\n",
      "Train Epoch: 168 [10008/22024] \t Train Loss(MSE): 44.5243 \t Train RMSE: 6.6727\n",
      "Train Epoch: 168 [12008/22024] \t Train Loss(MSE): 6.3956 \t Train RMSE: 2.5290\n",
      "Train Epoch: 168 [14008/22024] \t Train Loss(MSE): 3.2968 \t Train RMSE: 1.8157\n",
      "Train Epoch: 168 [16008/22024] \t Train Loss(MSE): 3.4549 \t Train RMSE: 1.8587\n",
      "Train Epoch: 168 [18008/22024] \t Train Loss(MSE): 18.3695 \t Train RMSE: 4.2860\n",
      "Train Epoch: 168 [20008/22024] \t Train Loss(MSE): 10.3549 \t Train RMSE: 3.2179\n",
      "Train Epoch: 168 [22008/22024] \t Train Loss(MSE): 11.7901 \t Train RMSE: 3.4337\n",
      "[Epoch: 168 \t Valid MSE: 20.3284 \t Valid RMSE: 3.9279]\n",
      "Train Epoch: 169 [8/22024] \t Train Loss(MSE): 2.7170 \t Train RMSE: 1.6483\n",
      "Train Epoch: 169 [2008/22024] \t Train Loss(MSE): 10.5564 \t Train RMSE: 3.2491\n",
      "Train Epoch: 169 [4008/22024] \t Train Loss(MSE): 5.9611 \t Train RMSE: 2.4415\n",
      "Train Epoch: 169 [6008/22024] \t Train Loss(MSE): 10.5218 \t Train RMSE: 3.2437\n",
      "Train Epoch: 169 [8008/22024] \t Train Loss(MSE): 1.1196 \t Train RMSE: 1.0581\n",
      "Train Epoch: 169 [10008/22024] \t Train Loss(MSE): 3.1233 \t Train RMSE: 1.7673\n",
      "Train Epoch: 169 [12008/22024] \t Train Loss(MSE): 8.0435 \t Train RMSE: 2.8361\n",
      "Train Epoch: 169 [14008/22024] \t Train Loss(MSE): 8.0261 \t Train RMSE: 2.8330\n",
      "Train Epoch: 169 [16008/22024] \t Train Loss(MSE): 3.2437 \t Train RMSE: 1.8010\n",
      "Train Epoch: 169 [18008/22024] \t Train Loss(MSE): 5.2541 \t Train RMSE: 2.2922\n",
      "Train Epoch: 169 [20008/22024] \t Train Loss(MSE): 34.2736 \t Train RMSE: 5.8544\n",
      "Train Epoch: 169 [22008/22024] \t Train Loss(MSE): 2.0705 \t Train RMSE: 1.4389\n",
      "[Epoch: 169 \t Valid MSE: 20.1197 \t Valid RMSE: 3.8238]\n",
      "Train Epoch: 170 [8/22024] \t Train Loss(MSE): 2.9870 \t Train RMSE: 1.7283\n",
      "Train Epoch: 170 [2008/22024] \t Train Loss(MSE): 10.5110 \t Train RMSE: 3.2421\n",
      "Train Epoch: 170 [4008/22024] \t Train Loss(MSE): 2.6183 \t Train RMSE: 1.6181\n",
      "Train Epoch: 170 [6008/22024] \t Train Loss(MSE): 12.0470 \t Train RMSE: 3.4709\n",
      "Train Epoch: 170 [8008/22024] \t Train Loss(MSE): 4.4614 \t Train RMSE: 2.1122\n",
      "Train Epoch: 170 [10008/22024] \t Train Loss(MSE): 12.2176 \t Train RMSE: 3.4954\n",
      "Train Epoch: 170 [12008/22024] \t Train Loss(MSE): 28.1983 \t Train RMSE: 5.3102\n",
      "Train Epoch: 170 [14008/22024] \t Train Loss(MSE): 8.1472 \t Train RMSE: 2.8543\n",
      "Train Epoch: 170 [16008/22024] \t Train Loss(MSE): 19.9304 \t Train RMSE: 4.4643\n",
      "Train Epoch: 170 [18008/22024] \t Train Loss(MSE): 3.5154 \t Train RMSE: 1.8749\n",
      "Train Epoch: 170 [20008/22024] \t Train Loss(MSE): 2.5994 \t Train RMSE: 1.6123\n",
      "Train Epoch: 170 [22008/22024] \t Train Loss(MSE): 2.0796 \t Train RMSE: 1.4421\n",
      "[Epoch: 170 \t Valid MSE: 22.7646 \t Valid RMSE: 4.2125]\n",
      "Train Epoch: 171 [8/22024] \t Train Loss(MSE): 53.9474 \t Train RMSE: 7.3449\n",
      "Train Epoch: 171 [2008/22024] \t Train Loss(MSE): 9.6849 \t Train RMSE: 3.1121\n",
      "Train Epoch: 171 [4008/22024] \t Train Loss(MSE): 17.4909 \t Train RMSE: 4.1822\n",
      "Train Epoch: 171 [6008/22024] \t Train Loss(MSE): 11.1012 \t Train RMSE: 3.3318\n",
      "Train Epoch: 171 [8008/22024] \t Train Loss(MSE): 11.3392 \t Train RMSE: 3.3674\n",
      "Train Epoch: 171 [10008/22024] \t Train Loss(MSE): 4.0835 \t Train RMSE: 2.0208\n",
      "Train Epoch: 171 [12008/22024] \t Train Loss(MSE): 5.7734 \t Train RMSE: 2.4028\n",
      "Train Epoch: 171 [14008/22024] \t Train Loss(MSE): 4.1528 \t Train RMSE: 2.0378\n",
      "Train Epoch: 171 [16008/22024] \t Train Loss(MSE): 5.3341 \t Train RMSE: 2.3096\n",
      "Train Epoch: 171 [18008/22024] \t Train Loss(MSE): 34.0552 \t Train RMSE: 5.8357\n",
      "Train Epoch: 171 [20008/22024] \t Train Loss(MSE): 4.6185 \t Train RMSE: 2.1491\n",
      "Train Epoch: 171 [22008/22024] \t Train Loss(MSE): 7.2771 \t Train RMSE: 2.6976\n",
      "[Epoch: 171 \t Valid MSE: 20.3614 \t Valid RMSE: 3.8427]\n",
      "Train Epoch: 172 [8/22024] \t Train Loss(MSE): 0.6094 \t Train RMSE: 0.7806\n",
      "Train Epoch: 172 [2008/22024] \t Train Loss(MSE): 1.8620 \t Train RMSE: 1.3646\n",
      "Train Epoch: 172 [4008/22024] \t Train Loss(MSE): 2.9789 \t Train RMSE: 1.7260\n",
      "Train Epoch: 172 [6008/22024] \t Train Loss(MSE): 13.0407 \t Train RMSE: 3.6112\n",
      "Train Epoch: 172 [8008/22024] \t Train Loss(MSE): 15.6294 \t Train RMSE: 3.9534\n",
      "Train Epoch: 172 [10008/22024] \t Train Loss(MSE): 8.0986 \t Train RMSE: 2.8458\n",
      "Train Epoch: 172 [12008/22024] \t Train Loss(MSE): 3.0488 \t Train RMSE: 1.7461\n",
      "Train Epoch: 172 [14008/22024] \t Train Loss(MSE): 49.2971 \t Train RMSE: 7.0212\n",
      "Train Epoch: 172 [16008/22024] \t Train Loss(MSE): 5.7856 \t Train RMSE: 2.4053\n",
      "Train Epoch: 172 [18008/22024] \t Train Loss(MSE): 5.3233 \t Train RMSE: 2.3072\n",
      "Train Epoch: 172 [20008/22024] \t Train Loss(MSE): 7.2753 \t Train RMSE: 2.6973\n",
      "Train Epoch: 172 [22008/22024] \t Train Loss(MSE): 4.4687 \t Train RMSE: 2.1139\n",
      "[Epoch: 172 \t Valid MSE: 22.6268 \t Valid RMSE: 4.1321]\n",
      "Train Epoch: 173 [8/22024] \t Train Loss(MSE): 6.8085 \t Train RMSE: 2.6093\n",
      "Train Epoch: 173 [2008/22024] \t Train Loss(MSE): 4.2410 \t Train RMSE: 2.0594\n",
      "Train Epoch: 173 [4008/22024] \t Train Loss(MSE): 70.4989 \t Train RMSE: 8.3964\n",
      "Train Epoch: 173 [6008/22024] \t Train Loss(MSE): 1.6351 \t Train RMSE: 1.2787\n",
      "Train Epoch: 173 [8008/22024] \t Train Loss(MSE): 29.6585 \t Train RMSE: 5.4460\n",
      "Train Epoch: 173 [10008/22024] \t Train Loss(MSE): 3.0563 \t Train RMSE: 1.7482\n",
      "Train Epoch: 173 [12008/22024] \t Train Loss(MSE): 52.9068 \t Train RMSE: 7.2737\n",
      "Train Epoch: 173 [14008/22024] \t Train Loss(MSE): 2.0839 \t Train RMSE: 1.4436\n",
      "Train Epoch: 173 [16008/22024] \t Train Loss(MSE): 22.6111 \t Train RMSE: 4.7551\n",
      "Train Epoch: 173 [18008/22024] \t Train Loss(MSE): 0.3269 \t Train RMSE: 0.5718\n",
      "Train Epoch: 173 [20008/22024] \t Train Loss(MSE): 33.3086 \t Train RMSE: 5.7714\n",
      "Train Epoch: 173 [22008/22024] \t Train Loss(MSE): 0.7846 \t Train RMSE: 0.8858\n",
      "[Epoch: 173 \t Valid MSE: 19.7652 \t Valid RMSE: 3.8342]\n",
      "Train Epoch: 174 [8/22024] \t Train Loss(MSE): 14.7250 \t Train RMSE: 3.8373\n",
      "Train Epoch: 174 [2008/22024] \t Train Loss(MSE): 4.7582 \t Train RMSE: 2.1813\n",
      "Train Epoch: 174 [4008/22024] \t Train Loss(MSE): 15.4743 \t Train RMSE: 3.9337\n",
      "Train Epoch: 174 [6008/22024] \t Train Loss(MSE): 4.7229 \t Train RMSE: 2.1732\n",
      "Train Epoch: 174 [8008/22024] \t Train Loss(MSE): 5.5917 \t Train RMSE: 2.3647\n",
      "Train Epoch: 174 [10008/22024] \t Train Loss(MSE): 3.5661 \t Train RMSE: 1.8884\n",
      "Train Epoch: 174 [12008/22024] \t Train Loss(MSE): 0.2879 \t Train RMSE: 0.5366\n",
      "Train Epoch: 174 [14008/22024] \t Train Loss(MSE): 2.2329 \t Train RMSE: 1.4943\n",
      "Train Epoch: 174 [16008/22024] \t Train Loss(MSE): 5.9497 \t Train RMSE: 2.4392\n",
      "Train Epoch: 174 [18008/22024] \t Train Loss(MSE): 53.6556 \t Train RMSE: 7.3250\n",
      "Train Epoch: 174 [20008/22024] \t Train Loss(MSE): 0.5443 \t Train RMSE: 0.7377\n",
      "Train Epoch: 174 [22008/22024] \t Train Loss(MSE): 7.3942 \t Train RMSE: 2.7192\n",
      "[Epoch: 174 \t Valid MSE: 21.4144 \t Valid RMSE: 4.0127]\n",
      "Train Epoch: 175 [8/22024] \t Train Loss(MSE): 6.1659 \t Train RMSE: 2.4831\n",
      "Train Epoch: 175 [2008/22024] \t Train Loss(MSE): 6.3864 \t Train RMSE: 2.5271\n",
      "Train Epoch: 175 [4008/22024] \t Train Loss(MSE): 15.3308 \t Train RMSE: 3.9155\n",
      "Train Epoch: 175 [6008/22024] \t Train Loss(MSE): 1.8678 \t Train RMSE: 1.3667\n",
      "Train Epoch: 175 [8008/22024] \t Train Loss(MSE): 7.1748 \t Train RMSE: 2.6786\n",
      "Train Epoch: 175 [10008/22024] \t Train Loss(MSE): 14.1611 \t Train RMSE: 3.7631\n",
      "Train Epoch: 175 [12008/22024] \t Train Loss(MSE): 24.4122 \t Train RMSE: 4.9409\n",
      "Train Epoch: 175 [14008/22024] \t Train Loss(MSE): 4.3516 \t Train RMSE: 2.0860\n",
      "Train Epoch: 175 [16008/22024] \t Train Loss(MSE): 3.8819 \t Train RMSE: 1.9703\n",
      "Train Epoch: 175 [18008/22024] \t Train Loss(MSE): 2.3217 \t Train RMSE: 1.5237\n",
      "Train Epoch: 175 [20008/22024] \t Train Loss(MSE): 1.8176 \t Train RMSE: 1.3482\n",
      "Train Epoch: 175 [22008/22024] \t Train Loss(MSE): 3.5016 \t Train RMSE: 1.8712\n",
      "[Epoch: 175 \t Valid MSE: 24.5755 \t Valid RMSE: 4.2769]\n",
      "Train Epoch: 176 [8/22024] \t Train Loss(MSE): 3.5959 \t Train RMSE: 1.8963\n",
      "Train Epoch: 176 [2008/22024] \t Train Loss(MSE): 8.3973 \t Train RMSE: 2.8978\n",
      "Train Epoch: 176 [4008/22024] \t Train Loss(MSE): 2.5843 \t Train RMSE: 1.6076\n",
      "Train Epoch: 176 [6008/22024] \t Train Loss(MSE): 0.6938 \t Train RMSE: 0.8329\n",
      "Train Epoch: 176 [8008/22024] \t Train Loss(MSE): 2.4061 \t Train RMSE: 1.5511\n",
      "Train Epoch: 176 [10008/22024] \t Train Loss(MSE): 23.1704 \t Train RMSE: 4.8136\n",
      "Train Epoch: 176 [12008/22024] \t Train Loss(MSE): 27.8661 \t Train RMSE: 5.2788\n",
      "Train Epoch: 176 [14008/22024] \t Train Loss(MSE): 2.0935 \t Train RMSE: 1.4469\n",
      "Train Epoch: 176 [16008/22024] \t Train Loss(MSE): 6.1095 \t Train RMSE: 2.4717\n",
      "Train Epoch: 176 [18008/22024] \t Train Loss(MSE): 70.3818 \t Train RMSE: 8.3894\n",
      "Train Epoch: 176 [20008/22024] \t Train Loss(MSE): 3.4677 \t Train RMSE: 1.8622\n",
      "Train Epoch: 176 [22008/22024] \t Train Loss(MSE): 2.0375 \t Train RMSE: 1.4274\n",
      "[Epoch: 176 \t Valid MSE: 19.7171 \t Valid RMSE: 3.7930]\n",
      "Train Epoch: 177 [8/22024] \t Train Loss(MSE): 10.6154 \t Train RMSE: 3.2581\n",
      "Train Epoch: 177 [2008/22024] \t Train Loss(MSE): 47.8405 \t Train RMSE: 6.9167\n",
      "Train Epoch: 177 [4008/22024] \t Train Loss(MSE): 9.2620 \t Train RMSE: 3.0433\n",
      "Train Epoch: 177 [6008/22024] \t Train Loss(MSE): 3.2537 \t Train RMSE: 1.8038\n",
      "Train Epoch: 177 [8008/22024] \t Train Loss(MSE): 23.2736 \t Train RMSE: 4.8243\n",
      "Train Epoch: 177 [10008/22024] \t Train Loss(MSE): 11.2956 \t Train RMSE: 3.3609\n",
      "Train Epoch: 177 [12008/22024] \t Train Loss(MSE): 4.8019 \t Train RMSE: 2.1913\n",
      "Train Epoch: 177 [14008/22024] \t Train Loss(MSE): 0.6866 \t Train RMSE: 0.8286\n",
      "Train Epoch: 177 [16008/22024] \t Train Loss(MSE): 32.3718 \t Train RMSE: 5.6896\n",
      "Train Epoch: 177 [18008/22024] \t Train Loss(MSE): 7.2182 \t Train RMSE: 2.6867\n",
      "Train Epoch: 177 [20008/22024] \t Train Loss(MSE): 7.9098 \t Train RMSE: 2.8124\n",
      "Train Epoch: 177 [22008/22024] \t Train Loss(MSE): 53.5858 \t Train RMSE: 7.3202\n",
      "[Epoch: 177 \t Valid MSE: 22.8173 \t Valid RMSE: 4.1949]\n",
      "Train Epoch: 178 [8/22024] \t Train Loss(MSE): 53.9759 \t Train RMSE: 7.3468\n",
      "Train Epoch: 178 [2008/22024] \t Train Loss(MSE): 3.6050 \t Train RMSE: 1.8987\n",
      "Train Epoch: 178 [4008/22024] \t Train Loss(MSE): 11.5725 \t Train RMSE: 3.4018\n",
      "Train Epoch: 178 [6008/22024] \t Train Loss(MSE): 4.6274 \t Train RMSE: 2.1511\n",
      "Train Epoch: 178 [8008/22024] \t Train Loss(MSE): 11.7980 \t Train RMSE: 3.4348\n",
      "Train Epoch: 178 [10008/22024] \t Train Loss(MSE): 4.8117 \t Train RMSE: 2.1935\n",
      "Train Epoch: 178 [12008/22024] \t Train Loss(MSE): 2.5994 \t Train RMSE: 1.6123\n",
      "Train Epoch: 178 [14008/22024] \t Train Loss(MSE): 7.2485 \t Train RMSE: 2.6923\n",
      "Train Epoch: 178 [16008/22024] \t Train Loss(MSE): 178.9046 \t Train RMSE: 13.3755\n",
      "Train Epoch: 178 [18008/22024] \t Train Loss(MSE): 58.5309 \t Train RMSE: 7.6505\n",
      "Train Epoch: 178 [20008/22024] \t Train Loss(MSE): 3.8971 \t Train RMSE: 1.9741\n",
      "Train Epoch: 178 [22008/22024] \t Train Loss(MSE): 5.8486 \t Train RMSE: 2.4184\n",
      "[Epoch: 178 \t Valid MSE: 20.6616 \t Valid RMSE: 3.9018]\n",
      "Train Epoch: 179 [8/22024] \t Train Loss(MSE): 2.8793 \t Train RMSE: 1.6968\n",
      "Train Epoch: 179 [2008/22024] \t Train Loss(MSE): 1.9846 \t Train RMSE: 1.4088\n",
      "Train Epoch: 179 [4008/22024] \t Train Loss(MSE): 3.1741 \t Train RMSE: 1.7816\n",
      "Train Epoch: 179 [6008/22024] \t Train Loss(MSE): 7.4935 \t Train RMSE: 2.7374\n",
      "Train Epoch: 179 [8008/22024] \t Train Loss(MSE): 3.0735 \t Train RMSE: 1.7531\n",
      "Train Epoch: 179 [10008/22024] \t Train Loss(MSE): 6.1507 \t Train RMSE: 2.4801\n",
      "Train Epoch: 179 [12008/22024] \t Train Loss(MSE): 2.5271 \t Train RMSE: 1.5897\n",
      "Train Epoch: 179 [14008/22024] \t Train Loss(MSE): 14.6531 \t Train RMSE: 3.8279\n",
      "Train Epoch: 179 [16008/22024] \t Train Loss(MSE): 10.7139 \t Train RMSE: 3.2732\n",
      "Train Epoch: 179 [18008/22024] \t Train Loss(MSE): 7.4926 \t Train RMSE: 2.7373\n",
      "Train Epoch: 179 [20008/22024] \t Train Loss(MSE): 6.1278 \t Train RMSE: 2.4754\n",
      "Train Epoch: 179 [22008/22024] \t Train Loss(MSE): 1.7128 \t Train RMSE: 1.3087\n",
      "[Epoch: 179 \t Valid MSE: 20.4079 \t Valid RMSE: 3.9338]\n",
      "Train Epoch: 180 [8/22024] \t Train Loss(MSE): 1.0207 \t Train RMSE: 1.0103\n",
      "Train Epoch: 180 [2008/22024] \t Train Loss(MSE): 5.6521 \t Train RMSE: 2.3774\n",
      "Train Epoch: 180 [4008/22024] \t Train Loss(MSE): 46.4235 \t Train RMSE: 6.8135\n",
      "Train Epoch: 180 [6008/22024] \t Train Loss(MSE): 3.3968 \t Train RMSE: 1.8430\n",
      "Train Epoch: 180 [8008/22024] \t Train Loss(MSE): 3.5376 \t Train RMSE: 1.8808\n",
      "Train Epoch: 180 [10008/22024] \t Train Loss(MSE): 0.8123 \t Train RMSE: 0.9013\n",
      "Train Epoch: 180 [12008/22024] \t Train Loss(MSE): 62.1230 \t Train RMSE: 7.8818\n",
      "Train Epoch: 180 [14008/22024] \t Train Loss(MSE): 14.3188 \t Train RMSE: 3.7840\n",
      "Train Epoch: 180 [16008/22024] \t Train Loss(MSE): 2.9078 \t Train RMSE: 1.7052\n",
      "Train Epoch: 180 [18008/22024] \t Train Loss(MSE): 2.9042 \t Train RMSE: 1.7042\n",
      "Train Epoch: 180 [20008/22024] \t Train Loss(MSE): 13.7765 \t Train RMSE: 3.7117\n",
      "Train Epoch: 180 [22008/22024] \t Train Loss(MSE): 5.2504 \t Train RMSE: 2.2914\n",
      "[Epoch: 180 \t Valid MSE: 23.2230 \t Valid RMSE: 4.2731]\n",
      "Train Epoch: 181 [8/22024] \t Train Loss(MSE): 22.1073 \t Train RMSE: 4.7018\n",
      "Train Epoch: 181 [2008/22024] \t Train Loss(MSE): 2.5535 \t Train RMSE: 1.5980\n",
      "Train Epoch: 181 [4008/22024] \t Train Loss(MSE): 6.7823 \t Train RMSE: 2.6043\n",
      "Train Epoch: 181 [6008/22024] \t Train Loss(MSE): 3.2266 \t Train RMSE: 1.7963\n",
      "Train Epoch: 181 [8008/22024] \t Train Loss(MSE): 3.1305 \t Train RMSE: 1.7693\n",
      "Train Epoch: 181 [10008/22024] \t Train Loss(MSE): 3.1956 \t Train RMSE: 1.7876\n",
      "Train Epoch: 181 [12008/22024] \t Train Loss(MSE): 4.2886 \t Train RMSE: 2.0709\n",
      "Train Epoch: 181 [14008/22024] \t Train Loss(MSE): 2.4766 \t Train RMSE: 1.5737\n",
      "Train Epoch: 181 [16008/22024] \t Train Loss(MSE): 15.6376 \t Train RMSE: 3.9544\n",
      "Train Epoch: 181 [18008/22024] \t Train Loss(MSE): 39.9166 \t Train RMSE: 6.3180\n",
      "Train Epoch: 181 [20008/22024] \t Train Loss(MSE): 3.6892 \t Train RMSE: 1.9207\n",
      "Train Epoch: 181 [22008/22024] \t Train Loss(MSE): 1.8583 \t Train RMSE: 1.3632\n",
      "[Epoch: 181 \t Valid MSE: 23.3025 \t Valid RMSE: 4.3082]\n",
      "Train Epoch: 182 [8/22024] \t Train Loss(MSE): 3.5539 \t Train RMSE: 1.8852\n",
      "Train Epoch: 182 [2008/22024] \t Train Loss(MSE): 4.3878 \t Train RMSE: 2.0947\n",
      "Train Epoch: 182 [4008/22024] \t Train Loss(MSE): 2.3384 \t Train RMSE: 1.5292\n",
      "Train Epoch: 182 [6008/22024] \t Train Loss(MSE): 22.5268 \t Train RMSE: 4.7462\n",
      "Train Epoch: 182 [8008/22024] \t Train Loss(MSE): 8.3616 \t Train RMSE: 2.8916\n",
      "Train Epoch: 182 [10008/22024] \t Train Loss(MSE): 3.5819 \t Train RMSE: 1.8926\n",
      "Train Epoch: 182 [12008/22024] \t Train Loss(MSE): 13.8337 \t Train RMSE: 3.7194\n",
      "Train Epoch: 182 [14008/22024] \t Train Loss(MSE): 30.1122 \t Train RMSE: 5.4875\n",
      "Train Epoch: 182 [16008/22024] \t Train Loss(MSE): 6.5107 \t Train RMSE: 2.5516\n",
      "Train Epoch: 182 [18008/22024] \t Train Loss(MSE): 21.9248 \t Train RMSE: 4.6824\n",
      "Train Epoch: 182 [20008/22024] \t Train Loss(MSE): 11.3793 \t Train RMSE: 3.3733\n",
      "Train Epoch: 182 [22008/22024] \t Train Loss(MSE): 27.5983 \t Train RMSE: 5.2534\n",
      "[Epoch: 182 \t Valid MSE: 23.0185 \t Valid RMSE: 4.2202]\n",
      "Train Epoch: 183 [8/22024] \t Train Loss(MSE): 6.4669 \t Train RMSE: 2.5430\n",
      "Train Epoch: 183 [2008/22024] \t Train Loss(MSE): 2.1954 \t Train RMSE: 1.4817\n",
      "Train Epoch: 183 [4008/22024] \t Train Loss(MSE): 2.4122 \t Train RMSE: 1.5531\n",
      "Train Epoch: 183 [6008/22024] \t Train Loss(MSE): 11.6927 \t Train RMSE: 3.4195\n",
      "Train Epoch: 183 [8008/22024] \t Train Loss(MSE): 10.2026 \t Train RMSE: 3.1941\n",
      "Train Epoch: 183 [10008/22024] \t Train Loss(MSE): 9.5123 \t Train RMSE: 3.0842\n",
      "Train Epoch: 183 [12008/22024] \t Train Loss(MSE): 2.5429 \t Train RMSE: 1.5946\n",
      "Train Epoch: 183 [14008/22024] \t Train Loss(MSE): 3.3455 \t Train RMSE: 1.8291\n",
      "Train Epoch: 183 [16008/22024] \t Train Loss(MSE): 2.6498 \t Train RMSE: 1.6278\n",
      "Train Epoch: 183 [18008/22024] \t Train Loss(MSE): 2.9661 \t Train RMSE: 1.7222\n",
      "Train Epoch: 183 [20008/22024] \t Train Loss(MSE): 11.5879 \t Train RMSE: 3.4041\n",
      "Train Epoch: 183 [22008/22024] \t Train Loss(MSE): 10.0991 \t Train RMSE: 3.1779\n",
      "[Epoch: 183 \t Valid MSE: 20.3995 \t Valid RMSE: 3.9228]\n",
      "Train Epoch: 184 [8/22024] \t Train Loss(MSE): 3.4241 \t Train RMSE: 1.8504\n",
      "Train Epoch: 184 [2008/22024] \t Train Loss(MSE): 6.3304 \t Train RMSE: 2.5160\n",
      "Train Epoch: 184 [4008/22024] \t Train Loss(MSE): 11.5478 \t Train RMSE: 3.3982\n",
      "Train Epoch: 184 [6008/22024] \t Train Loss(MSE): 6.7156 \t Train RMSE: 2.5914\n",
      "Train Epoch: 184 [8008/22024] \t Train Loss(MSE): 5.3516 \t Train RMSE: 2.3133\n",
      "Train Epoch: 184 [10008/22024] \t Train Loss(MSE): 43.2374 \t Train RMSE: 6.5755\n",
      "Train Epoch: 184 [12008/22024] \t Train Loss(MSE): 4.5720 \t Train RMSE: 2.1382\n",
      "Train Epoch: 184 [14008/22024] \t Train Loss(MSE): 4.5654 \t Train RMSE: 2.1367\n",
      "Train Epoch: 184 [16008/22024] \t Train Loss(MSE): 10.4535 \t Train RMSE: 3.2332\n",
      "Train Epoch: 184 [18008/22024] \t Train Loss(MSE): 2.0515 \t Train RMSE: 1.4323\n",
      "Train Epoch: 184 [20008/22024] \t Train Loss(MSE): 2.7420 \t Train RMSE: 1.6559\n",
      "Train Epoch: 184 [22008/22024] \t Train Loss(MSE): 9.6102 \t Train RMSE: 3.1000\n",
      "[Epoch: 184 \t Valid MSE: 22.3121 \t Valid RMSE: 4.1834]\n",
      "Train Epoch: 185 [8/22024] \t Train Loss(MSE): 11.3476 \t Train RMSE: 3.3686\n",
      "Train Epoch: 185 [2008/22024] \t Train Loss(MSE): 1.7978 \t Train RMSE: 1.3408\n",
      "Train Epoch: 185 [4008/22024] \t Train Loss(MSE): 15.2645 \t Train RMSE: 3.9070\n",
      "Train Epoch: 185 [6008/22024] \t Train Loss(MSE): 5.5846 \t Train RMSE: 2.3632\n",
      "Train Epoch: 185 [8008/22024] \t Train Loss(MSE): 6.0469 \t Train RMSE: 2.4591\n",
      "Train Epoch: 185 [10008/22024] \t Train Loss(MSE): 11.2831 \t Train RMSE: 3.3590\n",
      "Train Epoch: 185 [12008/22024] \t Train Loss(MSE): 3.4796 \t Train RMSE: 1.8654\n",
      "Train Epoch: 185 [14008/22024] \t Train Loss(MSE): 1.1536 \t Train RMSE: 1.0741\n",
      "Train Epoch: 185 [16008/22024] \t Train Loss(MSE): 119.3891 \t Train RMSE: 10.9265\n",
      "Train Epoch: 185 [18008/22024] \t Train Loss(MSE): 12.3661 \t Train RMSE: 3.5165\n",
      "Train Epoch: 185 [20008/22024] \t Train Loss(MSE): 106.7388 \t Train RMSE: 10.3314\n",
      "Train Epoch: 185 [22008/22024] \t Train Loss(MSE): 3.6200 \t Train RMSE: 1.9026\n",
      "[Epoch: 185 \t Valid MSE: 20.9134 \t Valid RMSE: 3.9334]\n",
      "Train Epoch: 186 [8/22024] \t Train Loss(MSE): 1.6798 \t Train RMSE: 1.2961\n",
      "Train Epoch: 186 [2008/22024] \t Train Loss(MSE): 4.9554 \t Train RMSE: 2.2261\n",
      "Train Epoch: 186 [4008/22024] \t Train Loss(MSE): 7.4997 \t Train RMSE: 2.7386\n",
      "Train Epoch: 186 [6008/22024] \t Train Loss(MSE): 3.3138 \t Train RMSE: 1.8204\n",
      "Train Epoch: 186 [8008/22024] \t Train Loss(MSE): 5.4149 \t Train RMSE: 2.3270\n",
      "Train Epoch: 186 [10008/22024] \t Train Loss(MSE): 6.3318 \t Train RMSE: 2.5163\n",
      "Train Epoch: 186 [12008/22024] \t Train Loss(MSE): 6.5683 \t Train RMSE: 2.5629\n",
      "Train Epoch: 186 [14008/22024] \t Train Loss(MSE): 3.8900 \t Train RMSE: 1.9723\n",
      "Train Epoch: 186 [16008/22024] \t Train Loss(MSE): 7.6996 \t Train RMSE: 2.7748\n",
      "Train Epoch: 186 [18008/22024] \t Train Loss(MSE): 11.0561 \t Train RMSE: 3.3251\n",
      "Train Epoch: 186 [20008/22024] \t Train Loss(MSE): 16.6334 \t Train RMSE: 4.0784\n",
      "Train Epoch: 186 [22008/22024] \t Train Loss(MSE): 16.8374 \t Train RMSE: 4.1033\n",
      "[Epoch: 186 \t Valid MSE: 21.1498 \t Valid RMSE: 3.9994]\n",
      "Train Epoch: 187 [8/22024] \t Train Loss(MSE): 1.2227 \t Train RMSE: 1.1058\n",
      "Train Epoch: 187 [2008/22024] \t Train Loss(MSE): 28.8575 \t Train RMSE: 5.3719\n",
      "Train Epoch: 187 [4008/22024] \t Train Loss(MSE): 2.4217 \t Train RMSE: 1.5562\n",
      "Train Epoch: 187 [6008/22024] \t Train Loss(MSE): 5.1641 \t Train RMSE: 2.2725\n",
      "Train Epoch: 187 [8008/22024] \t Train Loss(MSE): 4.4346 \t Train RMSE: 2.1059\n",
      "Train Epoch: 187 [10008/22024] \t Train Loss(MSE): 7.5926 \t Train RMSE: 2.7555\n",
      "Train Epoch: 187 [12008/22024] \t Train Loss(MSE): 3.6977 \t Train RMSE: 1.9229\n",
      "Train Epoch: 187 [14008/22024] \t Train Loss(MSE): 9.0733 \t Train RMSE: 3.0122\n",
      "Train Epoch: 187 [16008/22024] \t Train Loss(MSE): 2.0503 \t Train RMSE: 1.4319\n",
      "Train Epoch: 187 [18008/22024] \t Train Loss(MSE): 5.2825 \t Train RMSE: 2.2984\n",
      "Train Epoch: 187 [20008/22024] \t Train Loss(MSE): 0.0932 \t Train RMSE: 0.3052\n",
      "Train Epoch: 187 [22008/22024] \t Train Loss(MSE): 35.4482 \t Train RMSE: 5.9538\n",
      "[Epoch: 187 \t Valid MSE: 20.3486 \t Valid RMSE: 3.9453]\n",
      "Train Epoch: 188 [8/22024] \t Train Loss(MSE): 54.3682 \t Train RMSE: 7.3735\n",
      "Train Epoch: 188 [2008/22024] \t Train Loss(MSE): 1.9680 \t Train RMSE: 1.4029\n",
      "Train Epoch: 188 [4008/22024] \t Train Loss(MSE): 8.7720 \t Train RMSE: 2.9618\n",
      "Train Epoch: 188 [6008/22024] \t Train Loss(MSE): 4.2505 \t Train RMSE: 2.0617\n",
      "Train Epoch: 188 [8008/22024] \t Train Loss(MSE): 3.2829 \t Train RMSE: 1.8119\n",
      "Train Epoch: 188 [10008/22024] \t Train Loss(MSE): 6.5009 \t Train RMSE: 2.5497\n",
      "Train Epoch: 188 [12008/22024] \t Train Loss(MSE): 9.4461 \t Train RMSE: 3.0735\n",
      "Train Epoch: 188 [14008/22024] \t Train Loss(MSE): 0.9648 \t Train RMSE: 0.9823\n",
      "Train Epoch: 188 [16008/22024] \t Train Loss(MSE): 4.2796 \t Train RMSE: 2.0687\n",
      "Train Epoch: 188 [18008/22024] \t Train Loss(MSE): 4.4036 \t Train RMSE: 2.0985\n",
      "Train Epoch: 188 [20008/22024] \t Train Loss(MSE): 11.2811 \t Train RMSE: 3.3587\n",
      "Train Epoch: 188 [22008/22024] \t Train Loss(MSE): 3.2945 \t Train RMSE: 1.8151\n",
      "[Epoch: 188 \t Valid MSE: 24.6348 \t Valid RMSE: 4.3151]\n",
      "Train Epoch: 189 [8/22024] \t Train Loss(MSE): 40.1926 \t Train RMSE: 6.3398\n",
      "Train Epoch: 189 [2008/22024] \t Train Loss(MSE): 2.4417 \t Train RMSE: 1.5626\n",
      "Train Epoch: 189 [4008/22024] \t Train Loss(MSE): 0.9703 \t Train RMSE: 0.9850\n",
      "Train Epoch: 189 [6008/22024] \t Train Loss(MSE): 5.4271 \t Train RMSE: 2.3296\n",
      "Train Epoch: 189 [8008/22024] \t Train Loss(MSE): 2.7563 \t Train RMSE: 1.6602\n",
      "Train Epoch: 189 [10008/22024] \t Train Loss(MSE): 1.7591 \t Train RMSE: 1.3263\n",
      "Train Epoch: 189 [12008/22024] \t Train Loss(MSE): 47.7486 \t Train RMSE: 6.9100\n",
      "Train Epoch: 189 [14008/22024] \t Train Loss(MSE): 2.7654 \t Train RMSE: 1.6629\n",
      "Train Epoch: 189 [16008/22024] \t Train Loss(MSE): 1.9776 \t Train RMSE: 1.4063\n",
      "Train Epoch: 189 [18008/22024] \t Train Loss(MSE): 1.8693 \t Train RMSE: 1.3672\n",
      "Train Epoch: 189 [20008/22024] \t Train Loss(MSE): 10.6579 \t Train RMSE: 3.2647\n",
      "Train Epoch: 189 [22008/22024] \t Train Loss(MSE): 3.7326 \t Train RMSE: 1.9320\n",
      "[Epoch: 189 \t Valid MSE: 22.3892 \t Valid RMSE: 4.1540]\n",
      "Train Epoch: 190 [8/22024] \t Train Loss(MSE): 4.1974 \t Train RMSE: 2.0488\n",
      "Train Epoch: 190 [2008/22024] \t Train Loss(MSE): 24.8494 \t Train RMSE: 4.9849\n",
      "Train Epoch: 190 [4008/22024] \t Train Loss(MSE): 0.2625 \t Train RMSE: 0.5123\n",
      "Train Epoch: 190 [6008/22024] \t Train Loss(MSE): 4.2051 \t Train RMSE: 2.0506\n",
      "Train Epoch: 190 [8008/22024] \t Train Loss(MSE): 2.0130 \t Train RMSE: 1.4188\n",
      "Train Epoch: 190 [10008/22024] \t Train Loss(MSE): 1.6625 \t Train RMSE: 1.2894\n",
      "Train Epoch: 190 [12008/22024] \t Train Loss(MSE): 25.8519 \t Train RMSE: 5.0845\n",
      "Train Epoch: 190 [14008/22024] \t Train Loss(MSE): 5.3085 \t Train RMSE: 2.3040\n",
      "Train Epoch: 190 [16008/22024] \t Train Loss(MSE): 6.2694 \t Train RMSE: 2.5039\n",
      "Train Epoch: 190 [18008/22024] \t Train Loss(MSE): 12.8770 \t Train RMSE: 3.5884\n",
      "Train Epoch: 190 [20008/22024] \t Train Loss(MSE): 2.5505 \t Train RMSE: 1.5970\n",
      "Train Epoch: 190 [22008/22024] \t Train Loss(MSE): 3.8813 \t Train RMSE: 1.9701\n",
      "[Epoch: 190 \t Valid MSE: 21.2901 \t Valid RMSE: 4.0425]\n",
      "Train Epoch: 191 [8/22024] \t Train Loss(MSE): 2.1346 \t Train RMSE: 1.4610\n",
      "Train Epoch: 191 [2008/22024] \t Train Loss(MSE): 2.1259 \t Train RMSE: 1.4580\n",
      "Train Epoch: 191 [4008/22024] \t Train Loss(MSE): 14.9474 \t Train RMSE: 3.8662\n",
      "Train Epoch: 191 [6008/22024] \t Train Loss(MSE): 2.5724 \t Train RMSE: 1.6039\n",
      "Train Epoch: 191 [8008/22024] \t Train Loss(MSE): 1.8750 \t Train RMSE: 1.3693\n",
      "Train Epoch: 191 [10008/22024] \t Train Loss(MSE): 6.2513 \t Train RMSE: 2.5003\n",
      "Train Epoch: 191 [12008/22024] \t Train Loss(MSE): 18.1632 \t Train RMSE: 4.2618\n",
      "Train Epoch: 191 [14008/22024] \t Train Loss(MSE): 8.8326 \t Train RMSE: 2.9720\n",
      "Train Epoch: 191 [16008/22024] \t Train Loss(MSE): 7.0578 \t Train RMSE: 2.6567\n",
      "Train Epoch: 191 [18008/22024] \t Train Loss(MSE): 7.5109 \t Train RMSE: 2.7406\n",
      "Train Epoch: 191 [20008/22024] \t Train Loss(MSE): 7.3749 \t Train RMSE: 2.7157\n",
      "Train Epoch: 191 [22008/22024] \t Train Loss(MSE): 34.6185 \t Train RMSE: 5.8838\n",
      "[Epoch: 191 \t Valid MSE: 21.7142 \t Valid RMSE: 4.1169]\n",
      "Train Epoch: 192 [8/22024] \t Train Loss(MSE): 8.9994 \t Train RMSE: 2.9999\n",
      "Train Epoch: 192 [2008/22024] \t Train Loss(MSE): 7.6710 \t Train RMSE: 2.7697\n",
      "Train Epoch: 192 [4008/22024] \t Train Loss(MSE): 7.3541 \t Train RMSE: 2.7119\n",
      "Train Epoch: 192 [6008/22024] \t Train Loss(MSE): 11.8753 \t Train RMSE: 3.4461\n",
      "Train Epoch: 192 [8008/22024] \t Train Loss(MSE): 29.0797 \t Train RMSE: 5.3926\n",
      "Train Epoch: 192 [10008/22024] \t Train Loss(MSE): 3.7437 \t Train RMSE: 1.9349\n",
      "Train Epoch: 192 [12008/22024] \t Train Loss(MSE): 3.7219 \t Train RMSE: 1.9292\n",
      "Train Epoch: 192 [14008/22024] \t Train Loss(MSE): 6.5255 \t Train RMSE: 2.5545\n",
      "Train Epoch: 192 [16008/22024] \t Train Loss(MSE): 0.9452 \t Train RMSE: 0.9722\n",
      "Train Epoch: 192 [18008/22024] \t Train Loss(MSE): 33.6899 \t Train RMSE: 5.8043\n",
      "Train Epoch: 192 [20008/22024] \t Train Loss(MSE): 10.5441 \t Train RMSE: 3.2472\n",
      "Train Epoch: 192 [22008/22024] \t Train Loss(MSE): 1.2108 \t Train RMSE: 1.1004\n",
      "[Epoch: 192 \t Valid MSE: 22.6799 \t Valid RMSE: 4.1242]\n",
      "Train Epoch: 193 [8/22024] \t Train Loss(MSE): 5.6069 \t Train RMSE: 2.3679\n",
      "Train Epoch: 193 [2008/22024] \t Train Loss(MSE): 7.1447 \t Train RMSE: 2.6730\n",
      "Train Epoch: 193 [4008/22024] \t Train Loss(MSE): 9.5538 \t Train RMSE: 3.0909\n",
      "Train Epoch: 193 [6008/22024] \t Train Loss(MSE): 11.4588 \t Train RMSE: 3.3851\n",
      "Train Epoch: 193 [8008/22024] \t Train Loss(MSE): 9.0662 \t Train RMSE: 3.0110\n",
      "Train Epoch: 193 [10008/22024] \t Train Loss(MSE): 3.5631 \t Train RMSE: 1.8876\n",
      "Train Epoch: 193 [12008/22024] \t Train Loss(MSE): 3.5911 \t Train RMSE: 1.8950\n",
      "Train Epoch: 193 [14008/22024] \t Train Loss(MSE): 76.9082 \t Train RMSE: 8.7697\n",
      "Train Epoch: 193 [16008/22024] \t Train Loss(MSE): 2.7284 \t Train RMSE: 1.6518\n",
      "Train Epoch: 193 [18008/22024] \t Train Loss(MSE): 3.4316 \t Train RMSE: 1.8524\n",
      "Train Epoch: 193 [20008/22024] \t Train Loss(MSE): 8.6057 \t Train RMSE: 2.9335\n",
      "Train Epoch: 193 [22008/22024] \t Train Loss(MSE): 2.0885 \t Train RMSE: 1.4452\n",
      "[Epoch: 193 \t Valid MSE: 19.8167 \t Valid RMSE: 3.8234]\n",
      "Train Epoch: 194 [8/22024] \t Train Loss(MSE): 36.4179 \t Train RMSE: 6.0347\n",
      "Train Epoch: 194 [2008/22024] \t Train Loss(MSE): 3.9232 \t Train RMSE: 1.9807\n",
      "Train Epoch: 194 [4008/22024] \t Train Loss(MSE): 13.3044 \t Train RMSE: 3.6475\n",
      "Train Epoch: 194 [6008/22024] \t Train Loss(MSE): 1.2873 \t Train RMSE: 1.1346\n",
      "Train Epoch: 194 [8008/22024] \t Train Loss(MSE): 1.9135 \t Train RMSE: 1.3833\n",
      "Train Epoch: 194 [10008/22024] \t Train Loss(MSE): 2.6014 \t Train RMSE: 1.6129\n",
      "Train Epoch: 194 [12008/22024] \t Train Loss(MSE): 8.5768 \t Train RMSE: 2.9286\n",
      "Train Epoch: 194 [14008/22024] \t Train Loss(MSE): 1.8893 \t Train RMSE: 1.3745\n",
      "Train Epoch: 194 [16008/22024] \t Train Loss(MSE): 5.7355 \t Train RMSE: 2.3949\n",
      "Train Epoch: 194 [18008/22024] \t Train Loss(MSE): 7.9048 \t Train RMSE: 2.8115\n",
      "Train Epoch: 194 [20008/22024] \t Train Loss(MSE): 4.1928 \t Train RMSE: 2.0476\n",
      "Train Epoch: 194 [22008/22024] \t Train Loss(MSE): 33.7666 \t Train RMSE: 5.8109\n",
      "[Epoch: 194 \t Valid MSE: 22.4808 \t Valid RMSE: 4.1670]\n",
      "Train Epoch: 195 [8/22024] \t Train Loss(MSE): 27.2620 \t Train RMSE: 5.2213\n",
      "Train Epoch: 195 [2008/22024] \t Train Loss(MSE): 7.5181 \t Train RMSE: 2.7419\n",
      "Train Epoch: 195 [4008/22024] \t Train Loss(MSE): 7.4908 \t Train RMSE: 2.7369\n",
      "Train Epoch: 195 [6008/22024] \t Train Loss(MSE): 0.9578 \t Train RMSE: 0.9787\n",
      "Train Epoch: 195 [8008/22024] \t Train Loss(MSE): 12.8027 \t Train RMSE: 3.5781\n",
      "Train Epoch: 195 [10008/22024] \t Train Loss(MSE): 3.6171 \t Train RMSE: 1.9019\n",
      "Train Epoch: 195 [12008/22024] \t Train Loss(MSE): 11.0502 \t Train RMSE: 3.3242\n",
      "Train Epoch: 195 [14008/22024] \t Train Loss(MSE): 20.5855 \t Train RMSE: 4.5371\n",
      "Train Epoch: 195 [16008/22024] \t Train Loss(MSE): 0.6762 \t Train RMSE: 0.8223\n",
      "Train Epoch: 195 [18008/22024] \t Train Loss(MSE): 5.4905 \t Train RMSE: 2.3432\n",
      "Train Epoch: 195 [20008/22024] \t Train Loss(MSE): 7.7251 \t Train RMSE: 2.7794\n",
      "Train Epoch: 195 [22008/22024] \t Train Loss(MSE): 20.8738 \t Train RMSE: 4.5688\n",
      "[Epoch: 195 \t Valid MSE: 20.4094 \t Valid RMSE: 3.8915]\n",
      "Train Epoch: 196 [8/22024] \t Train Loss(MSE): 28.0497 \t Train RMSE: 5.2962\n",
      "Train Epoch: 196 [2008/22024] \t Train Loss(MSE): 12.2022 \t Train RMSE: 3.4932\n",
      "Train Epoch: 196 [4008/22024] \t Train Loss(MSE): 34.5969 \t Train RMSE: 5.8819\n",
      "Train Epoch: 196 [6008/22024] \t Train Loss(MSE): 13.5238 \t Train RMSE: 3.6775\n",
      "Train Epoch: 196 [8008/22024] \t Train Loss(MSE): 4.4447 \t Train RMSE: 2.1082\n",
      "Train Epoch: 196 [10008/22024] \t Train Loss(MSE): 19.8538 \t Train RMSE: 4.4558\n",
      "Train Epoch: 196 [12008/22024] \t Train Loss(MSE): 1.0453 \t Train RMSE: 1.0224\n",
      "Train Epoch: 196 [14008/22024] \t Train Loss(MSE): 1.9762 \t Train RMSE: 1.4058\n",
      "Train Epoch: 196 [16008/22024] \t Train Loss(MSE): 52.7224 \t Train RMSE: 7.2610\n",
      "Train Epoch: 196 [18008/22024] \t Train Loss(MSE): 1.6246 \t Train RMSE: 1.2746\n",
      "Train Epoch: 196 [20008/22024] \t Train Loss(MSE): 3.7648 \t Train RMSE: 1.9403\n",
      "Train Epoch: 196 [22008/22024] \t Train Loss(MSE): 2.0406 \t Train RMSE: 1.4285\n",
      "[Epoch: 196 \t Valid MSE: 20.7884 \t Valid RMSE: 3.9544]\n",
      "Train Epoch: 197 [8/22024] \t Train Loss(MSE): 1.1249 \t Train RMSE: 1.0606\n",
      "Train Epoch: 197 [2008/22024] \t Train Loss(MSE): 2.5213 \t Train RMSE: 1.5878\n",
      "Train Epoch: 197 [4008/22024] \t Train Loss(MSE): 3.6580 \t Train RMSE: 1.9126\n",
      "Train Epoch: 197 [6008/22024] \t Train Loss(MSE): 1.7426 \t Train RMSE: 1.3201\n",
      "Train Epoch: 197 [8008/22024] \t Train Loss(MSE): 23.8594 \t Train RMSE: 4.8846\n",
      "Train Epoch: 197 [10008/22024] \t Train Loss(MSE): 3.8174 \t Train RMSE: 1.9538\n",
      "Train Epoch: 197 [12008/22024] \t Train Loss(MSE): 109.5519 \t Train RMSE: 10.4667\n",
      "Train Epoch: 197 [14008/22024] \t Train Loss(MSE): 0.5292 \t Train RMSE: 0.7275\n",
      "Train Epoch: 197 [16008/22024] \t Train Loss(MSE): 13.4646 \t Train RMSE: 3.6694\n",
      "Train Epoch: 197 [18008/22024] \t Train Loss(MSE): 1.7758 \t Train RMSE: 1.3326\n",
      "Train Epoch: 197 [20008/22024] \t Train Loss(MSE): 13.8940 \t Train RMSE: 3.7275\n",
      "Train Epoch: 197 [22008/22024] \t Train Loss(MSE): 2.4192 \t Train RMSE: 1.5554\n",
      "[Epoch: 197 \t Valid MSE: 20.5896 \t Valid RMSE: 3.9686]\n",
      "Train Epoch: 198 [8/22024] \t Train Loss(MSE): 1.0300 \t Train RMSE: 1.0149\n",
      "Train Epoch: 198 [2008/22024] \t Train Loss(MSE): 7.7570 \t Train RMSE: 2.7851\n",
      "Train Epoch: 198 [4008/22024] \t Train Loss(MSE): 4.7452 \t Train RMSE: 2.1783\n",
      "Train Epoch: 198 [6008/22024] \t Train Loss(MSE): 4.0834 \t Train RMSE: 2.0208\n",
      "Train Epoch: 198 [8008/22024] \t Train Loss(MSE): 9.9924 \t Train RMSE: 3.1611\n",
      "Train Epoch: 198 [10008/22024] \t Train Loss(MSE): 1.8713 \t Train RMSE: 1.3680\n",
      "Train Epoch: 198 [12008/22024] \t Train Loss(MSE): 8.6832 \t Train RMSE: 2.9467\n",
      "Train Epoch: 198 [14008/22024] \t Train Loss(MSE): 17.7791 \t Train RMSE: 4.2165\n",
      "Train Epoch: 198 [16008/22024] \t Train Loss(MSE): 1.2837 \t Train RMSE: 1.1330\n",
      "Train Epoch: 198 [18008/22024] \t Train Loss(MSE): 2.8433 \t Train RMSE: 1.6862\n",
      "Train Epoch: 198 [20008/22024] \t Train Loss(MSE): 15.9384 \t Train RMSE: 3.9923\n",
      "Train Epoch: 198 [22008/22024] \t Train Loss(MSE): 3.6816 \t Train RMSE: 1.9188\n",
      "[Epoch: 198 \t Valid MSE: 22.9187 \t Valid RMSE: 4.2228]\n",
      "Train Epoch: 199 [8/22024] \t Train Loss(MSE): 0.9074 \t Train RMSE: 0.9526\n",
      "Train Epoch: 199 [2008/22024] \t Train Loss(MSE): 7.6788 \t Train RMSE: 2.7711\n",
      "Train Epoch: 199 [4008/22024] \t Train Loss(MSE): 13.6762 \t Train RMSE: 3.6981\n",
      "Train Epoch: 199 [6008/22024] \t Train Loss(MSE): 15.0243 \t Train RMSE: 3.8761\n",
      "Train Epoch: 199 [8008/22024] \t Train Loss(MSE): 3.4436 \t Train RMSE: 1.8557\n",
      "Train Epoch: 199 [10008/22024] \t Train Loss(MSE): 0.5955 \t Train RMSE: 0.7717\n",
      "Train Epoch: 199 [12008/22024] \t Train Loss(MSE): 91.5182 \t Train RMSE: 9.5665\n",
      "Train Epoch: 199 [14008/22024] \t Train Loss(MSE): 8.7865 \t Train RMSE: 2.9642\n",
      "Train Epoch: 199 [16008/22024] \t Train Loss(MSE): 6.9045 \t Train RMSE: 2.6276\n",
      "Train Epoch: 199 [18008/22024] \t Train Loss(MSE): 4.8252 \t Train RMSE: 2.1966\n",
      "Train Epoch: 199 [20008/22024] \t Train Loss(MSE): 15.6990 \t Train RMSE: 3.9622\n",
      "Train Epoch: 199 [22008/22024] \t Train Loss(MSE): 3.1624 \t Train RMSE: 1.7783\n",
      "[Epoch: 199 \t Valid MSE: 20.5776 \t Valid RMSE: 3.9828]\n",
      "Train Epoch: 200 [8/22024] \t Train Loss(MSE): 20.9515 \t Train RMSE: 4.5773\n",
      "Train Epoch: 200 [2008/22024] \t Train Loss(MSE): 6.7200 \t Train RMSE: 2.5923\n",
      "Train Epoch: 200 [4008/22024] \t Train Loss(MSE): 4.8377 \t Train RMSE: 2.1995\n",
      "Train Epoch: 200 [6008/22024] \t Train Loss(MSE): 8.8051 \t Train RMSE: 2.9673\n",
      "Train Epoch: 200 [8008/22024] \t Train Loss(MSE): 8.2031 \t Train RMSE: 2.8641\n",
      "Train Epoch: 200 [10008/22024] \t Train Loss(MSE): 6.6179 \t Train RMSE: 2.5725\n",
      "Train Epoch: 200 [12008/22024] \t Train Loss(MSE): 9.0691 \t Train RMSE: 3.0115\n",
      "Train Epoch: 200 [14008/22024] \t Train Loss(MSE): 106.7652 \t Train RMSE: 10.3327\n",
      "Train Epoch: 200 [16008/22024] \t Train Loss(MSE): 10.4287 \t Train RMSE: 3.2294\n",
      "Train Epoch: 200 [18008/22024] \t Train Loss(MSE): 6.4823 \t Train RMSE: 2.5460\n",
      "Train Epoch: 200 [20008/22024] \t Train Loss(MSE): 0.9964 \t Train RMSE: 0.9982\n",
      "Train Epoch: 200 [22008/22024] \t Train Loss(MSE): 6.3684 \t Train RMSE: 2.5236\n",
      "[Epoch: 200 \t Valid MSE: 19.5583 \t Valid RMSE: 3.7863]\n"
     ]
    }
   ],
   "source": [
    "Epoch = 200\n",
    "best_rmse = 99999\n",
    "\n",
    "train_mse_list = []\n",
    "train_rmse_list = []\n",
    "valid_mse_list = []\n",
    "valid_rmse_list = []\n",
    "\n",
    "for epoch in range(1,(Epoch+1)):\n",
    "    train_mse, train_rmse = train(model, train_dataloader, 250)\n",
    "    valid_mse, valid_rmse = evaluate(model, valid_dataloader)\n",
    "\n",
    "    print(\"[Epoch: {} \\t Valid MSE: {:.4f} \\t Valid RMSE: {:.4f}]\".format(epoch, valid_mse, valid_rmse))\n",
    "    \n",
    "    scheduler.step(train_mse)       \n",
    "    # Save model\n",
    "    if valid_rmse < best_rmse:\n",
    "        path = \"./weights/ODD_basic_batch.pth\" \n",
    "        torch.save(model.state_dict(), path) # 모델의 가중치만 저장 구조는 저장 x..?\n",
    "        \n",
    "    train_mse_list.append(train_mse)\n",
    "    train_rmse_list.append(train_rmse)\n",
    "    valid_mse_list.append(valid_mse)\n",
    "    valid_rmse_list.append(valid_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.plot(train_mse_list, ls='-', color='blue', label='train')\n",
    "ax1.set_ylim(0,30)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(valid_mse_list, ls='--', color='red', label='valid')\n",
    "ax2.set_ylim(0,30)\n",
    "\n",
    "ax1.set_title('MSE error')\n",
    "ax1.legend(loc='upper right')\n",
    "ax2.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.plot(train_rmse_list, ls='-', color='blue', label='train')\n",
    "ax1.set_ylim(0,7)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(valid_rmse_list, ls='--', color='red', label='valid')\n",
    "ax2.set_ylim(0,7)\n",
    "ax1.set_title('RMSE error')\n",
    "ax1.legend(loc='upper right')\n",
    "ax2.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistanceEstimator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=14, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (11): ReLU()\n",
       "    (12): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (13): ReLU()\n",
       "    (14): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (15): ReLU()\n",
       "    (16): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (17): ReLU()\n",
       "    (18): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (19): ReLU()\n",
       "    (20): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (21): ReLU()\n",
       "    (22): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (23): ReLU()\n",
       "    (24): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 가져오기\n",
    "model = DistanceEstimator()\n",
    "model.load_state_dict(torch.load('./weights/ODD_basic_batch.pth'))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 14.907510 \t Test RMSE: 3.171425\n"
     ]
    }
   ],
   "source": [
    "test_mse, test_rmse = evaluate(model, test_dataloader)\n",
    "print('Test MSE: {:4f} \\t Test RMSE: {:4f}'.format(test_mse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_zloc = model(torch.FloatTensor(df_test[variable].values).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zloc</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44.91</td>\n",
       "      <td>45.053768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.82</td>\n",
       "      <td>35.686333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.98</td>\n",
       "      <td>4.592454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.39</td>\n",
       "      <td>12.482031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.04</td>\n",
       "      <td>13.171516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.00</td>\n",
       "      <td>20.572550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.09</td>\n",
       "      <td>19.990051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.50</td>\n",
       "      <td>42.884384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22.45</td>\n",
       "      <td>23.891405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>51.44</td>\n",
       "      <td>53.087433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.42</td>\n",
       "      <td>5.763949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.59</td>\n",
       "      <td>19.424490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.49</td>\n",
       "      <td>18.562851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32.38</td>\n",
       "      <td>32.307762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39.15</td>\n",
       "      <td>36.598816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>43.40</td>\n",
       "      <td>45.371834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40.74</td>\n",
       "      <td>41.805691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>46.75</td>\n",
       "      <td>43.969585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21.32</td>\n",
       "      <td>22.007103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>35.62</td>\n",
       "      <td>33.461899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>54.18</td>\n",
       "      <td>46.855537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.99</td>\n",
       "      <td>7.008498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.27</td>\n",
       "      <td>14.960419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>37.26</td>\n",
       "      <td>39.682663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>41.47</td>\n",
       "      <td>43.719688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.46</td>\n",
       "      <td>8.426472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.53</td>\n",
       "      <td>20.345421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32.32</td>\n",
       "      <td>30.902826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>74.57</td>\n",
       "      <td>73.286209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14.09</td>\n",
       "      <td>14.250130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>23.57</td>\n",
       "      <td>23.523224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>46.10</td>\n",
       "      <td>50.215530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8.56</td>\n",
       "      <td>8.997269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.66</td>\n",
       "      <td>4.658785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52.48</td>\n",
       "      <td>56.019596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.15</td>\n",
       "      <td>9.226366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>14.99</td>\n",
       "      <td>14.838102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11.74</td>\n",
       "      <td>10.802340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>22.51</td>\n",
       "      <td>21.417109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>22.68</td>\n",
       "      <td>24.261028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>28.65</td>\n",
       "      <td>34.024261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>37.65</td>\n",
       "      <td>35.438221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41.10</td>\n",
       "      <td>32.211742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>24.88</td>\n",
       "      <td>25.211256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>62.34</td>\n",
       "      <td>58.295666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>30.51</td>\n",
       "      <td>30.587774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>37.94</td>\n",
       "      <td>40.117188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>22.52</td>\n",
       "      <td>23.036245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>53.34</td>\n",
       "      <td>52.508064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>21.43</td>\n",
       "      <td>25.624313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     zloc    predict\n",
       "0   44.91  45.053768\n",
       "1   33.82  35.686333\n",
       "2    3.98   4.592454\n",
       "3   11.39  12.482031\n",
       "4   13.04  13.171516\n",
       "5   21.00  20.572550\n",
       "6   21.09  19.990051\n",
       "7   48.50  42.884384\n",
       "8   22.45  23.891405\n",
       "9   51.44  53.087433\n",
       "10   6.42   5.763949\n",
       "11  19.59  19.424490\n",
       "12  20.49  18.562851\n",
       "13  32.38  32.307762\n",
       "14  39.15  36.598816\n",
       "15  43.40  45.371834\n",
       "16  40.74  41.805691\n",
       "17  46.75  43.969585\n",
       "18  21.32  22.007103\n",
       "19  35.62  33.461899\n",
       "20  54.18  46.855537\n",
       "21   7.99   7.008498\n",
       "22  15.27  14.960419\n",
       "23  37.26  39.682663\n",
       "24  41.47  43.719688\n",
       "25   7.46   8.426472\n",
       "26  19.53  20.345421\n",
       "27  32.32  30.902826\n",
       "28  74.57  73.286209\n",
       "29  14.09  14.250130\n",
       "30  23.57  23.523224\n",
       "31  46.10  50.215530\n",
       "32   8.56   8.997269\n",
       "33   2.66   4.658785\n",
       "34  52.48  56.019596\n",
       "35  10.15   9.226366\n",
       "36  14.99  14.838102\n",
       "37  11.74  10.802340\n",
       "38  22.51  21.417109\n",
       "39  22.68  24.261028\n",
       "40  28.65  34.024261\n",
       "41  37.65  35.438221\n",
       "42  41.10  32.211742\n",
       "43  24.88  25.211256\n",
       "44  62.34  58.295666\n",
       "45  30.51  30.587774\n",
       "46  37.94  40.117188\n",
       "47  22.52  23.036245\n",
       "48  53.34  52.508064\n",
       "49  21.43  25.624313"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['predict'] = predict_zloc.cpu().detach().numpy()\n",
    "df_test[['zloc','predict']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwdFu-xwEdAU"
   },
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def train_model(model, train_dataloader, valid_dataloader, loss_fn, lr=1e-5, batch_size=512, epochs=100, validate=False):\n",
    "\n",
    "  \n",
    "  # Convert model parameters and buffers to CPU or Cuda\n",
    "  model.to(device)\n",
    "\n",
    "  best_rmse = np.Inf\n",
    "  print(\"Begin training...\") \n",
    "  for epoch in range(1, epochs+1): \n",
    "    running_train_loss = 0.0 \n",
    "    running_rmse = 0.0 \n",
    "    running_vall_loss = 0.0 \n",
    "    total = 0 \n",
    "\n",
    "    for batch_ind, samples in enumerate(train_dataloader):\n",
    "      x_train, y_train = samples\n",
    "      optimizer.zero_grad()\n",
    "      pred = model.forward(x_train)\n",
    "      train_loss = loss_fn(pred, y_train)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      running_train_loss += train_loss.item()\n",
    "\n",
    "    train_loss_value = running_train_loss/len(train_dataloader)\n",
    "    with torch.no_grad(): \n",
    "      model.eval() \n",
    "      for data in valid_dataloader: \n",
    "        inputs, outputs = data \n",
    "        predicted_outputs = model(inputs) \n",
    "        val_loss = loss_fn(predicted_outputs, outputs) \n",
    "      \n",
    "        # The label with the highest value will be our prediction \n",
    "        running_vall_loss += val_loss.item()  \n",
    "        total += outputs.size(0) \n",
    "        rmse = mean_squared_error(outputs, predicted_outputs)**0.5\n",
    "        running_rmse += rmse\n",
    "\n",
    "    # Calculate validation loss value \n",
    "    val_loss_value = running_vall_loss/len(valid_dataloader)  \n",
    "    rmse = running_rmse / total\n",
    "\n",
    "    if rmse < best_rmse:\n",
    "      saveModel(model)\n",
    "      best_rmse = rmse\n",
    "\n",
    "    # Print the statistics of the epoch \n",
    "    print('Epoch {0}/{1} - loss: {2:.4f} / val_loss: {3:.4f} - RMSE: {4:.4f}'.format(epoch, epochs, train_loss_value, val_loss_value,rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3dgdPwJGdnQ"
   },
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tE5K6pUX-LG3"
   },
   "source": [
    "model = DistanceEstimator()\n",
    "#optimizer = torch.optim.Adam(model.parameters, lr=1e-5)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1664546,
     "status": "ok",
     "timestamp": 1649523139093,
     "user": {
      "displayName": "박선영",
      "userId": "02522110649935123410"
     },
     "user_tz": -540
    },
    "id": "bqPh_Uj9BzeV",
    "outputId": "97271dc0-8ed0-474d-97f9-ec6c20901ef6"
   },
   "source": [
    "train_model(model, train_dataloader, valid_dataloader, loss_func, epochs=100, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9b6kPgY96vK"
   },
   "source": [
    "##Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr-YyzYXBGsw"
   },
   "source": [
    "def predict(test_dataloader): \n",
    "    # Load the model that we saved at the end of the training loop \n",
    "    model = DistanceEstimator()\n",
    "    path = \"NetModel.pth\" \n",
    "    model.load_state_dict(torch.load(path)) \n",
    "     \n",
    "    running_rmse = 0 \n",
    "    total = 0 \n",
    "    pred = []\n",
    " \n",
    "    with torch.no_grad(): \n",
    "      for data in test_dataloader: \n",
    "        inputs, outputs = data \n",
    "        outputs = outputs.to(torch.float32) \n",
    "        predicted_outputs = model(inputs) \n",
    "        pred.append(float(predicted_outputs))\n",
    "        total += outputs.size(0) \n",
    "        rmse = mean_squared_error(outputs, predicted_outputs)**0.5\n",
    "        running_rmse += rmse\n",
    " \n",
    "      print('RMSE:',running_rmse / total)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ssK71esE-9q"
   },
   "source": [
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1649523483055,
     "user": {
      "displayName": "박선영",
      "userId": "02522110649935123410"
     },
     "user_tz": -540
    },
    "id": "v_EkCScgFxpb",
    "outputId": "f22a38a2-970e-4bf8-a9c0-e38ebc050db5"
   },
   "source": [
    "pred = predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1649523490608,
     "user": {
      "displayName": "박선영",
      "userId": "02522110649935123410"
     },
     "user_tz": -540
    },
    "id": "H1V6NrWNGQFu",
    "outputId": "4ed81376-c6ec-47b2-9351-fb34cc87c884"
   },
   "source": [
    "#Result with prediction\n",
    "df_test['zloc_pred'] = pred\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJgfmokyGc5B"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "randomfores_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # '-1' means last dimension. \n",
    "\n",
    "        out = (x - mean) / (std + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    compute scale dot product attention\n",
    "    Query : given sentence that we focused on (decoder)\n",
    "    Key : every sentence to check relationship with Qeury(encoder)\n",
    "    Value : every sentence same with Key (encoder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        # input is 4 dimension tensor\n",
    "        # [batch_size, head, length, d_tensor]\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        # 1. dot product Query with Key^T to compute similarity\n",
    "        k_t = k.transpose(2, 3)  # transpose\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "\n",
    "        # 2. apply masking (opt)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -e)\n",
    "\n",
    "        # 3. pass them softmax to make [0, 1] range\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        # 4. multiply with Value\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 1. dot product with weight matrices\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        # 2. split tensor by number of heads\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        # 3. do scale dot product to compute similarity\n",
    "        out, attention = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # 4. concat and pass to linear layer\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "\n",
    "        # 5. visualize attention map\n",
    "        # TODO : we should implement visualization\n",
    "\n",
    "        return out\n",
    "\n",
    "    def split(self, tensor):\n",
    "        \"\"\"\n",
    "        split tensor by number of head\n",
    "        :param tensor: [batch_size, length, d_model]\n",
    "        :return: [batch_size, head, length, d_tensor]\n",
    "        \"\"\"\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "        # it is similar with group convolution (split by number of heads)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def concat(self, tensor):\n",
    "        \"\"\"\n",
    "        inverse function of self.split(tensor : torch.Tensor)\n",
    "        :param tensor: [batch_size, head, length, d_tensor]\n",
    "        :return: [batch_size, length, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostionalEncoding(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PostionalEncoding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False  \n",
    "\n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "        # 1D => 2D unsqueeze to represent word's position\n",
    "\n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])\n",
    "        # \"step=2\" means 'i' multiplied with two (same with 2 * i)\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, seq_len = x.size()\n",
    "\n",
    "\n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    Token Embedding using torch.nn\n",
    "    they will dense representation of word using weighted matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        class for token embedding that included positional information\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param d_model: dimensions of model\n",
    "        \"\"\"\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        \n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PostionalEncoding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        return self.drop_out(tok_emb + pos_emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=enc_voc_size,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, s_mask):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # pass to LM head\n",
    "        output = self.linear(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
    "                 ffn_hidden, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src)\n",
    "\n",
    "        src_trg_mask = self.make_pad_mask(trg, src)\n",
    "\n",
    "        trg_mask = self.make_pad_mask(trg, trg) * \\\n",
    "                   self.make_no_peak_mask(trg, trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return output\n",
    "\n",
    "    def make_pad_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # batch_size x 1 x 1 x len_k\n",
    "        k = k.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        # batch_size x 1 x len_q x 1\n",
    "        q = q.ne(self.src_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        # batch_size x 1 x len_q x len_k\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "\n",
    "        mask = k & q\n",
    "        return mask\n",
    "\n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "\n",
    "        # len_q x len_k\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM3GgQX9wAHsY2zHAbVwAG/",
   "name": "DistanceEstimator.ipynb",
   "provenance": [
    {
     "file_id": "1WN5OSA-TXiMkTLDr9xyt2F7Z_Hd16-b4",
     "timestamp": 1649567852353
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
